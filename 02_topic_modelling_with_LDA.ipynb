{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Note: This notebook is inspired by the Topic-Modeling-Latent-Dirichlet-Allocation series at: https://github.com/rhasanbd/Topic-Modeling-Latent-Dirichlet-Allocation </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation - Implementation on Yelp dataset\n",
    "\n",
    "In this notebook, we implement Latent Dirichlet Allocation(LDA) on the Yelp reviews data to carry out Topic Modelling. We use the Gensim topic modelling API https://radimrehurek.com/gensim/models/ldamodel.html. Scikit-Learn implementation is also available (we use Gensim since it provides more functionality and application like Topic Coherence Pipeline or Dynamic Topic Modeling.)\n",
    "\n",
    "We build an **end-to-end Natural Language Processing (NLP) pipeline**, starting with raw data and running through preparing, modeling, visualization.\n",
    "The steps that we will carry out involves the following:\n",
    "1. Exploratory Data Analysis\n",
    "2. Data Cleaning and Pre-processing\n",
    "3. Topic modeling with LDA\n",
    "4. Determine optimal number of Topics\n",
    "5. Visualize topic model using pyLDAvis\n",
    "\n",
    "### Yelp Review Dataset\n",
    "The Yelp Review Dataset is a CSV file that contains a sub-sample of 10,000 reviews extracted from the Yelp dataset available at: https://www.yelp.com/dataset.\n",
    "\n",
    "The review dataset contains the following fields:\n",
    "- business_id : Unique identifier of business\n",
    "- date : Data of review posted YYYY-MM-DD\n",
    "- review_id : Unique identifier of review\n",
    "- stars : Star rating (upto 4 stars)\n",
    "- text : Review text\n",
    "- user_id : Unique identifier of user who posted the review\n",
    "- cool : Number of cool votes received\n",
    "- useful : Number of useful votes received\n",
    "- funny : Number of funny votes received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-18 10:14:17,433 : DEBUG : $HOME=C:\\Users\\rojin\n",
      "2020-03-18 10:14:17,439 : DEBUG : CONFIGDIR=C:\\Users\\rojin\\.matplotlib\n",
      "2020-03-18 10:14:17,441 : DEBUG : matplotlib data path: c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "2020-03-18 10:14:17,462 : DEBUG : loaded rc file c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\mpl-data\\matplotlibrc\n",
      "2020-03-18 10:14:17,470 : DEBUG : matplotlib version 3.1.2\n",
      "2020-03-18 10:14:17,472 : DEBUG : interactive is False\n",
      "2020-03-18 10:14:17,474 : DEBUG : platform is win32\n",
      "2020-03-18 10:14:17,476 : DEBUG : loaded modules: ['sys', 'builtins', '_frozen_importlib', '_imp', '_thread', '_warnings', '_weakref', 'zipimport', '_frozen_importlib_external', '_io', 'marshal', 'nt', 'winreg', 'encodings', 'codecs', '_codecs', 'encodings.aliases', 'encodings.utf_8', '_signal', '__main__', 'encodings.latin_1', 'io', 'abc', '_abc', '_bootlocale', '_locale', 'encodings.cp1252', 'site', 'os', 'stat', '_stat', 'ntpath', 'genericpath', 'os.path', '_collections_abc', '_sitebuiltins', 'types', 'importlib', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib.util', 'importlib.abc', 'importlib.machinery', 'contextlib', 'collections', 'operator', '_operator', 'keyword', 'heapq', '_heapq', 'itertools', 'reprlib', '_collections', 'functools', '_functools', 'mpl_toolkits', 'pywin32_bootstrap', 'runpy', 'pkgutil', 'weakref', '_weakrefset', 'ipykernel', 'ipykernel._version', 'ipykernel.connect', '__future__', 'json', 'json.decoder', 're', 'enum', 'sre_compile', '_sre', 'sre_parse', 'sre_constants', 'copyreg', 'json.scanner', '_json', 'json.encoder', 'subprocess', 'time', 'signal', 'errno', 'threading', 'traceback', 'linecache', 'tokenize', 'token', 'msvcrt', '_winapi', 'IPython', 'IPython.core', 'IPython.core.getipython', 'IPython.core.release', 'IPython.core.application', 'atexit', 'copy', 'glob', 'fnmatch', 'posixpath', 'logging', 'collections.abc', 'string', '_string', 'shutil', 'zlib', 'bz2', '_compression', '_bz2', 'lzma', '_lzma', 'traitlets', 'traitlets.traitlets', 'inspect', 'dis', 'opcode', '_opcode', 'six', 'struct', '_struct', 'traitlets.utils', 'traitlets.utils.getargspec', 'traitlets.utils.importstring', 'ipython_genutils', 'ipython_genutils._version', 'ipython_genutils.py3compat', 'ipython_genutils.encoding', 'locale', 'platform', 'traitlets.utils.sentinel', 'traitlets.utils.bunch', 'traitlets._version', 'traitlets.config', 'traitlets.config.application', 'decorator', 'traitlets.config.configurable', 'traitlets.config.loader', 'argparse', 'gettext', 'ast', '_ast', 'ipython_genutils.path', 'random', 'math', 'hashlib', '_hashlib', '_blake2', '_sha3', 'bisect', '_bisect', '_random', 'ipython_genutils.text', 'textwrap', 'ipython_genutils.importstring', 'IPython.core.crashhandler', 'pprint', 'IPython.core.ultratb', 'pydoc', 'urllib', 'urllib.parse', 'IPython.core.debugger', 'bdb', 'IPython.utils', 'IPython.utils.PyColorize', 'IPython.utils.coloransi', 'IPython.utils.ipstruct', 'IPython.utils.colorable', 'pygments', 'pygments.util', 'IPython.utils.py3compat', 'IPython.utils.encoding', 'IPython.core.excolors', 'IPython.testing', 'IPython.testing.skipdoctest', 'pdb', 'cmd', 'code', 'codeop', 'IPython.core.display_trap', 'IPython.utils.path', 'IPython.utils.process', 'IPython.utils._process_win32', 'ctypes', '_ctypes', 'ctypes._endian', 'ctypes.wintypes', 'IPython.utils._process_common', 'shlex', 'IPython.utils.decorators', 'IPython.utils.data', 'IPython.utils.terminal', 'IPython.utils.sysinfo', 'IPython.utils._sysinfo', 'IPython.core.profiledir', 'IPython.paths', 'tempfile', 'IPython.utils.importstring', 'IPython.terminal', 'IPython.terminal.embed', 'IPython.core.compilerop', 'IPython.core.magic_arguments', 'IPython.core.error', 'IPython.utils.text', 'pathlib', 'IPython.core.magic', 'getopt', 'IPython.core.oinspect', 'IPython.core.page', 'IPython.core.display', 'binascii', 'mimetypes', 'IPython.lib', 'IPython.lib.security', 'getpass', 'IPython.lib.pretty', 'datetime', '_datetime', 'IPython.utils.openpy', 'IPython.utils.dir2', 'IPython.utils.wildcard', 'pygments.lexers', 'pygments.lexers._mapping', 'pygments.modeline', 'pygments.plugin', 'pygments.lexers.python', 'pygments.lexer', 'pygments.filter', 'pygments.filters', 'pygments.token', 'pygments.regexopt', 'pygments.unistring', 'pygments.formatters', 'pygments.formatters._mapping', 'pygments.formatters.html', 'pygments.formatter', 'pygments.styles', 'IPython.core.inputtransformer2', 'typing', 'typing.io', 'typing.re', 'IPython.core.interactiveshell', 'asyncio', 'asyncio.base_events', 'concurrent', 'concurrent.futures', 'concurrent.futures._base', 'socket', '_socket', 'selectors', 'select', 'ssl', '_ssl', 'base64', 'asyncio.constants', 'asyncio.coroutines', 'asyncio.base_futures', 'asyncio.format_helpers', 'asyncio.log', 'asyncio.events', 'contextvars', '_contextvars', 'asyncio.base_tasks', '_asyncio', 'asyncio.futures', 'asyncio.protocols', 'asyncio.sslproto', 'asyncio.transports', 'asyncio.tasks', 'asyncio.locks', 'asyncio.runners', 'asyncio.queues', 'asyncio.streams', 'asyncio.subprocess', 'asyncio.windows_events', '_overlapped', 'asyncio.base_subprocess', 'asyncio.proactor_events', 'asyncio.selector_events', 'asyncio.windows_utils', 'pickleshare', 'pickle', '_compat_pickle', '_pickle', 'IPython.core.prefilter', 'IPython.core.autocall', 'IPython.core.macro', 'IPython.core.splitinput', 'IPython.core.alias', 'IPython.core.builtin_trap', 'IPython.core.events', 'backcall', 'backcall.backcall', 'IPython.core.displayhook', 'IPython.core.displaypub', 'IPython.core.extensions', 'IPython.core.formatters', 'IPython.utils.sentinel', 'IPython.core.history', 'sqlite3', 'sqlite3.dbapi2', '_sqlite3', 'IPython.core.logger', 'IPython.core.payload', 'IPython.core.usage', 'IPython.display', 'IPython.lib.display', 'html', 'html.entities', 'IPython.utils.io', 'IPython.utils.capture', 'IPython.utils.strdispatch', 'IPython.core.hooks', 'IPython.utils.syspathcontext', 'IPython.utils.tempdir', 'IPython.utils.contexts', 'IPython.core.async_helpers', 'IPython.terminal.interactiveshell', 'prompt_toolkit', 'prompt_toolkit.application', 'prompt_toolkit.application.application', 'prompt_toolkit.buffer', 'six.moves', 'prompt_toolkit.application.current', 'prompt_toolkit.eventloop', 'prompt_toolkit.eventloop.async_generator', 'queue', '_queue', 'six.moves.queue', 'prompt_toolkit.eventloop.coroutine', 'prompt_toolkit.eventloop.defaults', 'prompt_toolkit.utils', 'wcwidth', 'wcwidth.wcwidth', 'wcwidth.table_wide', 'wcwidth.table_zero', 'prompt_toolkit.cache', 'prompt_toolkit.eventloop.base', 'prompt_toolkit.log', 'prompt_toolkit.eventloop.future', 'prompt_toolkit.eventloop.context', 'prompt_toolkit.eventloop.event', 'prompt_toolkit.application.run_in_terminal', 'prompt_toolkit.auto_suggest', 'prompt_toolkit.filters', 'prompt_toolkit.filters.app', 'prompt_toolkit.enums', 'prompt_toolkit.filters.base', 'prompt_toolkit.filters.cli', 'prompt_toolkit.filters.utils', 'prompt_toolkit.clipboard', 'prompt_toolkit.clipboard.base', 'prompt_toolkit.selection', 'prompt_toolkit.clipboard.in_memory', 'prompt_toolkit.completion', 'prompt_toolkit.completion.base', 'prompt_toolkit.completion.filesystem', 'prompt_toolkit.completion.fuzzy_completer', 'prompt_toolkit.document', 'prompt_toolkit.completion.word_completer', 'prompt_toolkit.history', 'prompt_toolkit.search', 'prompt_toolkit.key_binding', 'prompt_toolkit.key_binding.key_bindings', 'prompt_toolkit.keys', 'prompt_toolkit.key_binding.vi_state', 'prompt_toolkit.validation', 'prompt_toolkit.input', 'prompt_toolkit.input.base', 'prompt_toolkit.input.defaults', 'prompt_toolkit.input.typeahead', 'prompt_toolkit.key_binding.bindings', 'prompt_toolkit.key_binding.bindings.page_navigation', 'prompt_toolkit.key_binding.bindings.scroll', 'prompt_toolkit.key_binding.defaults', 'prompt_toolkit.key_binding.bindings.basic', 'prompt_toolkit.key_binding.key_processor', 'prompt_toolkit.key_binding.bindings.named_commands', 'prompt_toolkit.key_binding.bindings.completion', 'prompt_toolkit.key_binding.bindings.cpr', 'prompt_toolkit.key_binding.bindings.emacs', 'prompt_toolkit.key_binding.bindings.mouse', 'prompt_toolkit.layout', 'prompt_toolkit.layout.containers', 'prompt_toolkit.formatted_text', 'prompt_toolkit.formatted_text.ansi', 'prompt_toolkit.output', 'prompt_toolkit.output.base', 'prompt_toolkit.layout.screen', 'prompt_toolkit.output.color_depth', 'prompt_toolkit.output.defaults', 'prompt_toolkit.output.vt100', 'array', 'prompt_toolkit.styles', 'prompt_toolkit.styles.base', 'prompt_toolkit.styles.defaults', 'prompt_toolkit.styles.named_colors', 'prompt_toolkit.styles.style', 'prompt_toolkit.styles.pygments', 'prompt_toolkit.styles.style_transformation', 'colorsys', 'prompt_toolkit.formatted_text.base', 'prompt_toolkit.formatted_text.html', 'xml', 'xml.dom', 'xml.dom.domreg', 'xml.dom.minidom', 'xml.dom.minicompat', 'xml.dom.xmlbuilder', 'xml.dom.NodeFilter', 'prompt_toolkit.formatted_text.pygments', 'prompt_toolkit.formatted_text.utils', 'prompt_toolkit.mouse_events', 'prompt_toolkit.layout.controls', 'prompt_toolkit.lexers', 'prompt_toolkit.lexers.base', 'prompt_toolkit.lexers.pygments', 'prompt_toolkit.layout.processors', 'prompt_toolkit.layout.utils', 'prompt_toolkit.layout.dimension', 'prompt_toolkit.layout.margins', 'prompt_toolkit.layout.layout', 'prompt_toolkit.layout.menus', 'prompt_toolkit.renderer', 'prompt_toolkit.layout.mouse_handlers', 'prompt_toolkit.key_binding.bindings.vi', 'prompt_toolkit.input.vt100_parser', 'prompt_toolkit.input.ansi_escape_sequences', 'prompt_toolkit.key_binding.digraphs', 'prompt_toolkit.key_binding.emacs_state', 'prompt_toolkit.layout.dummy', 'prompt_toolkit.application.dummy', 'prompt_toolkit.shortcuts', 'prompt_toolkit.shortcuts.dialogs', 'prompt_toolkit.key_binding.bindings.focus', 'prompt_toolkit.widgets', 'prompt_toolkit.widgets.base', 'prompt_toolkit.widgets.toolbars', 'prompt_toolkit.widgets.dialogs', 'prompt_toolkit.widgets.menus', 'prompt_toolkit.shortcuts.progress_bar', 'prompt_toolkit.shortcuts.progress_bar.base', 'prompt_toolkit.shortcuts.progress_bar.formatters', 'prompt_toolkit.shortcuts.prompt', 'prompt_toolkit.key_binding.bindings.auto_suggest', 'prompt_toolkit.key_binding.bindings.open_in_editor', 'prompt_toolkit.shortcuts.utils', 'prompt_toolkit.patch_stdout', 'pygments.style', 'IPython.terminal.debugger', 'IPython.core.completer', 'unicodedata', 'IPython.core.latex_symbols', 'IPython.utils.generics', 'jedi', 'jedi.api', 'parso', 'parso.parser', 'parso.tree', 'parso._compatibility', 'parso.utils', 'parso.pgen2', 'parso.pgen2.generator', 'parso.pgen2.grammar_parser', 'parso.python', 'parso.python.tokenize', 'parso.python.token', 'parso.grammar', 'parso.python.diff', 'difflib', 'parso.python.parser', 'parso.python.tree', 'parso.python.prefix', 'parso.cache', 'gc', 'parso.python.errors', 'parso.normalizer', 'parso.python.pep8', 'parso.file_io', 'jedi._compatibility', 'jedi.file_io', 'jedi.parser_utils', 'jedi.debug', 'jedi.settings', 'jedi.cache', 'jedi.api.classes', 'jedi.evaluate', 'jedi.evaluate.utils', 'jedi.evaluate.imports', 'jedi.evaluate.sys_path', 'jedi.evaluate.cache', 'jedi.evaluate.base_context', 'jedi.common', 'jedi.common.context', 'jedi.evaluate.helpers', 'jedi.common.utils', 'jedi.evaluate.compiled', 'jedi.evaluate.compiled.context', 'jedi.evaluate.filters', 'jedi.evaluate.flow_analysis', 'jedi.evaluate.recursion', 'jedi.evaluate.names', 'jedi.evaluate.lazy_context', 'jedi.evaluate.compiled.access', 'jedi.evaluate.compiled.getattr_static', 'jedi.evaluate.signature', 'jedi.evaluate.analysis', 'jedi.evaluate.gradual', 'jedi.evaluate.gradual.typeshed', 'jedi.evaluate.gradual.stub_context', 'jedi.evaluate.context', 'jedi.evaluate.context.module', 'jedi.evaluate.context.klass', 'jedi.evaluate.arguments', 'jedi.evaluate.context.iterable', 'jedi.evaluate.param', 'jedi.evaluate.docstrings', 'jedi.evaluate.context.function', 'jedi.evaluate.parser_cache', 'jedi.plugins', 'jedi.evaluate.context.instance', 'jedi.evaluate.gradual.typing', 'jedi.evaluate.syntax_tree', 'jedi.evaluate.finder', 'jedi.evaluate.gradual.conversion', 'jedi.evaluate.gradual.annotation', 'jedi.evaluate.context.decorator', 'jedi.api.keywords', 'pydoc_data', 'pydoc_data.topics', 'jedi.api.interpreter', 'jedi.evaluate.compiled.mixed', 'jedi.api.helpers', 'jedi.api.completion', 'jedi.api.file_name', 'jedi.api.environment', 'filecmp', 'jedi.evaluate.compiled.subprocess', 'jedi.evaluate.compiled.subprocess.functions', 'jedi.api.exceptions', 'jedi.api.project', 'jedi.evaluate.usages', 'jedi.evaluate.gradual.utils', 'jedi.plugins.registry', 'jedi.plugins.stdlib', 'jedi.plugins.flask', 'IPython.terminal.ptutils', 'IPython.terminal.shortcuts', 'IPython.lib.clipboard', 'IPython.terminal.magics', 'IPython.terminal.pt_inputhooks', 'IPython.terminal.prompts', 'IPython.terminal.ipapp', 'IPython.core.magics', 'IPython.core.magics.auto', 'IPython.core.magics.basic', 'IPython.core.magics.code', 'urllib.request', 'email', 'http', 'http.client', 'email.parser', 'email.feedparser', 'email.errors', 'email._policybase', 'email.header', 'email.quoprimime', 'email.base64mime', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'email._parseaddr', 'calendar', 'email.message', 'uu', 'email._encoded_words', 'email.iterators', 'urllib.error', 'urllib.response', 'nturl2path', 'IPython.core.magics.config', 'IPython.core.magics.display', 'IPython.core.magics.execution', 'timeit', 'cProfile', '_lsprof', 'profile', 'pstats', 'IPython.utils.module_paths', 'IPython.utils.timing', 'IPython.core.magics.extension', 'IPython.core.magics.history', 'IPython.core.magics.logging', 'IPython.core.magics.namespace', 'IPython.core.magics.osm', 'IPython.core.magics.packaging', 'IPython.core.magics.pylab', 'IPython.core.pylabtools', 'IPython.core.magics.script', 'IPython.lib.backgroundjobs', 'IPython.core.shellapp', 'IPython.extensions', 'IPython.extensions.storemagic', 'IPython.utils.frame', 'jupyter_client', 'jupyter_client._version', 'jupyter_client.connect', 'zmq', 'zmq.libzmq', 'zmq.backend', 'zmq.backend.select', 'zmq.backend.cython', 'zmq.backend.cython.constants', 'cython_runtime', 'zmq.backend.cython.error', '_cython_0_29_14', 'zmq.backend.cython.message', 'zmq.error', 'zmq.backend.cython.context', 'zmq.backend.cython.socket', 'zmq.backend.cython.utils', 'zmq.backend.cython._poll', 'zmq.backend.cython._version', 'zmq.backend.cython._device', 'zmq.backend.cython._proxy_steerable', 'zmq.sugar', 'zmq.sugar.constants', 'zmq.utils', 'zmq.utils.constant_names', 'zmq.sugar.context', 'zmq.sugar.attrsettr', 'zmq.sugar.socket', 'zmq.sugar.poll', 'zmq.utils.jsonapi', 'zmq.utils.strtypes', 'simplejson', 'decimal', 'numbers', '_decimal', 'simplejson.errors', 'simplejson.raw_json', 'simplejson.decoder', 'simplejson.compat', 'simplejson.scanner', 'simplejson._speedups', 'simplejson.encoder', 'zmq.sugar.frame', 'zmq.sugar.tracker', 'zmq.sugar.version', 'zmq.sugar.stopwatch', 'jupyter_client.localinterfaces', 'jupyter_core', 'jupyter_core.version', 'jupyter_core.paths', 'jupyter_client.launcher', 'traitlets.log', 'jupyter_client.client', 'jupyter_client.channels', 'jupyter_client.channelsabc', 'jupyter_client.clientabc', 'jupyter_client.manager', 'jupyter_client.kernelspec', 'jupyter_client.managerabc', 'jupyter_client.blocking', 'jupyter_client.blocking.client', 'jupyter_client.blocking.channels', 'jupyter_client.multikernelmanager', 'uuid', 'ipykernel.kernelapp', 'tornado', 'tornado.ioloop', 'tornado.concurrent', 'tornado.log', 'logging.handlers', 'tornado.escape', 'tornado.util', 'tornado.speedups', 'colorama', 'colorama.initialise', 'colorama.ansitowin32', 'colorama.ansi', 'colorama.winterm', 'colorama.win32', 'zmq.eventloop', 'zmq.eventloop.ioloop', 'tornado.platform', 'tornado.platform.asyncio', 'tornado.gen', 'zmq.eventloop.zmqstream', 'ipykernel.iostream', 'imp', 'jupyter_client.session', 'hmac', 'jupyter_client.jsonutil', 'dateutil', 'dateutil._version', 'dateutil.parser', 'dateutil.parser._parser', 'dateutil.relativedelta', 'dateutil._common', 'dateutil.tz', 'dateutil.tz.tz', 'dateutil.tz._common', 'dateutil.tz._factories', 'dateutil.tz.win', 'dateutil.parser.isoparser', '_strptime', 'jupyter_client.adapter', 'ipykernel.heartbeat', 'ipykernel.ipkernel', 'IPython.utils.tokenutil', 'ipykernel.comm', 'ipykernel.comm.manager', 'ipykernel.comm.comm', 'ipykernel.kernelbase', 'tornado.queues', 'tornado.locks', 'ipykernel.jsonutil', 'ipykernel.zmqshell', 'IPython.core.payloadpage', 'ipykernel.displayhook', 'ipykernel.eventloops', 'distutils', 'distutils.version', 'ipykernel.parentpoller', 'win32api', 'win32security', 'ntsecuritycon', 'faulthandler', 'ipykernel.datapub', 'ipykernel.serialize', 'ipykernel.pickleutil', 'ipykernel.codeutil', 'IPython.core.completerlib', 'storemagic', 'ipywidgets', 'ipywidgets._version', 'ipywidgets.widgets', 'ipywidgets.widgets.widget', 'ipywidgets.widgets.domwidget', 'ipywidgets.widgets.trait_types', 'ipywidgets.widgets.util', 'ipywidgets.widgets.widget_layout', 'ipywidgets.widgets.widget_style', 'ipywidgets.widgets.valuewidget', 'ipywidgets.widgets.widget_core', 'ipywidgets.widgets.widget_bool', 'ipywidgets.widgets.widget_description', 'ipywidgets.widgets.widget_button', 'ipywidgets.widgets.widget_box', 'ipywidgets.widgets.docutils', 'ipywidgets.widgets.widget_float', 'ipywidgets.widgets.widget_int', 'ipywidgets.widgets.widget_color', 'ipywidgets.widgets.widget_date', 'ipywidgets.widgets.widget_output', 'ipywidgets.widgets.widget_selection', 'ipywidgets.widgets.widget_selectioncontainer', 'ipywidgets.widgets.widget_string', 'ipywidgets.widgets.widget_controller', 'ipywidgets.widgets.interaction', 'ipywidgets.widgets.widget_link', 'ipywidgets.widgets.widget_media', 'ipywidgets.widgets.widget_templates', 'ipywidgets.widgets.widget_upload', 'matplotlib', 'matplotlib.cbook', 'gzip', 'numpy', 'numpy._globals', 'numpy.__config__', 'numpy.version', 'numpy._distributor_init', 'numpy.core', 'numpy.core.info', 'numpy.core.multiarray', 'numpy.core.overrides', 'numpy.core._multiarray_umath', 'numpy.compat', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.core.umath', 'numpy.core.numerictypes', 'numpy.core._string_helpers', 'numpy.core._type_aliases', 'numpy.core._dtype', 'numpy.core.numeric', 'numpy.core._exceptions', 'numpy.core._asarray', 'numpy.core._ufunc_config', 'numpy.core.fromnumeric', 'numpy.core._methods', 'numpy.core.arrayprint', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.shape_base', 'numpy.core.einsumfunc', 'numpy.core._add_newdocs', 'numpy.core._multiarray_tests', 'numpy.core._dtype_ctypes', 'numpy.core._internal', 'numpy._pytesttester', 'numpy.lib', 'numpy.lib.info', 'numpy.lib.type_check', 'numpy.lib.ufunclike', 'numpy.lib.index_tricks', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'numpy.linalg', 'numpy.linalg.info', 'numpy.linalg.linalg', 'numpy.lib.twodim_base', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.lib.function_base', 'numpy.lib.histograms', 'numpy.lib.stride_tricks', 'numpy.lib.mixins', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.scimath', 'numpy.lib.polynomial', 'numpy.lib.utils', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.financial', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.fft', 'numpy.fft._pocketfft', 'numpy.fft._pocketfft_internal', 'numpy.fft.helper', 'numpy.polynomial', 'numpy.polynomial.polynomial', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.random', 'numpy.random._pickle', 'numpy.random.mtrand', 'numpy.random.common', 'numpy.random.bounded_integers', 'numpy.random.mt19937', 'numpy.random.bit_generator', 'secrets', 'numpy.random.philox', 'numpy.random.pcg64', 'numpy.random.sfc64', 'numpy.random.generator', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'numpy.ma.extras', 'numpy.testing', 'unittest', 'unittest.result', 'unittest.util', 'unittest.case', 'unittest.suite', 'unittest.loader', 'unittest.main', 'unittest.runner', 'unittest.signals', 'numpy.testing._private', 'numpy.testing._private.utils', 'numpy.testing._private.decorators', 'numpy.testing._private.nosetester', 'matplotlib.cbook.deprecation', 'matplotlib.rcsetup', 'matplotlib.fontconfig_pattern', 'pyparsing', 'matplotlib.colors', 'matplotlib._color_data', 'cycler', 'matplotlib._version', 'matplotlib.ft2font', 'kiwisolver']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-18 10:14:17,618 : DEBUG : CACHEDIR=C:\\Users\\rojin\\.matplotlib\n",
      "2020-03-18 10:14:17,638 : DEBUG : Using fontManager instance from C:\\Users\\rojin\\.matplotlib\\fontlist-v310.json\n",
      "2020-03-18 10:14:18,340 : DEBUG : Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "2020-03-18 10:14:18,352 : DEBUG : Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "2020-03-18 10:14:18,361 : DEBUG : Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)\n",
    "\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "from scipy import sparse as sp\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>1/26/2011</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>7/27/2011</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>6/14/2012</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>5/27/2010</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id       date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  1/26/2011  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  7/27/2011  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  6/14/2012  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  5/27/2010  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw   1/5/2012  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text                 user_id  \\\n",
       "0  My wife took me here on my birthday for breakf...  rLtl8ZkDX5vH5nAx9C3q5Q   \n",
       "1  I have no idea why some people give bad review...  0a2KyEL0d3Yb1V6aivbIuQ   \n",
       "2  love the gyro plate. Rice is so good and I als...  0hT2KtfLiobPvh6cDC8JQg   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  uZetl9T0NcROGOyFfughhg   \n",
       "4  General Manager Scott Petello is a good egg!!!...  vYmM4KTsC8ZfQBg-j5MWkw   \n",
       "\n",
       "   cool  useful  funny  \n",
       "0     2       5      0  \n",
       "1     0       0      0  \n",
       "2     0       1      0  \n",
       "3     1       2      0  \n",
       "4     0       0      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/yelp_academic_dataset_review_10000.csv') # Read data into pandas dataframe\n",
    "\n",
    "df.head() # Quick check of the data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 9 columns):\n",
      "business_id    10000 non-null object\n",
      "date           10000 non-null object\n",
      "review_id      10000 non-null object\n",
      "stars          10000 non-null int64\n",
      "text           10000 non-null object\n",
      "user_id        10000 non-null object\n",
      "cool           10000 non-null int64\n",
      "useful         10000 non-null int64\n",
      "funny          10000 non-null int64\n",
      "dtypes: int64(4), object(5)\n",
      "memory usage: 703.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() # View data description (Total rows, Column names, type and number of non-null values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the data:  (10000, 9)\n",
      "No. of Rows: 10000\n",
      "No. of Columns: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of the data: \", df.shape) # View data dimension\n",
    "\n",
    "no_of_rows = df.shape[0]\n",
    "no_of_columns = df.shape[1]\n",
    "\n",
    "print(\"No. of Rows: %d\" % no_of_rows)\n",
    "print(\"No. of Columns: %d\" % no_of_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the Text column into a 2D Array of Documents\n",
    "\n",
    "We convert the documents from the text column to an array of documents.\n",
    "\n",
    "It's a 2D array in which each row reprents a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the documents array:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "docs_array = array(df['text']) # Convert the 'text' column into array\n",
    "\n",
    "print(\"Dimension of the documents array: \", docs_array.shape) # View dimensions of new array\n",
    "\n",
    "#print(docs_array[0]) # Display the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the Data\n",
    "\n",
    "Pre-processing of the text data is done using the following steps:\n",
    "\n",
    "- Convert to lowercase \n",
    "- Tokenize (split the documents into tokens or words)\n",
    "- Remove numbers, but not words that contain numbers\n",
    "- Remove words that are only a single character\n",
    "- Lemmatize the tokens/words\n",
    "\n",
    "\n",
    "### Tokenization and Lemmatization\n",
    "\n",
    "We convert all the words into lowercase then tokenize each word using NLTK Regular-Expression Tokenizer class \"RegexpTokenizer\". It splits a given string to substrings using a regular expression. Then we remove numbers and single character words since they usually don't impart much useful information and are very high in number. Then, we lemmatize the tokens using WordNetLemmatizer from NLTK, where we extract the root words of the tokens using the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def docs_preprocessor(docs):\n",
    "    '''Function to Convert the 2D Document Array into a 2D Array of Processed Words'''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # Tokenize the words\n",
    "    \n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert doc to lowercase\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split doc into words\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers\n",
    "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words with only one character\n",
    "    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the 2D Document Array into a 2D Array of Tokenized Words using the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-32b9a4e5a540>\u001b[0m in \u001b[0;36mdocs_preprocessor\u001b[1;34m(docs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Convert doc to lowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Split doc into words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the 2D Array of Tokenized Documents:  10000\n"
     ]
    }
   ],
   "source": [
    "%time docs = docs_preprocessor(docs_array)\n",
    "print(\"Length of the 2D Array of Tokenized Documents: \", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the 2D Array of Tokenized Documents:  10000\n",
      "[['wife', 'took', 'here', 'birthday', 'breakfast', 'excellent', 'weather', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'ground', 'absolute', 'pleasure', 'waitress', 'excellent', 'food', 'arrived', 'quickly', 'semi', 'busy', 'saturday', 'morning', 'looked', 'like', 'place', 'fill', 'pretty', 'quickly', 'earlier', 'here', 'better', 'yourself', 'favor', 'their', 'bloody', 'mary', 'phenomenal', 'simply', 'best', 'ever', 'pretty', 'sure', 'they', 'only', 'ingredient', 'from', 'their', 'garden', 'blend', 'them', 'fresh', 'when', 'order', 'amazing', 'while', 'everything', 'menu', 'look', 'excellent', 'white', 'truffle', 'scrambled', 'egg', 'vegetable', 'skillet', 'tasty', 'delicious', 'came', 'with', 'piece', 'their', 'griddled', 'bread', 'with', 'amazing', 'absolutely', 'made', 'meal', 'complete', 'best', 'toast', 'ever', 'anyway', 'wait', 'back'], ['have', 'idea', 'some', 'people', 'give', 'review', 'about', 'this', 'place', 'go', 'show', 'please', 'everyone', 'they', 'probably', 'griping', 'about', 'something', 'that', 'their', 'fault', 'there', 'many', 'people', 'like', 'that', 'case', 'friend', 'arrived', 'about', 'this', 'past', 'sunday', 'pretty', 'crowded', 'more', 'than', 'thought', 'sunday', 'evening', 'thought', 'would', 'have', 'wait', 'forever', 'seat', 'they', 'said', 'seated', 'when', 'girl', 'come', 'back', 'from', 'seating', 'someone', 'else', 'were', 'seated', 'waiter', 'came', 'drink', 'order', 'everyone', 'very', 'pleasant', 'from', 'host', 'that', 'seated', 'waiter', 'server', 'price', 'were', 'very', 'good', 'well', 'placed', 'order', 'once', 'decided', 'what', 'wanted', 'shared', 'baked', 'spaghetti', 'calzone', 'small', 'here', 'beef', 'pizza', 'both', 'them', 'calzone', 'huge', 'smallest', 'personal', 'small', 'pizza', 'both', 'were', 'awesome', 'friend', 'liked', 'pizza', 'better', 'liked', 'calzone', 'better', 'calzone', 'doe', 'have', 'sweetish', 'sauce', 'that', 'like', 'sauce', 'part', 'pizza', 'take', 'home', 'were', 'door', 'everything', 'great', 'like', 'these', 'reviewer', 'that', 'go', 'show', 'that', 'have', 'these', 'thing', 'yourself', 'because', 'these', 'reviewer', 'have', 'some', 'serious', 'issue']]\n"
     ]
    }
   ],
   "source": [
    "print(docs[0:2]) #Display the first two documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Bigrams/Trigrams:\n",
    "\n",
    "N-grams are combinations of adjacent words or letters of length 'n' that you can find in your source text. These combinations of words carry a special meaning. For example: car-pool is an n-gram formed using the two words car and pool that carries a distinct meaning different from the individual words. \n",
    "\n",
    "If n=2, it is called a Bigram and if n=3, it is called a Trigram.\n",
    "\n",
    "We find all the combinations of Bigrams and Trigrams. Then, we keep only the frequent phrases. We finally add the frequent phrases to the original data, since we would like to keep the words “car” and “pool” as well as the bigram “car_pool”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-18 10:14:52,010 : INFO : collecting all words and their counts\n",
      "2020-03-18 10:14:52,011 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-03-18 10:14:54,643 : INFO : collected 410104 word types from a corpus of 733158 words (unigram + bigrams) and 10000 sentences\n",
      "2020-03-18 10:14:54,645 : INFO : using 410104 counts as vocab in Phrases<0 vocab, min_count=300, threshold=10.0, max_vocab_size=40000000>\n",
      "2020-03-18 10:14:54,647 : INFO : collecting all words and their counts\n",
      "2020-03-18 10:14:54,649 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-03-18 10:15:04,255 : INFO : collected 411739 word types from a corpus of 728342 words (unigram + bigrams) and 10000 sentences\n",
      "2020-03-18 10:15:04,258 : INFO : using 411739 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "bigram = Phrases(docs, min_count=300) # Add bigrams (if appears 300 times or more)\n",
    "trigram = Phrases(bigram[docs], min_count=300) # Add trigrams (if appears 300 times or more)\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            docs[idx].append(token)  # Token is a bigram, add to document\n",
    "    for token in trigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            docs[idx].append(token)  # Token is a trigram, add to document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-18 10:15:17,266 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-03-18 10:15:20,279 : INFO : built Dictionary(26643 unique tokens: ['absolute', 'absolutely', 'amazing', 'anyway', 'arrived']...) from 10000 documents (total 799182 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in initital documents: 26643\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs) # Create a dictionary representation of the documents\n",
    "print('Number of unique words in initital documents:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute\n",
      "absolutely\n",
      "amazing\n",
      "anyway\n",
      "arrived\n",
      "back\n",
      "best\n",
      "best_ever\n",
      "better\n",
      "birthday\n",
      "blend\n",
      "bloody\n",
      "bloody_mary\n",
      "bread\n",
      "breakfast\n",
      "busy\n",
      "came\n",
      "complete\n",
      "delicious\n",
      "earlier\n",
      "egg\n",
      "ever\n",
      "everything\n",
      "excellent\n",
      "favor\n",
      "fill\n",
      "food\n",
      "fresh\n",
      "from\n",
      "garden\n",
      "griddled\n",
      "ground\n",
      "here\n",
      "ingredient\n",
      "like\n",
      "look\n",
      "looked\n",
      "looked_like\n",
      "made\n",
      "mary\n",
      "meal\n",
      "menu\n",
      "morning\n",
      "only\n",
      "order\n",
      "outside\n",
      "overlooking\n",
      "perfect\n",
      "phenomenal\n",
      "piece\n",
      "place\n",
      "pleasure\n",
      "pretty\n",
      "pretty_quickly\n",
      "quickly\n",
      "saturday\n",
      "saturday_morning\n",
      "scrambled\n",
      "scrambled_egg\n",
      "semi\n",
      "simply\n",
      "sitting\n",
      "sitting_outside\n",
      "skillet\n",
      "sure\n",
      "tasty\n",
      "their\n",
      "them\n",
      "they\n",
      "toast\n",
      "took\n",
      "truffle\n",
      "vegetable\n",
      "wait\n",
      "waitress\n",
      "weather\n",
      "when\n",
      "which\n",
      "while\n",
      "white\n",
      "wife\n",
      "with\n",
      "yourself\n",
      "yourself_favor\n",
      "about\n",
      "awesome\n",
      "baked\n",
      "because\n",
      "beef\n",
      "both\n",
      "calzone\n",
      "case\n",
      "come\n",
      "come_back\n",
      "crowded\n",
      "decided\n",
      "doe\n",
      "door\n",
      "drink\n",
      "else\n",
      "evening\n",
      "everyone\n",
      "fault\n",
      "forever\n",
      "friend\n",
      "girl\n",
      "give\n",
      "go\n",
      "good\n",
      "great\n",
      "griping\n",
      "have\n",
      "home\n",
      "host\n",
      "huge\n",
      "idea\n",
      "issue\n",
      "liked\n",
      "many\n",
      "many_people\n",
      "more\n",
      "more_than\n",
      "once\n",
      "part\n",
      "past\n",
      "people\n",
      "personal\n",
      "pizza\n",
      "placed\n",
      "placed_order\n",
      "pleasant\n",
      "please\n",
      "price\n",
      "probably\n",
      "review\n",
      "reviewer\n",
      "said\n",
      "sauce\n",
      "seat\n",
      "seated\n",
      "seating\n",
      "serious\n",
      "server\n",
      "shared\n",
      "show\n",
      "small\n",
      "smallest\n",
      "some\n",
      "some_serious\n",
      "someone\n",
      "someone_else\n",
      "something\n",
      "spaghetti\n",
      "sunday\n",
      "sunday_evening\n",
      "sweetish\n",
      "take\n",
      "take_home\n",
      "than\n",
      "that\n",
      "there\n",
      "these\n",
      "thing\n",
      "this\n",
      "this_place\n",
      "thought\n",
      "very\n",
      "very_pleasant\n",
      "waiter\n",
      "wanted\n",
      "well\n",
      "were\n",
      "were_seated\n",
      "what\n",
      "would\n",
      "also\n",
      "candy\n",
      "gyro\n",
      "love\n",
      "plate\n",
      "rice\n",
      "selection\n",
      "area\n",
      "ballpark\n",
      "baseball\n",
      "can\n",
      "chaparral\n",
      "clean\n",
      "convenient\n",
      "dakota\n",
      "dept\n",
      "desert\n",
      "dog\n",
      "duck\n",
      "fenced\n",
      "field\n",
      "find\n",
      "keeping\n",
      "lake\n",
      "located\n",
      "mitt\n",
      "over\n",
      "park\n",
      "path\n",
      "pick\n",
      "play\n",
      "poopy\n",
      "rosie\n",
      "scottsdale\n",
      "shaded\n",
      "sniff\n",
      "surrounded\n",
      "trash\n",
      "trash_can\n",
      "very_convenient\n",
      "wonderful\n",
      "xeriscape\n",
      "albeit\n",
      "always\n",
      "assure\n",
      "customer\n",
      "detail\n",
      "general\n",
      "general_manager\n",
      "important\n",
      "inevitable\n",
      "into\n",
      "just\n",
      "life\n",
      "manager\n",
      "mistake\n",
      "petello\n",
      "rare\n",
      "recover\n",
      "respect\n",
      "satisfied\n",
      "scott\n",
      "speak\n",
      "staff\n",
      "state\n",
      "surprised\n",
      "thanks\n",
      "totally\n",
      "treat\n",
      "walk\n",
      "your\n",
      "after\n",
      "aioli\n",
      "almost\n",
      "anew\n",
      "another\n",
      "apologized\n",
      "apparently\n",
      "apple\n",
      "beautiful\n",
      "before\n",
      "bill\n",
      "bill_came\n",
      "bothered\n",
      "bring\n",
      "butter\n",
      "cake\n",
      "chef\n",
      "communicated\n",
      "comped\n",
      "couple\n",
      "couple_day\n",
      "crisp\n",
      "crudites\n",
      "day\n",
      "definitely\n",
      "dessert\n",
      "didn\n",
      "didn_disappoint\n",
      "didn_even\n",
      "disappoint\n",
      "earthy\n",
      "echo\n",
      "enough\n",
      "entree\n",
      "even\n",
      "explain\n",
      "fairly\n",
      "farm\n",
      "feeling\n",
      "finished\n",
      "five\n",
      "five_star\n",
      "foccacia\n",
      "forgot\n",
      "forgot_bring\n",
      "fresh_veggie\n",
      "freshness\n",
      "full\n",
      "gingerbread\n",
      "glass\n",
      "glass_wine\n",
      "grapefruit\n",
      "honey\n",
      "impressed\n"
     ]
    }
   ],
   "source": [
    "for i in range (0 , 100):\n",
    "    print(dictionary[i]) #View first 100 words in the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Rare and Common Tokens/Words\n",
    "\n",
    "Now we remove in-frequent words from our dictionary. We also remove words that appear frequently in most documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-18 10:15:20,442 : INFO : discarding 26290 tokens: [('absolute', 55), ('absolutely', 298), ('anyway', 238), ('arrived', 245), ('back', 2326), ('best_ever', 62), ('birthday', 170), ('blend', 44), ('bloody', 46), ('bloody_mary', 37)]...\n",
      "2020-03-18 10:15:20,444 : INFO : keeping 353 tokens which were in no less than 300 and no more than 2000 (=20.0%) documents\n",
      "2020-03-18 10:15:20,463 : DEBUG : rebuilding dictionary, shrinking gaps\n",
      "2020-03-18 10:15:20,469 : INFO : resulting dictionary: Dictionary(353 unique tokens: ['amazing', 'best', 'better', 'bread', 'breakfast']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words after removing rare and common words: 353\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur less than 300 documents, or more than 20% of the documents\n",
    "dictionary.filter_extremes(no_below=300, no_above=0.20) \n",
    "\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Representation of Data\n",
    "\n",
    "\n",
    "Finally, we transform the documents to a **vectorized form**. \n",
    "\n",
    "We simply compute the frequency of each word, including the bigrams/trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 353\n",
      "Number of documents: 10000\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs] # Bag-of-words representation of the docs\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LDA Model\n",
    "\n",
    "We use the gensim.models.LdaModel class for performing LDA. [https://radimrehurek.com/gensim/models/ldamodel.html]\n",
    "\n",
    "\n",
    "#### Below we discuss the setting of some of the key parameters.\n",
    "\n",
    "- num_topics (int, optional) – The number of requested latent topics to be extracted from the training corpus.\n",
    "\n",
    " \n",
    "LDA is an unsupervised technique, meaning that we don't know prior to running the model how many topics exits in our corpus. It depends on the data and the application. We may use the following two technique to determine the number of topics.\n",
    "\n",
    "\n",
    "        Technique 1: Topic Coherence \n",
    "The main technique to determine the number of topics is **Topic coherence** [http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf]\n",
    "\n",
    "\n",
    "        Technique 2: Visualizing Inter-Topic Distance \n",
    "Use the LDA visualization tool pyLDAvis to observe Intertopic Distance Map (discussed later). By varying the number of topics we could determine the optimal value from the visualization.\n",
    "\n",
    "We **use both techniques** to determine the optimal number of topics.\n",
    "\n",
    "- chunksize (int, optional) – Number of documents to be used in each training chunk.\n",
    "\n",
    "It controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory. \n",
    "\n",
    "We set chunksize = 10000, which is equal to the amount of documents. Thus, it processes all the data in one go. Chunksize can however influence the quality of the model.\n",
    "\n",
    "- passes (int, optional) – Number of passes through the corpus during training.\n",
    "\n",
    "It controls how often we train the model on the entire corpus. Another word for passes might be “epochs”. \n",
    "\n",
    "- iterations (int, optional) – Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
    "\n",
    "It is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. \n",
    "\n",
    "        It is important to set the number of “passes” and “iterations” high enough.\n",
    "        \n",
    "\n",
    "#### How to Set \"passes\" and \"iterations\":\n",
    "\n",
    "First, enable logging and set eval_every = 1 (however, it might slow down, so, we use None) in LdaModel. \n",
    "\n",
    "When training the model look for a line in the log that looks something like this:\n",
    "\n",
    "        2020-02-25 19:07:04,716 : DEBUG : 9985/10000 documents converged within 300 iterations\n",
    "\n",
    "If we set passes = 20, we will see this line 20 times. \n",
    "\n",
    "### Important: We need to make sure that by the final passes, most of the documents have converged. Thus, want to choose both passes and iterations to be high enough for this to happen.\n",
    "\n",
    "- eval_every (int, optional) – Log perplexity is estimated every that many updates. Setting this to 1 slows down training by ~2x.\n",
    "\n",
    "\n",
    "- alpha ({numpy.ndarray, str}, optional): Can be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. \n",
    "\n",
    "Alternatively default prior selecting strategies can be employed by supplying a string:\n",
    "\n",
    "        ’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.\n",
    "\n",
    "        ’auto’: Learns an asymmetric prior from the corpus (not available if distributed==True).\n",
    "        \n",
    "        \n",
    "- eta ({float, np.array, str}, optional) – A-priori belief on word probability.\n",
    "\n",
    "It can be:\n",
    "\n",
    "        scalar for a symmetric prior over topic/word probability,\n",
    "\n",
    "        vector of length num_words to denote an asymmetric user defined probability for each word,\n",
    "\n",
    "        matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
    "\n",
    "        the string ‘auto’ to learn the asymmetric prior from the data.\n",
    "\n",
    "\n",
    "We set alpha = 'auto' and eta = 'auto'. Again this is somewhat technical, but essentially we are automatically learning two parameters in the model that we usually would have to specify explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "#------Set training parameters\n",
    "num_topics = 14 # Number of topics to discover\n",
    "chunksize = 10000 # Size of the doc looked at every pass\n",
    "passes = 20 # Number of passes through the corpus\n",
    "iterations = 300 # Maximum number of iterations through the corpus when inferring the topic distribution of a corpus\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "#-------Make an index to word dictionary\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 1 for Determining Optimal Number of Topics: Topic Coherence\n",
    "\n",
    "Topic Coherence is a measure used to evaluate topic models. Each such generated topic consists of words, and the topic coherence is applied to the top N words from the topic. \n",
    "\n",
    "Topic Coherence measures score a single topic by **measuring the degree of semantic similarity between high scoring words in the topic**. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. \n",
    "\n",
    "A set of statements or facts is said to be coherent, if they support each other. Thus, a coherent fact set can be interpreted in a context that covers all or most of the facts. An example of a coherent fact set is “the game is a team sport”, “the game is played with a ball”, “the game demands great physical efforts”\n",
    "\n",
    "Topic Coherence is defined as the average of the pairwise word-similarity scores of the words in the topic.\n",
    "\n",
    "A good model will generate coherent topics, i.e., topics with high topic coherence scores. Good topics are topics that can be described by a short label, therefore this is what the topic coherence measure should capture.\n",
    "\n",
    "\n",
    "Below we display \n",
    "- the average topic coherence and\n",
    "- print the topics in order of topic coherence\n",
    "\n",
    "\n",
    "We use LdaModel's \"top_topics\" method to get the topics with the highest coherence score the coherence for each topic.\n",
    "\n",
    "Note that we use the “Umass” topic coherence measure here (see gensim.models.ldamodel.LdaModel.top_topics())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 2 for Determining Optimal Number of Topics: Visualization\n",
    "\n",
    "We use **pyLDAvis** to interpret the topics in a topic model that has been fit to a corpus of text data. \n",
    "\n",
    "It extracts information from a fitted LDA topic model to inform an interactive web-based visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "pyLDAvis.gensim.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Interpretation of the Visualization \n",
    "\n",
    "\n",
    "\n",
    "- Left Panel: \n",
    "The labeld Intertopic Distance Map, circles represent different topics and the distance between them. Similar topics appear closer and the dissimilar topics farther. The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus. An individual topic may be selected for closer scrutiny by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left.\n",
    "\n",
    "\n",
    "\n",
    "- Right Panel:\n",
    "It includes the bar chart of the top 30 terms. When no topic is selected in the plot on the left, the bar chart shows the top-30 most \"salient\" terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics. Selecting each topic on the right, modifies the bar chart to show the \"relevant\" terms for the selected topic. \n",
    "\n",
    "Relevence is defined as in footer 2 and can be tuned by parameter $\\lambda$.\n",
    "- Smaller $\\lambda$ gives higher weight to the term's distinctiveness.\n",
    "- larger $\\lambda$ corresponds to probablity of the term occurance per topics.\n",
    "\n",
    "Therefore, to get a better sense of terms per topic we use $\\lambda = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Top Words in the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_topics(model, num_topics, top_words):\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = top_words);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lda_topics(model, num_topics, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Labels for the Topics\n",
    "\n",
    "We can manually generate human-interpretable labels for each topic by looking at the terms that appear more in each topic.\n",
    "\n",
    "\n",
    "We use LdaModel's \"show_topic\" method that returns **Word-probability pairs** for the most relevant words generated by the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, a topic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:30} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_summaries = []\n",
    "\n",
    "print(u'{:25} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "\n",
    "for i in range(num_topics):\n",
    "    print('\\nTopic '+str(i)+' |---------------------------\\n')\n",
    "    tmp = explore_topic(model,topic_number=i, topn=10, output=True )\n",
    "#     print tmp[:5]\n",
    "    topic_summaries += [tmp[:5]]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Generate Topic Labels\n",
    "\n",
    "Based on the most probable words generated by each topic, we assign human-interpretable labels for the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_labels = {0: 'Asian Cuisine', 1:'Mall', 2:'First Visit', 3:'Customer Service', 4:'Comparison', 5:'Store', 6:'Pizza', 7:'Night', 8:'Happy Hour'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
