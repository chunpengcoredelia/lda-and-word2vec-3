{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Note: This notebook is inspired by the Topic-Modeling-Latent-Dirichlet-Allocation series at: https://github.com/rhasanbd/Topic-Modeling-Latent-Dirichlet-Allocation </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation - Implementation on Yelp dataset\n",
    "\n",
    "In this notebook, we implement Latent Dirichlet Allocation(LDA) on the Yelp reviews data to carry out Topic Modelling. We use the Gensim topic modelling API https://radimrehurek.com/gensim/models/ldamodel.html. Scikit-Learn implementation is also available (we use Gensim since it provides more functionality and application like Topic Coherence Pipeline or Dynamic Topic Modeling.)\n",
    "\n",
    "We build an **end-to-end Natural Language Processing (NLP) pipeline**, starting with raw data and running through preparing, modeling, visualization.\n",
    "The steps that we will carry out involves the following:\n",
    "1. Exploratory Data Analysis\n",
    "2. Data Cleaning and Pre-processing\n",
    "3. Topic modeling with LDA\n",
    "4. Determine optimal number of Topics\n",
    "5. Visualize topic model using pyLDAvis\n",
    "\n",
    "### Yelp Review Dataset\n",
    "The Yelp Review Dataset is a CSV file that contains a sub-sample of 10,000 reviews extracted from the Yelp dataset available at: https://www.yelp.com/dataset.\n",
    "\n",
    "The review dataset contains the following fields:\n",
    "- business_id : Unique identifier of business\n",
    "- date : Data of review posted YYYY-MM-DD\n",
    "- review_id : Unique identifier of review\n",
    "- stars : Star rating (upto 4 stars)\n",
    "- text : Review text\n",
    "- user_id : Unique identifier of user who posted the review\n",
    "- cool : Number of cool votes received\n",
    "- useful : Number of useful votes received\n",
    "- funny : Number of funny votes received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 21:25:41,307 : DEBUG : $HOME=C:\\Users\\rojin\n",
      "2020-03-17 21:25:41,310 : DEBUG : CONFIGDIR=C:\\Users\\rojin\\.matplotlib\n",
      "2020-03-17 21:25:41,311 : DEBUG : matplotlib data path: c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\mpl-data\n",
      "2020-03-17 21:25:41,315 : DEBUG : loaded rc file c:\\users\\rojin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\matplotlib\\mpl-data\\matplotlibrc\n",
      "2020-03-17 21:25:41,319 : DEBUG : matplotlib version 3.1.2\n",
      "2020-03-17 21:25:41,320 : DEBUG : interactive is False\n",
      "2020-03-17 21:25:41,320 : DEBUG : platform is win32\n",
      "2020-03-17 21:25:41,321 : DEBUG : loaded modules: ['sys', 'builtins', '_frozen_importlib', '_imp', '_thread', '_warnings', '_weakref', 'zipimport', '_frozen_importlib_external', '_io', 'marshal', 'nt', 'winreg', 'encodings', 'codecs', '_codecs', 'encodings.aliases', 'encodings.utf_8', '_signal', '__main__', 'encodings.latin_1', 'io', 'abc', '_abc', '_bootlocale', '_locale', 'encodings.cp1252', 'site', 'os', 'stat', '_stat', 'ntpath', 'genericpath', 'os.path', '_collections_abc', '_sitebuiltins', 'types', 'importlib', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib.util', 'importlib.abc', 'importlib.machinery', 'contextlib', 'collections', 'operator', '_operator', 'keyword', 'heapq', '_heapq', 'itertools', 'reprlib', '_collections', 'functools', '_functools', 'mpl_toolkits', 'pywin32_bootstrap', 'runpy', 'pkgutil', 'weakref', '_weakrefset', 'ipykernel', 'ipykernel._version', 'ipykernel.connect', '__future__', 'json', 'json.decoder', 're', 'enum', 'sre_compile', '_sre', 'sre_parse', 'sre_constants', 'copyreg', 'json.scanner', '_json', 'json.encoder', 'subprocess', 'time', 'signal', 'errno', 'threading', 'traceback', 'linecache', 'tokenize', 'token', 'msvcrt', '_winapi', 'IPython', 'IPython.core', 'IPython.core.getipython', 'IPython.core.release', 'IPython.core.application', 'atexit', 'copy', 'glob', 'fnmatch', 'posixpath', 'logging', 'collections.abc', 'string', '_string', 'shutil', 'zlib', 'bz2', '_compression', '_bz2', 'lzma', '_lzma', 'traitlets', 'traitlets.traitlets', 'inspect', 'dis', 'opcode', '_opcode', 'six', 'struct', '_struct', 'traitlets.utils', 'traitlets.utils.getargspec', 'traitlets.utils.importstring', 'ipython_genutils', 'ipython_genutils._version', 'ipython_genutils.py3compat', 'ipython_genutils.encoding', 'locale', 'platform', 'traitlets.utils.sentinel', 'traitlets.utils.bunch', 'traitlets._version', 'traitlets.config', 'traitlets.config.application', 'decorator', 'traitlets.config.configurable', 'traitlets.config.loader', 'argparse', 'gettext', 'ast', '_ast', 'ipython_genutils.path', 'random', 'math', 'hashlib', '_hashlib', '_blake2', '_sha3', 'bisect', '_bisect', '_random', 'ipython_genutils.text', 'textwrap', 'ipython_genutils.importstring', 'IPython.core.crashhandler', 'pprint', 'IPython.core.ultratb', 'pydoc', 'urllib', 'urllib.parse', 'IPython.core.debugger', 'bdb', 'IPython.utils', 'IPython.utils.PyColorize', 'IPython.utils.coloransi', 'IPython.utils.ipstruct', 'IPython.utils.colorable', 'pygments', 'pygments.util', 'IPython.utils.py3compat', 'IPython.utils.encoding', 'IPython.core.excolors', 'IPython.testing', 'IPython.testing.skipdoctest', 'pdb', 'cmd', 'code', 'codeop', 'IPython.core.display_trap', 'IPython.utils.path', 'IPython.utils.process', 'IPython.utils._process_win32', 'ctypes', '_ctypes', 'ctypes._endian', 'ctypes.wintypes', 'IPython.utils._process_common', 'shlex', 'IPython.utils.decorators', 'IPython.utils.data', 'IPython.utils.terminal', 'IPython.utils.sysinfo', 'IPython.utils._sysinfo', 'IPython.core.profiledir', 'IPython.paths', 'tempfile', 'IPython.utils.importstring', 'IPython.terminal', 'IPython.terminal.embed', 'IPython.core.compilerop', 'IPython.core.magic_arguments', 'IPython.core.error', 'IPython.utils.text', 'pathlib', 'IPython.core.magic', 'getopt', 'IPython.core.oinspect', 'IPython.core.page', 'IPython.core.display', 'binascii', 'mimetypes', 'IPython.lib', 'IPython.lib.security', 'getpass', 'IPython.lib.pretty', 'datetime', '_datetime', 'IPython.utils.openpy', 'IPython.utils.dir2', 'IPython.utils.wildcard', 'pygments.lexers', 'pygments.lexers._mapping', 'pygments.modeline', 'pygments.plugin', 'pygments.lexers.python', 'pygments.lexer', 'pygments.filter', 'pygments.filters', 'pygments.token', 'pygments.regexopt', 'pygments.unistring', 'pygments.formatters', 'pygments.formatters._mapping', 'pygments.formatters.html', 'pygments.formatter', 'pygments.styles', 'IPython.core.inputtransformer2', 'typing', 'typing.io', 'typing.re', 'IPython.core.interactiveshell', 'asyncio', 'asyncio.base_events', 'concurrent', 'concurrent.futures', 'concurrent.futures._base', 'socket', '_socket', 'selectors', 'select', 'ssl', '_ssl', 'base64', 'asyncio.constants', 'asyncio.coroutines', 'asyncio.base_futures', 'asyncio.format_helpers', 'asyncio.log', 'asyncio.events', 'contextvars', '_contextvars', 'asyncio.base_tasks', '_asyncio', 'asyncio.futures', 'asyncio.protocols', 'asyncio.sslproto', 'asyncio.transports', 'asyncio.tasks', 'asyncio.locks', 'asyncio.runners', 'asyncio.queues', 'asyncio.streams', 'asyncio.subprocess', 'asyncio.windows_events', '_overlapped', 'asyncio.base_subprocess', 'asyncio.proactor_events', 'asyncio.selector_events', 'asyncio.windows_utils', 'pickleshare', 'pickle', '_compat_pickle', '_pickle', 'IPython.core.prefilter', 'IPython.core.autocall', 'IPython.core.macro', 'IPython.core.splitinput', 'IPython.core.alias', 'IPython.core.builtin_trap', 'IPython.core.events', 'backcall', 'backcall.backcall', 'IPython.core.displayhook', 'IPython.core.displaypub', 'IPython.core.extensions', 'IPython.core.formatters', 'IPython.utils.sentinel', 'IPython.core.history', 'sqlite3', 'sqlite3.dbapi2', '_sqlite3', 'IPython.core.logger', 'IPython.core.payload', 'IPython.core.usage', 'IPython.display', 'IPython.lib.display', 'html', 'html.entities', 'IPython.utils.io', 'IPython.utils.capture', 'IPython.utils.strdispatch', 'IPython.core.hooks', 'IPython.utils.syspathcontext', 'IPython.utils.tempdir', 'IPython.utils.contexts', 'IPython.core.async_helpers', 'IPython.terminal.interactiveshell', 'prompt_toolkit', 'prompt_toolkit.application', 'prompt_toolkit.application.application', 'prompt_toolkit.buffer', 'six.moves', 'prompt_toolkit.application.current', 'prompt_toolkit.eventloop', 'prompt_toolkit.eventloop.async_generator', 'queue', '_queue', 'six.moves.queue', 'prompt_toolkit.eventloop.coroutine', 'prompt_toolkit.eventloop.defaults', 'prompt_toolkit.utils', 'wcwidth', 'wcwidth.wcwidth', 'wcwidth.table_wide', 'wcwidth.table_zero', 'prompt_toolkit.cache', 'prompt_toolkit.eventloop.base', 'prompt_toolkit.log', 'prompt_toolkit.eventloop.future', 'prompt_toolkit.eventloop.context', 'prompt_toolkit.eventloop.event', 'prompt_toolkit.application.run_in_terminal', 'prompt_toolkit.auto_suggest', 'prompt_toolkit.filters', 'prompt_toolkit.filters.app', 'prompt_toolkit.enums', 'prompt_toolkit.filters.base', 'prompt_toolkit.filters.cli', 'prompt_toolkit.filters.utils', 'prompt_toolkit.clipboard', 'prompt_toolkit.clipboard.base', 'prompt_toolkit.selection', 'prompt_toolkit.clipboard.in_memory', 'prompt_toolkit.completion', 'prompt_toolkit.completion.base', 'prompt_toolkit.completion.filesystem', 'prompt_toolkit.completion.fuzzy_completer', 'prompt_toolkit.document', 'prompt_toolkit.completion.word_completer', 'prompt_toolkit.history', 'prompt_toolkit.search', 'prompt_toolkit.key_binding', 'prompt_toolkit.key_binding.key_bindings', 'prompt_toolkit.keys', 'prompt_toolkit.key_binding.vi_state', 'prompt_toolkit.validation', 'prompt_toolkit.input', 'prompt_toolkit.input.base', 'prompt_toolkit.input.defaults', 'prompt_toolkit.input.typeahead', 'prompt_toolkit.key_binding.bindings', 'prompt_toolkit.key_binding.bindings.page_navigation', 'prompt_toolkit.key_binding.bindings.scroll', 'prompt_toolkit.key_binding.defaults', 'prompt_toolkit.key_binding.bindings.basic', 'prompt_toolkit.key_binding.key_processor', 'prompt_toolkit.key_binding.bindings.named_commands', 'prompt_toolkit.key_binding.bindings.completion', 'prompt_toolkit.key_binding.bindings.cpr', 'prompt_toolkit.key_binding.bindings.emacs', 'prompt_toolkit.key_binding.bindings.mouse', 'prompt_toolkit.layout', 'prompt_toolkit.layout.containers', 'prompt_toolkit.formatted_text', 'prompt_toolkit.formatted_text.ansi', 'prompt_toolkit.output', 'prompt_toolkit.output.base', 'prompt_toolkit.layout.screen', 'prompt_toolkit.output.color_depth', 'prompt_toolkit.output.defaults', 'prompt_toolkit.output.vt100', 'array', 'prompt_toolkit.styles', 'prompt_toolkit.styles.base', 'prompt_toolkit.styles.defaults', 'prompt_toolkit.styles.named_colors', 'prompt_toolkit.styles.style', 'prompt_toolkit.styles.pygments', 'prompt_toolkit.styles.style_transformation', 'colorsys', 'prompt_toolkit.formatted_text.base', 'prompt_toolkit.formatted_text.html', 'xml', 'xml.dom', 'xml.dom.domreg', 'xml.dom.minidom', 'xml.dom.minicompat', 'xml.dom.xmlbuilder', 'xml.dom.NodeFilter', 'prompt_toolkit.formatted_text.pygments', 'prompt_toolkit.formatted_text.utils', 'prompt_toolkit.mouse_events', 'prompt_toolkit.layout.controls', 'prompt_toolkit.lexers', 'prompt_toolkit.lexers.base', 'prompt_toolkit.lexers.pygments', 'prompt_toolkit.layout.processors', 'prompt_toolkit.layout.utils', 'prompt_toolkit.layout.dimension', 'prompt_toolkit.layout.margins', 'prompt_toolkit.layout.layout', 'prompt_toolkit.layout.menus', 'prompt_toolkit.renderer', 'prompt_toolkit.layout.mouse_handlers', 'prompt_toolkit.key_binding.bindings.vi', 'prompt_toolkit.input.vt100_parser', 'prompt_toolkit.input.ansi_escape_sequences', 'prompt_toolkit.key_binding.digraphs', 'prompt_toolkit.key_binding.emacs_state', 'prompt_toolkit.layout.dummy', 'prompt_toolkit.application.dummy', 'prompt_toolkit.shortcuts', 'prompt_toolkit.shortcuts.dialogs', 'prompt_toolkit.key_binding.bindings.focus', 'prompt_toolkit.widgets', 'prompt_toolkit.widgets.base', 'prompt_toolkit.widgets.toolbars', 'prompt_toolkit.widgets.dialogs', 'prompt_toolkit.widgets.menus', 'prompt_toolkit.shortcuts.progress_bar', 'prompt_toolkit.shortcuts.progress_bar.base', 'prompt_toolkit.shortcuts.progress_bar.formatters', 'prompt_toolkit.shortcuts.prompt', 'prompt_toolkit.key_binding.bindings.auto_suggest', 'prompt_toolkit.key_binding.bindings.open_in_editor', 'prompt_toolkit.shortcuts.utils', 'prompt_toolkit.patch_stdout', 'pygments.style', 'IPython.terminal.debugger', 'IPython.core.completer', 'unicodedata', 'IPython.core.latex_symbols', 'IPython.utils.generics', 'jedi', 'jedi.api', 'parso', 'parso.parser', 'parso.tree', 'parso._compatibility', 'parso.utils', 'parso.pgen2', 'parso.pgen2.generator', 'parso.pgen2.grammar_parser', 'parso.python', 'parso.python.tokenize', 'parso.python.token', 'parso.grammar', 'parso.python.diff', 'difflib', 'parso.python.parser', 'parso.python.tree', 'parso.python.prefix', 'parso.cache', 'gc', 'parso.python.errors', 'parso.normalizer', 'parso.python.pep8', 'parso.file_io', 'jedi._compatibility', 'jedi.file_io', 'jedi.parser_utils', 'jedi.debug', 'jedi.settings', 'jedi.cache', 'jedi.api.classes', 'jedi.evaluate', 'jedi.evaluate.utils', 'jedi.evaluate.imports', 'jedi.evaluate.sys_path', 'jedi.evaluate.cache', 'jedi.evaluate.base_context', 'jedi.common', 'jedi.common.context', 'jedi.evaluate.helpers', 'jedi.common.utils', 'jedi.evaluate.compiled', 'jedi.evaluate.compiled.context', 'jedi.evaluate.filters', 'jedi.evaluate.flow_analysis', 'jedi.evaluate.recursion', 'jedi.evaluate.names', 'jedi.evaluate.lazy_context', 'jedi.evaluate.compiled.access', 'jedi.evaluate.compiled.getattr_static', 'jedi.evaluate.signature', 'jedi.evaluate.analysis', 'jedi.evaluate.gradual', 'jedi.evaluate.gradual.typeshed', 'jedi.evaluate.gradual.stub_context', 'jedi.evaluate.context', 'jedi.evaluate.context.module', 'jedi.evaluate.context.klass', 'jedi.evaluate.arguments', 'jedi.evaluate.context.iterable', 'jedi.evaluate.param', 'jedi.evaluate.docstrings', 'jedi.evaluate.context.function', 'jedi.evaluate.parser_cache', 'jedi.plugins', 'jedi.evaluate.context.instance', 'jedi.evaluate.gradual.typing', 'jedi.evaluate.syntax_tree', 'jedi.evaluate.finder', 'jedi.evaluate.gradual.conversion', 'jedi.evaluate.gradual.annotation', 'jedi.evaluate.context.decorator', 'jedi.api.keywords', 'pydoc_data', 'pydoc_data.topics', 'jedi.api.interpreter', 'jedi.evaluate.compiled.mixed', 'jedi.api.helpers', 'jedi.api.completion', 'jedi.api.file_name', 'jedi.api.environment', 'filecmp', 'jedi.evaluate.compiled.subprocess', 'jedi.evaluate.compiled.subprocess.functions', 'jedi.api.exceptions', 'jedi.api.project', 'jedi.evaluate.usages', 'jedi.evaluate.gradual.utils', 'jedi.plugins.registry', 'jedi.plugins.stdlib', 'jedi.plugins.flask', 'IPython.terminal.ptutils', 'IPython.terminal.shortcuts', 'IPython.lib.clipboard', 'IPython.terminal.magics', 'IPython.terminal.pt_inputhooks', 'IPython.terminal.prompts', 'IPython.terminal.ipapp', 'IPython.core.magics', 'IPython.core.magics.auto', 'IPython.core.magics.basic', 'IPython.core.magics.code', 'urllib.request', 'email', 'http', 'http.client', 'email.parser', 'email.feedparser', 'email.errors', 'email._policybase', 'email.header', 'email.quoprimime', 'email.base64mime', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'email._parseaddr', 'calendar', 'email.message', 'uu', 'email._encoded_words', 'email.iterators', 'urllib.error', 'urllib.response', 'nturl2path', 'IPython.core.magics.config', 'IPython.core.magics.display', 'IPython.core.magics.execution', 'timeit', 'cProfile', '_lsprof', 'profile', 'pstats', 'IPython.utils.module_paths', 'IPython.utils.timing', 'IPython.core.magics.extension', 'IPython.core.magics.history', 'IPython.core.magics.logging', 'IPython.core.magics.namespace', 'IPython.core.magics.osm', 'IPython.core.magics.packaging', 'IPython.core.magics.pylab', 'IPython.core.pylabtools', 'IPython.core.magics.script', 'IPython.lib.backgroundjobs', 'IPython.core.shellapp', 'IPython.extensions', 'IPython.extensions.storemagic', 'IPython.utils.frame', 'jupyter_client', 'jupyter_client._version', 'jupyter_client.connect', 'zmq', 'zmq.libzmq', 'zmq.backend', 'zmq.backend.select', 'zmq.backend.cython', 'zmq.backend.cython.constants', 'cython_runtime', 'zmq.backend.cython.error', '_cython_0_29_14', 'zmq.backend.cython.message', 'zmq.error', 'zmq.backend.cython.context', 'zmq.backend.cython.socket', 'zmq.backend.cython.utils', 'zmq.backend.cython._poll', 'zmq.backend.cython._version', 'zmq.backend.cython._device', 'zmq.backend.cython._proxy_steerable', 'zmq.sugar', 'zmq.sugar.constants', 'zmq.utils', 'zmq.utils.constant_names', 'zmq.sugar.context', 'zmq.sugar.attrsettr', 'zmq.sugar.socket', 'zmq.sugar.poll', 'zmq.utils.jsonapi', 'zmq.utils.strtypes', 'simplejson', 'decimal', 'numbers', '_decimal', 'simplejson.errors', 'simplejson.raw_json', 'simplejson.decoder', 'simplejson.compat', 'simplejson.scanner', 'simplejson._speedups', 'simplejson.encoder', 'zmq.sugar.frame', 'zmq.sugar.tracker', 'zmq.sugar.version', 'zmq.sugar.stopwatch', 'jupyter_client.localinterfaces', 'jupyter_core', 'jupyter_core.version', 'jupyter_core.paths', 'jupyter_client.launcher', 'traitlets.log', 'jupyter_client.client', 'jupyter_client.channels', 'jupyter_client.channelsabc', 'jupyter_client.clientabc', 'jupyter_client.manager', 'jupyter_client.kernelspec', 'jupyter_client.managerabc', 'jupyter_client.blocking', 'jupyter_client.blocking.client', 'jupyter_client.blocking.channels', 'jupyter_client.multikernelmanager', 'uuid', 'ipykernel.kernelapp', 'tornado', 'tornado.ioloop', 'tornado.concurrent', 'tornado.log', 'logging.handlers', 'tornado.escape', 'tornado.util', 'tornado.speedups', 'colorama', 'colorama.initialise', 'colorama.ansitowin32', 'colorama.ansi', 'colorama.winterm', 'colorama.win32', 'zmq.eventloop', 'zmq.eventloop.ioloop', 'tornado.platform', 'tornado.platform.asyncio', 'tornado.gen', 'zmq.eventloop.zmqstream', 'ipykernel.iostream', 'imp', 'jupyter_client.session', 'hmac', 'jupyter_client.jsonutil', 'dateutil', 'dateutil._version', 'dateutil.parser', 'dateutil.parser._parser', 'dateutil.relativedelta', 'dateutil._common', 'dateutil.tz', 'dateutil.tz.tz', 'dateutil.tz._common', 'dateutil.tz._factories', 'dateutil.tz.win', 'dateutil.parser.isoparser', '_strptime', 'jupyter_client.adapter', 'ipykernel.heartbeat', 'ipykernel.ipkernel', 'IPython.utils.tokenutil', 'ipykernel.comm', 'ipykernel.comm.manager', 'ipykernel.comm.comm', 'ipykernel.kernelbase', 'tornado.queues', 'tornado.locks', 'ipykernel.jsonutil', 'ipykernel.zmqshell', 'IPython.core.payloadpage', 'ipykernel.displayhook', 'ipykernel.eventloops', 'distutils', 'distutils.version', 'ipykernel.parentpoller', 'win32api', 'win32security', 'ntsecuritycon', 'faulthandler', 'ipykernel.datapub', 'ipykernel.serialize', 'ipykernel.pickleutil', 'ipykernel.codeutil', 'IPython.core.completerlib', 'storemagic', 'ipywidgets', 'ipywidgets._version', 'ipywidgets.widgets', 'ipywidgets.widgets.widget', 'ipywidgets.widgets.domwidget', 'ipywidgets.widgets.trait_types', 'ipywidgets.widgets.util', 'ipywidgets.widgets.widget_layout', 'ipywidgets.widgets.widget_style', 'ipywidgets.widgets.valuewidget', 'ipywidgets.widgets.widget_core', 'ipywidgets.widgets.widget_bool', 'ipywidgets.widgets.widget_description', 'ipywidgets.widgets.widget_button', 'ipywidgets.widgets.widget_box', 'ipywidgets.widgets.docutils', 'ipywidgets.widgets.widget_float', 'ipywidgets.widgets.widget_int', 'ipywidgets.widgets.widget_color', 'ipywidgets.widgets.widget_date', 'ipywidgets.widgets.widget_output', 'ipywidgets.widgets.widget_selection', 'ipywidgets.widgets.widget_selectioncontainer', 'ipywidgets.widgets.widget_string', 'ipywidgets.widgets.widget_controller', 'ipywidgets.widgets.interaction', 'ipywidgets.widgets.widget_link', 'ipywidgets.widgets.widget_media', 'ipywidgets.widgets.widget_templates', 'ipywidgets.widgets.widget_upload', 'matplotlib', 'matplotlib.cbook', 'gzip', 'numpy', 'numpy._globals', 'numpy.__config__', 'numpy.version', 'numpy._distributor_init', 'numpy.core', 'numpy.core.info', 'numpy.core.multiarray', 'numpy.core.overrides', 'numpy.core._multiarray_umath', 'numpy.compat', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.core.umath', 'numpy.core.numerictypes', 'numpy.core._string_helpers', 'numpy.core._type_aliases', 'numpy.core._dtype', 'numpy.core.numeric', 'numpy.core._exceptions', 'numpy.core._asarray', 'numpy.core._ufunc_config', 'numpy.core.fromnumeric', 'numpy.core._methods', 'numpy.core.arrayprint', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.shape_base', 'numpy.core.einsumfunc', 'numpy.core._add_newdocs', 'numpy.core._multiarray_tests', 'numpy.core._dtype_ctypes', 'numpy.core._internal', 'numpy._pytesttester', 'numpy.lib', 'numpy.lib.info', 'numpy.lib.type_check', 'numpy.lib.ufunclike', 'numpy.lib.index_tricks', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'numpy.linalg', 'numpy.linalg.info', 'numpy.linalg.linalg', 'numpy.lib.twodim_base', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.lib.function_base', 'numpy.lib.histograms', 'numpy.lib.stride_tricks', 'numpy.lib.mixins', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.scimath', 'numpy.lib.polynomial', 'numpy.lib.utils', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.financial', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.fft', 'numpy.fft._pocketfft', 'numpy.fft._pocketfft_internal', 'numpy.fft.helper', 'numpy.polynomial', 'numpy.polynomial.polynomial', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.random', 'numpy.random._pickle', 'numpy.random.mtrand', 'numpy.random.common', 'numpy.random.bounded_integers', 'numpy.random.mt19937', 'numpy.random.bit_generator', 'secrets', 'numpy.random.philox', 'numpy.random.pcg64', 'numpy.random.sfc64', 'numpy.random.generator', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'numpy.ma.extras', 'numpy.testing', 'unittest', 'unittest.result', 'unittest.util', 'unittest.case', 'unittest.suite', 'unittest.loader', 'unittest.main', 'unittest.runner', 'unittest.signals', 'numpy.testing._private', 'numpy.testing._private.utils', 'numpy.testing._private.decorators', 'numpy.testing._private.nosetester', 'matplotlib.cbook.deprecation', 'matplotlib.rcsetup', 'matplotlib.fontconfig_pattern', 'pyparsing', 'matplotlib.colors', 'matplotlib._color_data', 'cycler', 'matplotlib._version', 'matplotlib.ft2font', 'kiwisolver']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 21:25:41,378 : DEBUG : CACHEDIR=C:\\Users\\rojin\\.matplotlib\n",
      "2020-03-17 21:25:41,384 : DEBUG : Using fontManager instance from C:\\Users\\rojin\\.matplotlib\\fontlist-v310.json\n",
      "2020-03-17 21:25:41,552 : DEBUG : Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "2020-03-17 21:25:41,559 : DEBUG : Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n",
      "2020-03-17 21:25:41,563 : DEBUG : Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)\n",
    "\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "from scipy import sparse as sp\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>1/26/2011</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>7/27/2011</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>6/14/2012</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>5/27/2010</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id       date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  1/26/2011  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  7/27/2011  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  6/14/2012  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  5/27/2010  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw   1/5/2012  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text                 user_id  \\\n",
       "0  My wife took me here on my birthday for breakf...  rLtl8ZkDX5vH5nAx9C3q5Q   \n",
       "1  I have no idea why some people give bad review...  0a2KyEL0d3Yb1V6aivbIuQ   \n",
       "2  love the gyro plate. Rice is so good and I als...  0hT2KtfLiobPvh6cDC8JQg   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  uZetl9T0NcROGOyFfughhg   \n",
       "4  General Manager Scott Petello is a good egg!!!...  vYmM4KTsC8ZfQBg-j5MWkw   \n",
       "\n",
       "   cool  useful  funny  \n",
       "0     2       5      0  \n",
       "1     0       0      0  \n",
       "2     0       1      0  \n",
       "3     1       2      0  \n",
       "4     0       0      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/yelp_academic_dataset_review_10000.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the Data\n",
    "\n",
    "DataFrame’s info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 9 columns):\n",
      "business_id    10000 non-null object\n",
      "date           10000 non-null object\n",
      "review_id      10000 non-null object\n",
      "stars          10000 non-null int64\n",
      "text           10000 non-null object\n",
      "user_id        10000 non-null object\n",
      "cool           10000 non-null int64\n",
      "useful         10000 non-null int64\n",
      "funny          10000 non-null int64\n",
      "dtypes: int64(4), object(5)\n",
      "memory usage: 703.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension the Data\n",
    "\n",
    "Get the dimension (number of rows and columns) of the data using DataFrame's shape method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the data:  (10000, 9)\n",
      "No. of Rows: 10000\n",
      "No. of Columns: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of the data: \", df.shape)\n",
    "\n",
    "no_of_rows = df.shape[0]\n",
    "no_of_columns = df.shape[1]\n",
    "\n",
    "print(\"No. of Rows: %d\" % no_of_rows)\n",
    "print(\"No. of Columns: %d\" % no_of_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the DataFrame Object into a 2D Array of Documents\n",
    "\n",
    "We convert the documents from DataFrame object to an array of documents.\n",
    "\n",
    "It's a 2D array in which each row reprents a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the documents array:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "docs_array = array(df['text'])\n",
    "\n",
    "print(\"Dimension of the documents array: \", docs_array.shape)\n",
    "\n",
    "# Display the first document\n",
    "#print(docs_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the Data\n",
    "\n",
    "Pre-processing of the text data is done using the following steps:\n",
    "\n",
    "- Convert to lowercase \n",
    "- Tokenize (split the documents into tokens or words)\n",
    "- Remove numbers, but not words that contain numbers\n",
    "- Remove words that are only a single character\n",
    "- Lemmatize the tokens/words\n",
    "\n",
    "\n",
    "### Tokenization and Lemmatization\n",
    "\n",
    "We convert all the words into lowercase then tokenize each word using NLTK Regular-Expression Tokenizer class \"RegexpTokenizer\". It splits a given string to substrings using a regular expression. Then we remove numbers and single character words since they usually don't impart much useful information and are very high in number. Then, we lemmatize the tokens using WordNetLemmatizer from NLTK, where we extract the root words of the tokens using the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Convert the 2D Document Array into a 2D Array of Tokenized Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # Tokenize the words.\n",
    "    \n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the 2D Document Array into a 1D Array of Tokenized Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.34 s\n"
     ]
    }
   ],
   "source": [
    "# Convert the 2D Document Array into a 1D Array of Tokenized Words\n",
    "%time docs = docs_preprocessor(docs_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the 2D Array of Tokenized Documents:  10000\n",
      "[['wife', 'took', 'here', 'birthday', 'breakfast', 'excellent', 'weather', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'ground', 'absolute', 'pleasure', 'waitress', 'excellent', 'food', 'arrived', 'quickly', 'semi', 'busy', 'saturday', 'morning', 'looked', 'like', 'place', 'fill', 'pretty', 'quickly', 'earlier', 'here', 'better', 'yourself', 'favor', 'their', 'bloody', 'mary', 'phenomenal', 'simply', 'best', 'ever', 'pretty', 'sure', 'they', 'only', 'ingredient', 'from', 'their', 'garden', 'blend', 'them', 'fresh', 'when', 'order', 'amazing', 'while', 'everything', 'menu', 'look', 'excellent', 'white', 'truffle', 'scrambled', 'egg', 'vegetable', 'skillet', 'tasty', 'delicious', 'came', 'with', 'piece', 'their', 'griddled', 'bread', 'with', 'amazing', 'absolutely', 'made', 'meal', 'complete', 'best', 'toast', 'ever', 'anyway', 'wait', 'back'], ['have', 'idea', 'some', 'people', 'give', 'review', 'about', 'this', 'place', 'go', 'show', 'please', 'everyone', 'they', 'probably', 'griping', 'about', 'something', 'that', 'their', 'fault', 'there', 'many', 'people', 'like', 'that', 'case', 'friend', 'arrived', 'about', 'this', 'past', 'sunday', 'pretty', 'crowded', 'more', 'than', 'thought', 'sunday', 'evening', 'thought', 'would', 'have', 'wait', 'forever', 'seat', 'they', 'said', 'seated', 'when', 'girl', 'come', 'back', 'from', 'seating', 'someone', 'else', 'were', 'seated', 'waiter', 'came', 'drink', 'order', 'everyone', 'very', 'pleasant', 'from', 'host', 'that', 'seated', 'waiter', 'server', 'price', 'were', 'very', 'good', 'well', 'placed', 'order', 'once', 'decided', 'what', 'wanted', 'shared', 'baked', 'spaghetti', 'calzone', 'small', 'here', 'beef', 'pizza', 'both', 'them', 'calzone', 'huge', 'smallest', 'personal', 'small', 'pizza', 'both', 'were', 'awesome', 'friend', 'liked', 'pizza', 'better', 'liked', 'calzone', 'better', 'calzone', 'doe', 'have', 'sweetish', 'sauce', 'that', 'like', 'sauce', 'part', 'pizza', 'take', 'home', 'were', 'door', 'everything', 'great', 'like', 'these', 'reviewer', 'that', 'go', 'show', 'that', 'have', 'these', 'thing', 'yourself', 'because', 'these', 'reviewer', 'have', 'some', 'serious', 'issue']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the 2D Array of Tokenized Documents: \", len(docs))\n",
    "\n",
    "#Display the first two document\n",
    "print(docs[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Bigrams/Trigrams:\n",
    "\n",
    "N-grams are combinations of adjacent words or letters of length 'n' that you can find in your source text. These combinations of words carry a special meaning. For example: car-pool is an n-gram formed using the two words car and pool that carries a distinct meaning different from the individual words. \n",
    "\n",
    "If n=2, it is called a Bigram and if n=3, it is called a Trigram.\n",
    "\n",
    "We find all the combinations of Bigrams and Trigrams. Then, we keep only the frequent phrases. We finally add the frequent phrases to the original data, since we would like to keep the words “car” and “pool” as well as the bigram “car_pool”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 21:25:51,086 : INFO : collecting all words and their counts\n",
      "2020-03-17 21:25:51,086 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-03-17 21:25:52,996 : INFO : collected 410104 word types from a corpus of 733158 words (unigram + bigrams) and 10000 sentences\n",
      "2020-03-17 21:25:52,997 : INFO : using 410104 counts as vocab in Phrases<0 vocab, min_count=200, threshold=10.0, max_vocab_size=40000000>\n",
      "2020-03-17 21:25:52,998 : INFO : collecting all words and their counts\n",
      "2020-03-17 21:25:53,000 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-03-17 21:25:59,084 : INFO : collected 412756 word types from a corpus of 726457 words (unigram + bigrams) and 10000 sentences\n",
      "2020-03-17 21:25:59,085 : INFO : using 412756 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to doc (only ones that appear 100 times or more).\n",
    "bigram = Phrases(docs, min_count=200)\n",
    "trigram = Phrases(bigram[docs])\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "    for token in trigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a trigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 21:26:07,458 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-03-17 21:26:09,175 : INFO : built Dictionary(26649 unique tokens: ['absolute', 'absolutely', 'amazing', 'anyway', 'arrived']...) from 10000 documents (total 803130 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in initital documents: 26649\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "print('Number of unique words in initital documents:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute\n",
      "absolutely\n",
      "amazing\n",
      "anyway\n",
      "arrived\n",
      "back\n",
      "best\n",
      "best_ever\n",
      "better\n",
      "birthday\n",
      "blend\n",
      "bloody\n",
      "bloody_mary\n",
      "bread\n",
      "breakfast\n",
      "busy\n",
      "came\n",
      "complete\n",
      "delicious\n",
      "earlier\n",
      "egg\n",
      "ever\n",
      "everything\n",
      "excellent\n",
      "favor\n",
      "fill\n",
      "food\n",
      "fresh\n",
      "from\n",
      "garden\n",
      "griddled\n",
      "ground\n",
      "here\n",
      "ingredient\n",
      "like\n",
      "look\n",
      "looked\n",
      "looked_like\n",
      "made\n",
      "mary\n",
      "meal\n",
      "menu\n",
      "morning\n",
      "only\n",
      "order\n",
      "outside\n",
      "overlooking\n",
      "perfect\n",
      "phenomenal\n",
      "piece\n",
      "place\n",
      "pleasure\n",
      "pretty\n",
      "pretty_quickly\n",
      "quickly\n",
      "saturday\n",
      "saturday_morning\n",
      "scrambled\n",
      "scrambled_egg\n",
      "semi\n",
      "simply\n",
      "sitting\n",
      "sitting_outside\n",
      "skillet\n",
      "sure\n",
      "tasty\n",
      "their\n",
      "them\n",
      "they\n",
      "toast\n",
      "took\n",
      "truffle\n",
      "vegetable\n",
      "wait\n",
      "waitress\n",
      "weather\n",
      "when\n",
      "which\n",
      "while\n",
      "white\n",
      "wife\n",
      "with\n",
      "yourself\n",
      "yourself_favor\n",
      "about\n",
      "awesome\n",
      "baked\n",
      "because\n",
      "beef\n",
      "both\n",
      "calzone\n",
      "case\n",
      "come\n",
      "come_back\n",
      "crowded\n",
      "decided\n",
      "doe\n",
      "door\n",
      "drink\n",
      "else\n",
      "evening\n",
      "everyone\n",
      "fault\n",
      "forever\n",
      "friend\n",
      "girl\n",
      "give\n",
      "go\n",
      "good\n",
      "great\n",
      "griping\n",
      "have\n",
      "home\n",
      "host\n",
      "huge\n",
      "idea\n",
      "issue\n",
      "liked\n",
      "many\n",
      "many_people\n",
      "more\n",
      "more_than\n",
      "once\n",
      "part\n",
      "past\n",
      "people\n",
      "personal\n",
      "pizza\n",
      "placed\n",
      "placed_order\n",
      "pleasant\n",
      "please\n",
      "price\n",
      "probably\n",
      "review\n",
      "reviewer\n",
      "said\n",
      "sauce\n",
      "seat\n",
      "seated\n",
      "seating\n",
      "serious\n",
      "server\n",
      "shared\n",
      "show\n",
      "small\n",
      "smallest\n",
      "some\n",
      "some_serious\n",
      "someone\n",
      "someone_else\n",
      "something\n",
      "spaghetti\n",
      "sunday\n",
      "sunday_evening\n",
      "sweetish\n",
      "take\n",
      "take_home\n",
      "than\n",
      "that\n",
      "there\n",
      "these\n",
      "thing\n",
      "this\n",
      "this_place\n",
      "thought\n",
      "very\n",
      "very_pleasant\n",
      "waiter\n",
      "wanted\n",
      "well\n",
      "were\n",
      "were_seated\n",
      "what\n",
      "would\n",
      "also\n",
      "candy\n",
      "gyro\n",
      "love\n",
      "plate\n",
      "rice\n",
      "selection\n",
      "area\n",
      "ballpark\n",
      "baseball\n",
      "can\n",
      "chaparral\n",
      "clean\n",
      "convenient\n",
      "dakota\n",
      "dept\n",
      "desert\n",
      "dog\n",
      "duck\n",
      "fenced\n",
      "field\n",
      "find\n",
      "keeping\n",
      "lake\n",
      "located\n",
      "mitt\n",
      "over\n",
      "park\n",
      "path\n",
      "pick\n",
      "play\n",
      "poopy\n",
      "rosie\n",
      "scottsdale\n",
      "shaded\n",
      "sniff\n",
      "surrounded\n",
      "trash\n",
      "trash_can\n",
      "very_convenient\n",
      "wonderful\n",
      "xeriscape\n",
      "albeit\n",
      "always\n",
      "assure\n",
      "customer\n",
      "detail\n",
      "general\n",
      "general_manager\n",
      "important\n",
      "inevitable\n",
      "into\n",
      "just\n",
      "life\n",
      "manager\n",
      "mistake\n",
      "petello\n",
      "rare\n",
      "recover\n",
      "respect\n",
      "satisfied\n",
      "scott\n",
      "speak\n",
      "staff\n",
      "state\n",
      "surprised\n",
      "thanks\n",
      "totally\n",
      "treat\n",
      "walk\n",
      "your\n",
      "after\n",
      "aioli\n",
      "almost\n",
      "anew\n",
      "another\n",
      "apologized\n",
      "apparently\n",
      "apple\n",
      "beautiful\n",
      "before\n",
      "bill\n",
      "bill_came\n",
      "bothered\n",
      "bring\n",
      "butter\n",
      "cake\n",
      "chef\n",
      "communicated\n",
      "comped\n",
      "couple\n",
      "couple_day\n",
      "crisp\n",
      "crudites\n",
      "day\n",
      "definitely\n",
      "dessert\n",
      "didn\n",
      "didn_disappoint\n",
      "didn_even\n",
      "disappoint\n",
      "earthy\n",
      "echo\n",
      "enough\n",
      "entree\n",
      "even\n",
      "explain\n",
      "fairly\n",
      "farm\n",
      "feeling\n",
      "finished\n",
      "five\n",
      "five_star\n",
      "foccacia\n",
      "forgot\n",
      "forgot_bring\n",
      "fresh_veggie\n",
      "freshness\n",
      "full\n",
      "gingerbread\n",
      "glass\n",
      "glass_wine\n",
      "grapefruit\n",
      "honey\n",
      "impressed\n"
     ]
    }
   ],
   "source": [
    "for i in range (0 , 300):\n",
    "    print(dictionary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Rare and Common Tokens/Words\n",
    "\n",
    "Now we remove in-frequent words from our dictionary. We also remove words that appear frequently in most documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 21:26:09,276 : INFO : discarding 26118 tokens: [('absolute', 55), ('back', 2326), ('best_ever', 62), ('birthday', 170), ('blend', 44), ('bloody', 46), ('bloody_mary', 37), ('complete', 99), ('earlier', 71), ('egg', 178)]...\n",
      "2020-03-17 21:26:09,277 : INFO : keeping 531 tokens which were in no less than 200 and no more than 2000 (=20.0%) documents\n",
      "2020-03-17 21:26:09,285 : DEBUG : rebuilding dictionary, shrinking gaps\n",
      "2020-03-17 21:26:09,288 : INFO : resulting dictionary: Dictionary(531 unique tokens: ['absolutely', 'amazing', 'anyway', 'arrived', 'best']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words after removing rare and common words: 531\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur less than 100 documents, or more than 20% of the documents.\n",
    "dictionary.filter_extremes(no_below=200, no_above=0.20) #100,60\n",
    "\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Representation of Data\n",
    "\n",
    "\n",
    "Finally, we transform the documents to a **vectorized form**. \n",
    "\n",
    "We simply compute the frequency of each word, including the bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 531\n",
      "Number of documents: 10000\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolutely\n",
      "amazing\n",
      "anyway\n",
      "arrived\n",
      "best\n",
      "better\n",
      "bread\n",
      "breakfast\n",
      "busy\n",
      "came\n",
      "delicious\n",
      "ever\n",
      "everything\n",
      "excellent\n",
      "fresh\n",
      "ingredient\n",
      "look\n",
      "looked\n",
      "made\n",
      "meal\n",
      "menu\n",
      "morning\n",
      "order\n",
      "outside\n",
      "perfect\n",
      "piece\n",
      "pretty\n",
      "quickly\n",
      "saturday\n",
      "sitting\n",
      "sure\n",
      "tasty\n",
      "them\n",
      "took\n",
      "wait\n",
      "waitress\n",
      "while\n",
      "white\n",
      "wife\n",
      "awesome\n",
      "because\n",
      "beef\n",
      "both\n",
      "come\n",
      "come_back\n",
      "decided\n",
      "doe\n",
      "door\n",
      "drink\n",
      "else\n",
      "evening\n",
      "everyone\n",
      "friend\n",
      "girl\n",
      "give\n",
      "home\n",
      "huge\n",
      "idea\n",
      "liked\n",
      "many\n",
      "more_than\n",
      "once\n",
      "part\n",
      "past\n",
      "people\n",
      "pizza\n",
      "please\n",
      "price\n",
      "probably\n",
      "review\n",
      "said\n",
      "sauce\n",
      "seat\n",
      "seated\n",
      "seating\n",
      "server\n",
      "show\n",
      "small\n",
      "someone\n",
      "something\n",
      "sunday\n",
      "take\n",
      "than\n",
      "these\n",
      "thing\n",
      "thought\n",
      "waiter\n",
      "wanted\n",
      "well\n",
      "also\n",
      "love\n",
      "plate\n",
      "rice\n",
      "selection\n",
      "area\n",
      "clean\n",
      "find\n",
      "located\n",
      "over\n",
      "pick\n",
      "scottsdale\n",
      "wonderful\n",
      "always\n",
      "customer\n",
      "into\n",
      "life\n",
      "manager\n",
      "staff\n",
      "surprised\n",
      "thanks\n",
      "totally\n",
      "walk\n",
      "your\n",
      "after\n",
      "almost\n",
      "another\n",
      "beautiful\n",
      "before\n",
      "bill\n",
      "bring\n",
      "butter\n",
      "cake\n",
      "chef\n",
      "couple\n",
      "day\n",
      "definitely\n",
      "dessert\n",
      "didn\n",
      "enough\n",
      "entree\n",
      "even\n",
      "feeling\n",
      "five\n",
      "full\n",
      "glass\n",
      "impressed\n",
      "inside\n",
      "kitchen\n",
      "know\n",
      "lady\n",
      "later\n",
      "live\n",
      "long\n",
      "maybe\n",
      "meat\n",
      "minute\n",
      "much\n",
      "offer\n",
      "ordered\n",
      "pork\n",
      "problem\n",
      "quite\n",
      "restaurant\n",
      "return\n",
      "salad\n",
      "sandwich\n",
      "seemed\n",
      "should\n",
      "slice\n",
      "star\n",
      "start\n",
      "started\n",
      "tried\n",
      "until\n",
      "veggie\n",
      "waiting\n",
      "wall\n",
      "want\n",
      "warm\n",
      "week\n",
      "wine\n",
      "bite\n",
      "choose\n",
      "cute\n",
      "decor\n",
      "dinner\n",
      "doing\n",
      "down\n",
      "drive\n",
      "easy\n",
      "fact\n",
      "flavor\n",
      "gone\n",
      "green\n",
      "little\n",
      "loved\n",
      "mexican\n",
      "next\n",
      "pepper\n",
      "salsa\n",
      "shrimp\n",
      "sign\n",
      "street\n",
      "such\n",
      "taco\n",
      "variety\n",
      "wasn\n",
      "yummy\n",
      "employee\n",
      "kind\n",
      "make\n",
      "next_time\n",
      "phoenix\n",
      "atmosphere\n",
      "especially\n",
      "get\n",
      "happy\n",
      "happy_hour\n",
      "hour\n",
      "need\n",
      "nice\n",
      "right\n",
      "spot\n",
      "wish\n",
      "then\n",
      "unique\n",
      "cheap\n",
      "cream\n",
      "store\n",
      "super\n",
      "sweet\n",
      "along\n",
      "around\n",
      "card\n",
      "choice\n",
      "roll\n",
      "chicken\n",
      "every\n",
      "every_time\n",
      "going\n",
      "have_ever\n",
      "last\n",
      "other\n",
      "soup\n",
      "tomato\n",
      "available\n",
      "customer_service\n",
      "doesn\n",
      "done\n",
      "friendly\n",
      "keep\n",
      "looking\n",
      "most\n",
      "read\n",
      "shop\n",
      "space\n",
      "style\n",
      "though\n",
      "work\n",
      "working\n",
      "again\n",
      "bacon\n",
      "bartender\n",
      "care\n",
      "cold\n",
      "first\n",
      "found\n",
      "game\n",
      "helpful\n",
      "hope\n",
      "large\n",
      "myself\n",
      "night\n",
      "side\n",
      "since\n",
      "topping\n",
      "town\n",
      "under\n",
      "very_friendly\n",
      "went\n",
      "whole\n",
      "could\n",
      "experience\n",
      "guy\n",
      "have_been\n",
      "le\n",
      "left\n",
      "owner\n",
      "special\n",
      "tell\n",
      "told\n",
      "worth\n",
      "actually\n",
      "asked\n",
      "bottle\n",
      "crispy\n",
      "disappointed\n",
      "fish\n",
      "high\n",
      "leave\n",
      "lunch\n",
      "spicy\n",
      "trip\n",
      "wing\n",
      "wouldn\n",
      "arizona\n",
      "cheese\n",
      "okay\n",
      "shopping\n",
      "stuff\n",
      "world\n",
      "anything\n",
      "bean\n",
      "being\n",
      "chip\n",
      "each\n",
      "guess\n",
      "knew\n",
      "offered\n",
      "one\n",
      "patio\n",
      "pretty_good\n",
      "serve\n",
      "tasted\n",
      "those\n",
      "walking\n",
      "water\n",
      "called\n",
      "four\n",
      "having\n",
      "overall\n",
      "still\n",
      "think\n",
      "call\n",
      "family\n",
      "least\n",
      "might\n",
      "potato\n",
      "course\n",
      "favorite\n",
      "first_time\n",
      "check\n",
      "month\n",
      "without\n",
      "wrong\n",
      "burger\n",
      "coming\n",
      "cool\n",
      "bowl\n",
      "couldn\n",
      "different\n",
      "parking\n",
      "real\n",
      "taste\n",
      "date\n",
      "dish\n",
      "eating\n",
      "highly\n",
      "never\n",
      "onion\n",
      "sometimes\n",
      "table\n",
      "authentic\n",
      "downtown\n",
      "during\n",
      "given\n",
      "know_what\n",
      "mean\n",
      "name\n",
      "enjoyed\n",
      "entire\n",
      "fantastic\n",
      "felt\n",
      "flavorful\n",
      "gave\n",
      "remember\n",
      "stop\n",
      "stopped\n",
      "year\n",
      "yelp\n",
      "beer\n",
      "chain\n",
      "between\n",
      "close\n",
      "comfortable\n",
      "perfectly\n",
      "quality\n",
      "served\n",
      "feel\n",
      "itself\n",
      "simple\n",
      "trying\n",
      "already\n",
      "although\n",
      "amount\n",
      "believe\n",
      "expect\n",
      "front\n",
      "help\n",
      "open\n",
      "option\n",
      "recommend\n",
      "same\n",
      "soon\n",
      "ambiance\n",
      "getting\n",
      "glad\n",
      "nothing\n",
      "portion\n",
      "rather\n",
      "type\n",
      "attentive\n",
      "buck\n",
      "cooked\n",
      "eaten\n",
      "french\n",
      "half\n",
      "room\n",
      "steak\n",
      "where\n",
      "chocolate\n",
      "coffee\n",
      "extremely\n",
      "highly_recommend\n",
      "house\n",
      "must\n",
      "plenty\n",
      "today\n",
      "counter\n",
      "fry\n",
      "regular\n",
      "lot\n",
      "seem\n",
      "behind\n",
      "crust\n",
      "hard\n",
      "fine\n",
      "neighborhood\n",
      "music\n",
      "business\n",
      "better_than\n",
      "deal\n",
      "ended\n",
      "friday\n",
      "grilled\n",
      "hand\n",
      "however\n",
      "needed\n",
      "others\n",
      "rest\n",
      "stand\n",
      "walked\n",
      "watch\n",
      "average\n",
      "cost\n",
      "fast\n",
      "haven\n",
      "item\n",
      "note\n",
      "party\n",
      "point\n",
      "quick\n",
      "ready\n",
      "sushi\n",
      "usually\n",
      "fried\n",
      "finally\n",
      "hotel\n",
      "line\n",
      "reasonable\n",
      "head\n",
      "away\n",
      "early\n",
      "anyone\n",
      "appetizer\n",
      "enjoy\n",
      "even_though\n",
      "extra\n",
      "instead\n",
      "list\n",
      "location\n",
      "mind\n",
      "mouth\n",
      "priced\n",
      "seriously\n",
      "short\n",
      "across\n",
      "brought\n",
      "money\n",
      "local\n",
      "feel_like\n",
      "dining\n",
      "decent\n",
      "near\n",
      "three\n",
      "word\n",
      "several\n",
      "packed\n",
      "seems\n",
      "waited\n",
      "tempe\n",
      "joint\n",
      "slow\n",
      "either\n",
      "person\n",
      "making\n",
      "group\n",
      "weekend\n",
      "valley\n",
      "burrito\n"
     ]
    }
   ],
   "source": [
    "for i in range (0 , 500):\n",
    "    print(dictionary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LDA Model\n",
    "\n",
    "We use the gensim.models.LdaModel class for performing LDA. [https://radimrehurek.com/gensim/models/ldamodel.html]\n",
    "\n",
    "\n",
    "#### Below we discuss the setting of some of the key parameters.\n",
    "\n",
    "- num_topics (int, optional) – The number of requested latent topics to be extracted from the training corpus.\n",
    "\n",
    " \n",
    "LDA is an unsupervised technique, meaning that we don't know prior to running the model how many topics exits in our corpus. It depends on the data and the application. We may use the following two technique to determine the number of topics.\n",
    "\n",
    "\n",
    "        Technique 1: Topic Coherence \n",
    "The main technique to determine the number of topics is **Topic coherence** [http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf]\n",
    "\n",
    "\n",
    "        Technique 2: Visualizing Inter-Topic Distance \n",
    "Use the LDA visualization tool pyLDAvis to observe Intertopic Distance Map (discussed later). By varying the number of topics we could determine the optimal value from the visualization.\n",
    "\n",
    "We **use both techniques** to determine the optimal number of topics.\n",
    "\n",
    "- chunksize (int, optional) – Number of documents to be used in each training chunk.\n",
    "\n",
    "It controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory. \n",
    "\n",
    "We set chunksize = 20000, which is more than the amount of documents. Thus, it processes all the data in one go. Chunksize can however influence the quality of the model.\n",
    "\n",
    "- passes (int, optional) – Number of passes through the corpus during training.\n",
    "\n",
    "It controls how often we train the model on the entire corpus. Another word for passes might be “epochs”. \n",
    "\n",
    "- iterations (int, optional) – Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
    "\n",
    "It is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. \n",
    "\n",
    "        It is important to set the number of “passes” and “iterations” high enough.\n",
    "        \n",
    "\n",
    "#### How to Set \"passes\" and \"iterations\":\n",
    "\n",
    "First, enable logging and set eval_every = 1 (however, it might slow down, so, we use None) in LdaModel. \n",
    "\n",
    "When training the model look for a line in the log that looks something like this:\n",
    "\n",
    "        2020-02-25 19:07:04,716 : DEBUG : 49/403 documents converged within 400 iterations\n",
    "\n",
    "If we set passes = 20, we will see this line 20 times. \n",
    "\n",
    "### Important: We need to make sure that by the final passes, most of the documents have converged. \n",
    "\n",
    "For example, if passes = 20 and iterations = 400, then, we should see something like following:\n",
    "\n",
    "\n",
    "        2020-02-25 19:07:18,041 : INFO : PROGRESS: pass 19, at document #403/403\n",
    "        2020-02-25 19:07:18,042 : DEBUG : performing inference on a chunk of 403 documents\n",
    "        2020-02-25 19:07:18,627 : DEBUG : 402/403 documents converged within 400 iterations\n",
    "\n",
    "Thus, want to choose both passes and iterations to be high enough for this to happen.\n",
    "\n",
    "\n",
    "- eval_every (int, optional) – Log perplexity is estimated every that many updates. Setting this to 1 slows down training by ~2x.\n",
    "\n",
    "\n",
    "- alpha ({numpy.ndarray, str}, optional): Can be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. \n",
    "\n",
    "Alternatively default prior selecting strategies can be employed by supplying a string:\n",
    "\n",
    "        ’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.\n",
    "\n",
    "        ’auto’: Learns an asymmetric prior from the corpus (not available if distributed==True).\n",
    "        \n",
    "        \n",
    "- eta ({float, np.array, str}, optional) – A-priori belief on word probability.\n",
    "\n",
    "It can be:\n",
    "\n",
    "        scalar for a symmetric prior over topic/word probability,\n",
    "\n",
    "        vector of length num_words to denote an asymmetric user defined probability for each word,\n",
    "\n",
    "        matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
    "\n",
    "        the string ‘auto’ to learn the asymmetric prior from the data.\n",
    "\n",
    "\n",
    "We set alpha = 'auto' and eta = 'auto'. Again this is somewhat technical, but essentially we are automatically learning two parameters in the model that we usually would have to specify explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 21:32:58,000 : INFO : using autotuned alpha, starting with [0.071428575, 0.071428575, 0.071428575, 0.071428575, 0.071428575, 0.071428575, 0.071428575, 0.071428575, 0.071428575, 0.071428575, 0.071428575, 0.071428575, 0.071428575, 0.071428575]\n",
      "2020-03-17 21:32:58,001 : INFO : using serial LDA version on this node\n",
      "2020-03-17 21:32:58,003 : INFO : running online (multi-pass) LDA training, 14 topics, 20 passes over the supplied corpus of 10000 documents, updating model once every 10000 documents, evaluating perplexity every 0 documents, iterating 500x with a convergence threshold of 0.001000\n",
      "2020-03-17 21:32:58,005 : INFO : PROGRESS: pass 0, at document #10000/10000\n",
      "2020-03-17 21:32:58,006 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:33:14,903 : DEBUG : 9797/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:33:14,962 : INFO : optimized alpha [0.060104355, 0.057134457, 0.059176445, 0.057408243, 0.05710619, 0.05619271, 0.0606529, 0.055686314, 0.055328704, 0.05757154, 0.054301187, 0.05578178, 0.05797842, 0.05694989]\n",
      "2020-03-17 21:33:14,963 : DEBUG : updating topics\n",
      "2020-03-17 21:33:14,965 : INFO : topic #10 (0.054): 0.015*\"even\" + 0.014*\"nice\" + 0.010*\"didn\" + 0.009*\"even_though\" + 0.009*\"other\" + 0.008*\"drink\" + 0.008*\"price\" + 0.008*\"over\" + 0.008*\"though\" + 0.007*\"went\"\n",
      "2020-03-17 21:33:14,966 : INFO : topic #8 (0.055): 0.017*\"have_been\" + 0.010*\"much\" + 0.010*\"love\" + 0.010*\"than\" + 0.010*\"because\" + 0.009*\"restaurant\" + 0.009*\"also\" + 0.009*\"know\" + 0.009*\"always\" + 0.008*\"thing\"\n",
      "2020-03-17 21:33:14,967 : INFO : topic #2 (0.059): 0.012*\"because\" + 0.011*\"restaurant\" + 0.011*\"always\" + 0.010*\"have_been\" + 0.010*\"them\" + 0.008*\"well\" + 0.008*\"love\" + 0.007*\"price\" + 0.007*\"cheese\" + 0.007*\"more_than\"\n",
      "2020-03-17 21:33:14,968 : INFO : topic #0 (0.060): 0.011*\"also\" + 0.011*\"pizza\" + 0.010*\"other\" + 0.009*\"sauce\" + 0.009*\"happy_hour\" + 0.008*\"delicious\" + 0.008*\"pretty\" + 0.008*\"salad\" + 0.008*\"best\" + 0.007*\"love\"\n",
      "2020-03-17 21:33:14,969 : INFO : topic #6 (0.061): 0.015*\"your\" + 0.009*\"little\" + 0.008*\"best\" + 0.008*\"them\" + 0.008*\"know\" + 0.008*\"think\" + 0.007*\"make\" + 0.007*\"nice\" + 0.007*\"dish\" + 0.007*\"even\"\n",
      "2020-03-17 21:33:14,970 : INFO : topic diff=0.592852, rho=1.000000\n",
      "2020-03-17 21:33:14,973 : INFO : PROGRESS: pass 1, at document #10000/10000\n",
      "2020-03-17 21:33:14,974 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:33:29,543 : DEBUG : 9965/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:33:29,608 : INFO : optimized alpha [0.057109565, 0.052445017, 0.054066084, 0.052447714, 0.05223227, 0.05292848, 0.05584424, 0.051478606, 0.050977837, 0.052970093, 0.050178126, 0.051172834, 0.052923776, 0.052173533]\n",
      "2020-03-17 21:33:29,609 : DEBUG : updating topics\n",
      "2020-03-17 21:33:29,611 : INFO : topic #10 (0.050): 0.019*\"even_though\" + 0.018*\"even\" + 0.014*\"nice\" + 0.012*\"didn\" + 0.010*\"though\" + 0.010*\"drink\" + 0.009*\"coffee\" + 0.009*\"other\" + 0.008*\"over\" + 0.008*\"room\"\n",
      "2020-03-17 21:33:29,613 : INFO : topic #8 (0.051): 0.018*\"have_been\" + 0.012*\"than\" + 0.011*\"much\" + 0.010*\"because\" + 0.009*\"better\" + 0.009*\"love\" + 0.009*\"know\" + 0.008*\"thing\" + 0.008*\"also\" + 0.008*\"restaurant\"\n",
      "2020-03-17 21:33:29,613 : INFO : topic #2 (0.054): 0.013*\"always\" + 0.013*\"restaurant\" + 0.012*\"because\" + 0.011*\"them\" + 0.009*\"have_been\" + 0.009*\"more_than\" + 0.009*\"well\" + 0.008*\"price\" + 0.008*\"staff\" + 0.008*\"love\"\n",
      "2020-03-17 21:33:29,614 : INFO : topic #6 (0.056): 0.018*\"your\" + 0.010*\"little\" + 0.009*\"dish\" + 0.008*\"make\" + 0.008*\"know\" + 0.008*\"best\" + 0.008*\"them\" + 0.008*\"highly_recommend\" + 0.008*\"think\" + 0.007*\"because\"\n",
      "2020-03-17 21:33:29,615 : INFO : topic #0 (0.057): 0.017*\"pizza\" + 0.012*\"also\" + 0.011*\"sauce\" + 0.011*\"salad\" + 0.010*\"delicious\" + 0.009*\"sandwich\" + 0.009*\"other\" + 0.009*\"cheese\" + 0.008*\"happy_hour\" + 0.008*\"pretty\"\n",
      "2020-03-17 21:33:29,616 : INFO : topic diff=0.141706, rho=0.577350\n",
      "2020-03-17 21:33:29,618 : INFO : PROGRESS: pass 2, at document #10000/10000\n",
      "2020-03-17 21:33:29,619 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:33:42,110 : DEBUG : 9981/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:33:42,168 : INFO : optimized alpha [0.0557714, 0.04945538, 0.05030158, 0.049089566, 0.04861832, 0.051527787, 0.052424498, 0.04900847, 0.048303433, 0.050202183, 0.047709342, 0.048079655, 0.049159206, 0.048976857]\n",
      "2020-03-17 21:33:42,169 : DEBUG : updating topics\n",
      "2020-03-17 21:33:42,171 : INFO : topic #10 (0.048): 0.024*\"even_though\" + 0.020*\"even\" + 0.013*\"didn\" + 0.013*\"nice\" + 0.012*\"though\" + 0.011*\"coffee\" + 0.011*\"drink\" + 0.009*\"other\" + 0.009*\"room\" + 0.008*\"over\"\n",
      "2020-03-17 21:33:42,172 : INFO : topic #11 (0.048): 0.082*\"have_been\" + 0.011*\"year\" + 0.010*\"love\" + 0.009*\"first\" + 0.008*\"since\" + 0.008*\"also\" + 0.007*\"little\" + 0.007*\"never\" + 0.007*\"first_time\" + 0.007*\"could\"\n",
      "2020-03-17 21:33:42,173 : INFO : topic #5 (0.052): 0.016*\"always\" + 0.015*\"love\" + 0.013*\"staff\" + 0.013*\"drink\" + 0.012*\"friendly\" + 0.010*\"location\" + 0.009*\"never\" + 0.008*\"night\" + 0.008*\"know\" + 0.008*\"people\"\n",
      "2020-03-17 21:33:42,174 : INFO : topic #6 (0.052): 0.020*\"your\" + 0.012*\"highly_recommend\" + 0.011*\"dish\" + 0.010*\"little\" + 0.009*\"make\" + 0.008*\"know\" + 0.008*\"best\" + 0.008*\"flavor\" + 0.008*\"them\" + 0.007*\"think\"\n",
      "2020-03-17 21:33:42,174 : INFO : topic #0 (0.056): 0.022*\"pizza\" + 0.014*\"salad\" + 0.012*\"sauce\" + 0.012*\"also\" + 0.011*\"delicious\" + 0.011*\"sandwich\" + 0.010*\"cheese\" + 0.009*\"chicken\" + 0.009*\"fresh\" + 0.008*\"pretty\"\n",
      "2020-03-17 21:33:42,175 : INFO : topic diff=0.123026, rho=0.500000\n",
      "2020-03-17 21:33:42,177 : INFO : PROGRESS: pass 3, at document #10000/10000\n",
      "2020-03-17 21:33:42,178 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:33:53,621 : DEBUG : 9992/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:33:53,680 : INFO : optimized alpha [0.05510895, 0.047549892, 0.047486868, 0.046495885, 0.045828164, 0.050871503, 0.049901884, 0.04732104, 0.046393145, 0.048621915, 0.045897838, 0.04577083, 0.04624424, 0.046655543]\n",
      "2020-03-17 21:33:53,681 : DEBUG : updating topics\n",
      "2020-03-17 21:33:53,683 : INFO : topic #11 (0.046): 0.107*\"have_been\" + 0.012*\"year\" + 0.010*\"love\" + 0.009*\"first\" + 0.009*\"since\" + 0.008*\"also\" + 0.007*\"never\" + 0.007*\"little\" + 0.007*\"best\" + 0.006*\"over\"\n",
      "2020-03-17 21:33:53,684 : INFO : topic #4 (0.046): 0.034*\"better_than\" + 0.027*\"more_than\" + 0.026*\"than\" + 0.017*\"better\" + 0.013*\"restaurant\" + 0.011*\"have_been\" + 0.010*\"beer\" + 0.010*\"dish\" + 0.010*\"menu\" + 0.008*\"nice\"\n",
      "2020-03-17 21:33:53,685 : INFO : topic #6 (0.050): 0.021*\"your\" + 0.015*\"highly_recommend\" + 0.012*\"dish\" + 0.010*\"little\" + 0.010*\"make\" + 0.009*\"flavor\" + 0.009*\"best\" + 0.009*\"know\" + 0.008*\"recommend\" + 0.007*\"them\"\n",
      "2020-03-17 21:33:53,687 : INFO : topic #5 (0.051): 0.018*\"always\" + 0.017*\"love\" + 0.015*\"staff\" + 0.014*\"drink\" + 0.014*\"friendly\" + 0.011*\"location\" + 0.009*\"never\" + 0.009*\"night\" + 0.008*\"beer\" + 0.008*\"people\"\n",
      "2020-03-17 21:33:53,689 : INFO : topic #0 (0.055): 0.025*\"pizza\" + 0.016*\"salad\" + 0.013*\"sauce\" + 0.012*\"also\" + 0.012*\"sandwich\" + 0.011*\"delicious\" + 0.011*\"cheese\" + 0.010*\"chicken\" + 0.009*\"fresh\" + 0.008*\"best\"\n",
      "2020-03-17 21:33:53,690 : INFO : topic diff=0.105260, rho=0.447214\n",
      "2020-03-17 21:33:53,693 : INFO : PROGRESS: pass 4, at document #10000/10000\n",
      "2020-03-17 21:33:53,695 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:34:04,481 : DEBUG : 9994/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:34:04,570 : INFO : optimized alpha [0.054801643, 0.04642043, 0.04536045, 0.04440739, 0.043581717, 0.050683476, 0.04796991, 0.046051294, 0.044993594, 0.047671463, 0.04440021, 0.043940756, 0.043923933, 0.04505808]\n",
      "2020-03-17 21:34:04,571 : DEBUG : updating topics\n",
      "2020-03-17 21:34:04,574 : INFO : topic #4 (0.044): 0.042*\"better_than\" + 0.034*\"more_than\" + 0.032*\"than\" + 0.021*\"better\" + 0.013*\"restaurant\" + 0.010*\"beer\" + 0.010*\"dish\" + 0.010*\"menu\" + 0.009*\"have_been\" + 0.008*\"nice\"\n",
      "2020-03-17 21:34:04,576 : INFO : topic #12 (0.044): 0.025*\"sushi\" + 0.023*\"roll\" + 0.016*\"your\" + 0.013*\"restaurant\" + 0.011*\"order\" + 0.011*\"best\" + 0.010*\"mexican\" + 0.010*\"drink\" + 0.009*\"come\" + 0.008*\"happy_hour\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 21:34:04,578 : INFO : topic #6 (0.048): 0.022*\"your\" + 0.018*\"highly_recommend\" + 0.013*\"dish\" + 0.010*\"make\" + 0.010*\"flavor\" + 0.010*\"little\" + 0.010*\"recommend\" + 0.009*\"best\" + 0.009*\"know\" + 0.008*\"chicken\"\n",
      "2020-03-17 21:34:04,580 : INFO : topic #5 (0.051): 0.019*\"always\" + 0.019*\"love\" + 0.016*\"staff\" + 0.015*\"friendly\" + 0.014*\"drink\" + 0.012*\"location\" + 0.010*\"beer\" + 0.009*\"never\" + 0.009*\"night\" + 0.009*\"people\"\n",
      "2020-03-17 21:34:04,581 : INFO : topic #0 (0.055): 0.027*\"pizza\" + 0.017*\"salad\" + 0.013*\"sauce\" + 0.013*\"sandwich\" + 0.012*\"also\" + 0.012*\"delicious\" + 0.011*\"cheese\" + 0.011*\"chicken\" + 0.010*\"fresh\" + 0.009*\"lunch\"\n",
      "2020-03-17 21:34:04,582 : INFO : topic diff=0.094048, rho=0.408248\n",
      "2020-03-17 21:34:04,587 : INFO : PROGRESS: pass 5, at document #10000/10000\n",
      "2020-03-17 21:34:04,588 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:34:15,426 : DEBUG : 9995/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:34:15,482 : INFO : optimized alpha [0.054637846, 0.045717526, 0.04376585, 0.042662166, 0.04181324, 0.050734837, 0.046501637, 0.0450132, 0.04392843, 0.04705332, 0.043157566, 0.042427495, 0.042038977, 0.04391703]\n",
      "2020-03-17 21:34:15,483 : DEBUG : updating topics\n",
      "2020-03-17 21:34:15,485 : INFO : topic #4 (0.042): 0.048*\"better_than\" + 0.041*\"more_than\" + 0.038*\"than\" + 0.024*\"better\" + 0.013*\"restaurant\" + 0.010*\"dish\" + 0.010*\"beer\" + 0.009*\"menu\" + 0.008*\"other\" + 0.008*\"also\"\n",
      "2020-03-17 21:34:15,486 : INFO : topic #12 (0.042): 0.031*\"sushi\" + 0.027*\"roll\" + 0.016*\"your\" + 0.014*\"restaurant\" + 0.012*\"order\" + 0.011*\"best\" + 0.011*\"mexican\" + 0.010*\"drink\" + 0.009*\"come\" + 0.008*\"pretty\"\n",
      "2020-03-17 21:34:15,487 : INFO : topic #9 (0.047): 0.022*\"your\" + 0.020*\"store\" + 0.014*\"nice\" + 0.013*\"also\" + 0.011*\"price\" + 0.009*\"little\" + 0.009*\"find\" + 0.009*\"area\" + 0.009*\"other\" + 0.008*\"room\"\n",
      "2020-03-17 21:34:15,488 : INFO : topic #5 (0.051): 0.020*\"love\" + 0.020*\"always\" + 0.016*\"staff\" + 0.016*\"friendly\" + 0.015*\"drink\" + 0.012*\"location\" + 0.011*\"beer\" + 0.010*\"night\" + 0.009*\"never\" + 0.009*\"music\"\n",
      "2020-03-17 21:34:15,489 : INFO : topic #0 (0.055): 0.028*\"pizza\" + 0.018*\"salad\" + 0.014*\"sandwich\" + 0.014*\"sauce\" + 0.012*\"chicken\" + 0.012*\"cheese\" + 0.012*\"also\" + 0.012*\"delicious\" + 0.010*\"fresh\" + 0.009*\"lunch\"\n",
      "2020-03-17 21:34:15,491 : INFO : topic diff=0.085848, rho=0.377964\n",
      "2020-03-17 21:34:15,494 : INFO : PROGRESS: pass 6, at document #10000/10000\n",
      "2020-03-17 21:34:15,495 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:34:25,478 : DEBUG : 9998/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:34:25,540 : INFO : optimized alpha [0.054587603, 0.04527018, 0.042570386, 0.041185107, 0.0403265, 0.05091682, 0.045324028, 0.04417874, 0.04311394, 0.04665707, 0.04210444, 0.041164026, 0.040494427, 0.04313896]\n",
      "2020-03-17 21:34:25,541 : DEBUG : updating topics\n",
      "2020-03-17 21:34:25,543 : INFO : topic #4 (0.040): 0.054*\"better_than\" + 0.048*\"more_than\" + 0.044*\"than\" + 0.027*\"better\" + 0.013*\"restaurant\" + 0.010*\"dish\" + 0.009*\"beer\" + 0.009*\"menu\" + 0.008*\"other\" + 0.008*\"also\"\n",
      "2020-03-17 21:34:25,544 : INFO : topic #12 (0.040): 0.035*\"sushi\" + 0.031*\"roll\" + 0.016*\"your\" + 0.015*\"restaurant\" + 0.012*\"order\" + 0.012*\"best\" + 0.012*\"mexican\" + 0.009*\"drink\" + 0.009*\"come\" + 0.008*\"pretty\"\n",
      "2020-03-17 21:34:25,545 : INFO : topic #9 (0.047): 0.022*\"your\" + 0.021*\"store\" + 0.014*\"nice\" + 0.014*\"also\" + 0.011*\"price\" + 0.009*\"find\" + 0.009*\"little\" + 0.009*\"area\" + 0.009*\"room\" + 0.009*\"other\"\n",
      "2020-03-17 21:34:25,547 : INFO : topic #5 (0.051): 0.021*\"love\" + 0.020*\"always\" + 0.017*\"staff\" + 0.016*\"friendly\" + 0.015*\"drink\" + 0.013*\"location\" + 0.012*\"beer\" + 0.010*\"night\" + 0.010*\"music\" + 0.009*\"never\"\n",
      "2020-03-17 21:34:25,549 : INFO : topic #0 (0.055): 0.030*\"pizza\" + 0.019*\"salad\" + 0.015*\"sandwich\" + 0.014*\"sauce\" + 0.013*\"chicken\" + 0.012*\"cheese\" + 0.012*\"also\" + 0.012*\"delicious\" + 0.010*\"fresh\" + 0.010*\"lunch\"\n",
      "2020-03-17 21:34:25,551 : INFO : topic diff=0.080022, rho=0.353553\n",
      "2020-03-17 21:34:25,555 : INFO : PROGRESS: pass 7, at document #10000/10000\n",
      "2020-03-17 21:34:25,557 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:34:34,873 : DEBUG : 10000/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:34:34,931 : INFO : optimized alpha [0.054591842, 0.045001213, 0.04170609, 0.039926656, 0.039103966, 0.051117893, 0.04442721, 0.043498285, 0.04248081, 0.046449002, 0.041224726, 0.040073372, 0.03920229, 0.0425864]\n",
      "2020-03-17 21:34:34,932 : DEBUG : updating topics\n",
      "2020-03-17 21:34:34,934 : INFO : topic #4 (0.039): 0.060*\"better_than\" + 0.055*\"more_than\" + 0.049*\"than\" + 0.030*\"better\" + 0.013*\"restaurant\" + 0.009*\"dish\" + 0.009*\"menu\" + 0.009*\"beer\" + 0.008*\"other\" + 0.008*\"much\"\n",
      "2020-03-17 21:34:34,935 : INFO : topic #12 (0.039): 0.039*\"sushi\" + 0.034*\"roll\" + 0.016*\"restaurant\" + 0.016*\"your\" + 0.012*\"best\" + 0.012*\"order\" + 0.012*\"mexican\" + 0.009*\"come\" + 0.009*\"drink\" + 0.009*\"pretty\"\n",
      "2020-03-17 21:34:34,936 : INFO : topic #9 (0.046): 0.022*\"your\" + 0.022*\"store\" + 0.014*\"nice\" + 0.014*\"also\" + 0.011*\"price\" + 0.010*\"find\" + 0.010*\"room\" + 0.009*\"little\" + 0.009*\"area\" + 0.009*\"shop\"\n",
      "2020-03-17 21:34:34,937 : INFO : topic #5 (0.051): 0.022*\"love\" + 0.021*\"always\" + 0.017*\"staff\" + 0.017*\"friendly\" + 0.016*\"drink\" + 0.013*\"location\" + 0.013*\"beer\" + 0.011*\"night\" + 0.010*\"music\" + 0.009*\"never\"\n",
      "2020-03-17 21:34:34,938 : INFO : topic #0 (0.055): 0.031*\"pizza\" + 0.020*\"salad\" + 0.016*\"sandwich\" + 0.014*\"sauce\" + 0.013*\"cheese\" + 0.013*\"chicken\" + 0.012*\"also\" + 0.012*\"delicious\" + 0.010*\"fresh\" + 0.010*\"lunch\"\n",
      "2020-03-17 21:34:34,939 : INFO : topic diff=0.076718, rho=0.333333\n",
      "2020-03-17 21:34:34,941 : INFO : PROGRESS: pass 8, at document #10000/10000\n",
      "2020-03-17 21:34:34,942 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:34:44,002 : DEBUG : 9999/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:34:44,051 : INFO : optimized alpha [0.05466241, 0.044883557, 0.041077495, 0.038835645, 0.03805424, 0.051356874, 0.043732353, 0.042931322, 0.042016566, 0.046351027, 0.04046996, 0.03912135, 0.038065884, 0.042205606]\n",
      "2020-03-17 21:34:44,052 : DEBUG : updating topics\n",
      "2020-03-17 21:34:44,053 : INFO : topic #12 (0.038): 0.042*\"sushi\" + 0.037*\"roll\" + 0.017*\"restaurant\" + 0.016*\"your\" + 0.012*\"best\" + 0.012*\"order\" + 0.012*\"mexican\" + 0.009*\"come\" + 0.009*\"pretty\" + 0.008*\"star\"\n",
      "2020-03-17 21:34:44,054 : INFO : topic #4 (0.038): 0.064*\"better_than\" + 0.062*\"more_than\" + 0.054*\"than\" + 0.033*\"better\" + 0.012*\"restaurant\" + 0.009*\"dish\" + 0.009*\"much\" + 0.008*\"other\" + 0.008*\"menu\" + 0.008*\"beer\"\n",
      "2020-03-17 21:34:44,055 : INFO : topic #9 (0.046): 0.023*\"your\" + 0.022*\"store\" + 0.014*\"nice\" + 0.014*\"also\" + 0.011*\"price\" + 0.010*\"room\" + 0.010*\"find\" + 0.010*\"little\" + 0.010*\"area\" + 0.009*\"shop\"\n",
      "2020-03-17 21:34:44,056 : INFO : topic #5 (0.051): 0.023*\"love\" + 0.021*\"always\" + 0.017*\"staff\" + 0.017*\"friendly\" + 0.016*\"drink\" + 0.014*\"beer\" + 0.014*\"location\" + 0.011*\"night\" + 0.011*\"music\" + 0.009*\"never\"\n",
      "2020-03-17 21:34:44,057 : INFO : topic #0 (0.055): 0.033*\"pizza\" + 0.021*\"salad\" + 0.017*\"sandwich\" + 0.014*\"sauce\" + 0.014*\"cheese\" + 0.013*\"chicken\" + 0.012*\"also\" + 0.012*\"delicious\" + 0.010*\"fresh\" + 0.010*\"sweet\"\n",
      "2020-03-17 21:34:44,058 : INFO : topic diff=0.074916, rho=0.316228\n",
      "2020-03-17 21:34:44,061 : INFO : PROGRESS: pass 9, at document #10000/10000\n",
      "2020-03-17 21:34:44,062 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:34:52,969 : DEBUG : 9999/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:34:53,030 : INFO : optimized alpha [0.05478474, 0.044852335, 0.04062654, 0.037901655, 0.037176076, 0.051643983, 0.043168165, 0.042438935, 0.041674286, 0.046349943, 0.03981429, 0.038278863, 0.037075482, 0.041953288]\n",
      "2020-03-17 21:34:53,031 : DEBUG : updating topics\n",
      "2020-03-17 21:34:53,032 : INFO : topic #12 (0.037): 0.045*\"sushi\" + 0.040*\"roll\" + 0.018*\"restaurant\" + 0.016*\"your\" + 0.013*\"best\" + 0.012*\"mexican\" + 0.012*\"order\" + 0.009*\"pretty\" + 0.009*\"come\" + 0.009*\"spicy\"\n",
      "2020-03-17 21:34:53,033 : INFO : topic #4 (0.037): 0.069*\"more_than\" + 0.068*\"better_than\" + 0.059*\"than\" + 0.036*\"better\" + 0.012*\"restaurant\" + 0.009*\"much\" + 0.009*\"dish\" + 0.009*\"other\" + 0.008*\"menu\" + 0.007*\"beer\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 21:34:53,034 : INFO : topic #9 (0.046): 0.023*\"store\" + 0.023*\"your\" + 0.015*\"nice\" + 0.014*\"also\" + 0.012*\"price\" + 0.011*\"room\" + 0.011*\"find\" + 0.010*\"little\" + 0.010*\"area\" + 0.010*\"shop\"\n",
      "2020-03-17 21:34:53,035 : INFO : topic #5 (0.052): 0.024*\"love\" + 0.021*\"always\" + 0.017*\"staff\" + 0.017*\"drink\" + 0.017*\"friendly\" + 0.015*\"beer\" + 0.014*\"location\" + 0.012*\"night\" + 0.011*\"music\" + 0.009*\"people\"\n",
      "2020-03-17 21:34:53,036 : INFO : topic #0 (0.055): 0.034*\"pizza\" + 0.022*\"salad\" + 0.018*\"sandwich\" + 0.014*\"sauce\" + 0.014*\"cheese\" + 0.013*\"chicken\" + 0.012*\"also\" + 0.012*\"delicious\" + 0.011*\"sweet\" + 0.011*\"lunch\"\n",
      "2020-03-17 21:34:53,037 : INFO : topic diff=0.074428, rho=0.301511\n",
      "2020-03-17 21:34:53,040 : INFO : PROGRESS: pass 10, at document #10000/10000\n",
      "2020-03-17 21:34:53,041 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:35:01,928 : DEBUG : 9998/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:35:01,984 : INFO : optimized alpha [0.054951005, 0.044861693, 0.040359918, 0.03708979, 0.036399744, 0.051896147, 0.042720504, 0.041991856, 0.041479137, 0.04637858, 0.039236553, 0.037524335, 0.036219798, 0.041793976]\n",
      "2020-03-17 21:35:01,985 : DEBUG : updating topics\n",
      "2020-03-17 21:35:01,987 : INFO : topic #12 (0.036): 0.048*\"sushi\" + 0.043*\"roll\" + 0.019*\"restaurant\" + 0.016*\"your\" + 0.013*\"best\" + 0.012*\"order\" + 0.012*\"mexican\" + 0.010*\"pretty\" + 0.009*\"spicy\" + 0.009*\"come\"\n",
      "2020-03-17 21:35:01,988 : INFO : topic #4 (0.036): 0.076*\"more_than\" + 0.071*\"better_than\" + 0.064*\"than\" + 0.038*\"better\" + 0.012*\"restaurant\" + 0.010*\"much\" + 0.009*\"other\" + 0.008*\"dish\" + 0.008*\"menu\" + 0.007*\"taste\"\n",
      "2020-03-17 21:35:01,989 : INFO : topic #9 (0.046): 0.023*\"store\" + 0.023*\"your\" + 0.015*\"nice\" + 0.014*\"also\" + 0.012*\"price\" + 0.012*\"room\" + 0.011*\"find\" + 0.010*\"shop\" + 0.010*\"little\" + 0.010*\"area\"\n",
      "2020-03-17 21:35:01,990 : INFO : topic #5 (0.052): 0.025*\"love\" + 0.021*\"always\" + 0.017*\"staff\" + 0.017*\"drink\" + 0.017*\"friendly\" + 0.016*\"beer\" + 0.015*\"location\" + 0.012*\"night\" + 0.012*\"music\" + 0.009*\"people\"\n",
      "2020-03-17 21:35:01,991 : INFO : topic #0 (0.055): 0.035*\"pizza\" + 0.023*\"salad\" + 0.019*\"sandwich\" + 0.015*\"cheese\" + 0.015*\"sauce\" + 0.013*\"chicken\" + 0.012*\"also\" + 0.012*\"delicious\" + 0.011*\"sweet\" + 0.011*\"lunch\"\n",
      "2020-03-17 21:35:01,992 : INFO : topic diff=0.074567, rho=0.288675\n",
      "2020-03-17 21:35:01,996 : INFO : PROGRESS: pass 11, at document #10000/10000\n",
      "2020-03-17 21:35:01,996 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:35:10,685 : DEBUG : 9999/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:35:10,740 : INFO : optimized alpha [0.055114776, 0.0449508, 0.040194344, 0.036363512, 0.03571294, 0.05219788, 0.042353384, 0.041597035, 0.04135501, 0.04645532, 0.038718935, 0.036851782, 0.035443354, 0.041719873]\n",
      "2020-03-17 21:35:10,741 : DEBUG : updating topics\n",
      "2020-03-17 21:35:10,743 : INFO : topic #12 (0.035): 0.050*\"sushi\" + 0.045*\"roll\" + 0.019*\"restaurant\" + 0.016*\"your\" + 0.013*\"best\" + 0.012*\"order\" + 0.012*\"mexican\" + 0.010*\"pretty\" + 0.009*\"spicy\" + 0.009*\"come\"\n",
      "2020-03-17 21:35:10,744 : INFO : topic #4 (0.036): 0.082*\"more_than\" + 0.074*\"better_than\" + 0.068*\"than\" + 0.040*\"better\" + 0.011*\"restaurant\" + 0.011*\"much\" + 0.009*\"other\" + 0.008*\"dish\" + 0.007*\"menu\" + 0.007*\"taste\"\n",
      "2020-03-17 21:35:10,745 : INFO : topic #9 (0.046): 0.024*\"store\" + 0.023*\"your\" + 0.015*\"nice\" + 0.014*\"also\" + 0.012*\"room\" + 0.012*\"price\" + 0.011*\"find\" + 0.010*\"shop\" + 0.010*\"area\" + 0.010*\"little\"\n",
      "2020-03-17 21:35:10,747 : INFO : topic #5 (0.052): 0.026*\"love\" + 0.022*\"always\" + 0.018*\"drink\" + 0.017*\"staff\" + 0.017*\"friendly\" + 0.017*\"beer\" + 0.015*\"location\" + 0.012*\"night\" + 0.012*\"music\" + 0.009*\"people\"\n",
      "2020-03-17 21:35:10,748 : INFO : topic #0 (0.055): 0.036*\"pizza\" + 0.023*\"salad\" + 0.020*\"sandwich\" + 0.015*\"cheese\" + 0.015*\"sauce\" + 0.013*\"chicken\" + 0.012*\"also\" + 0.012*\"delicious\" + 0.011*\"sweet\" + 0.011*\"lunch\"\n",
      "2020-03-17 21:35:10,749 : INFO : topic diff=0.075122, rho=0.277350\n",
      "2020-03-17 21:35:10,752 : INFO : PROGRESS: pass 12, at document #10000/10000\n",
      "2020-03-17 21:35:10,753 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:35:19,340 : DEBUG : 10000/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:35:19,396 : INFO : optimized alpha [0.05524479, 0.04511038, 0.040163446, 0.035706617, 0.03509629, 0.05250264, 0.042027667, 0.041246228, 0.041295063, 0.046572395, 0.038278572, 0.036253713, 0.034768876, 0.041677326]\n",
      "2020-03-17 21:35:19,397 : DEBUG : updating topics\n",
      "2020-03-17 21:35:19,399 : INFO : topic #12 (0.035): 0.051*\"sushi\" + 0.047*\"roll\" + 0.020*\"restaurant\" + 0.016*\"your\" + 0.014*\"best\" + 0.012*\"order\" + 0.012*\"mexican\" + 0.010*\"pretty\" + 0.010*\"spicy\" + 0.009*\"come\"\n",
      "2020-03-17 21:35:19,401 : INFO : topic #4 (0.035): 0.087*\"more_than\" + 0.077*\"better_than\" + 0.072*\"than\" + 0.042*\"better\" + 0.011*\"much\" + 0.011*\"restaurant\" + 0.009*\"other\" + 0.007*\"dish\" + 0.007*\"taste\" + 0.007*\"menu\"\n",
      "2020-03-17 21:35:19,402 : INFO : topic #9 (0.047): 0.024*\"store\" + 0.023*\"your\" + 0.015*\"nice\" + 0.014*\"also\" + 0.013*\"room\" + 0.012*\"price\" + 0.011*\"find\" + 0.011*\"shop\" + 0.010*\"area\" + 0.010*\"little\"\n",
      "2020-03-17 21:35:19,403 : INFO : topic #5 (0.053): 0.026*\"love\" + 0.022*\"always\" + 0.018*\"drink\" + 0.018*\"beer\" + 0.017*\"staff\" + 0.017*\"friendly\" + 0.015*\"location\" + 0.013*\"night\" + 0.012*\"music\" + 0.009*\"nice\"\n",
      "2020-03-17 21:35:19,404 : INFO : topic #0 (0.055): 0.036*\"pizza\" + 0.024*\"salad\" + 0.021*\"sandwich\" + 0.016*\"cheese\" + 0.015*\"sauce\" + 0.013*\"chicken\" + 0.012*\"also\" + 0.012*\"delicious\" + 0.011*\"sweet\" + 0.011*\"lunch\"\n",
      "2020-03-17 21:35:19,404 : INFO : topic diff=0.076246, rho=0.267261\n",
      "2020-03-17 21:35:19,407 : INFO : PROGRESS: pass 13, at document #10000/10000\n",
      "2020-03-17 21:35:19,408 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:35:27,986 : DEBUG : 9999/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:35:28,045 : INFO : optimized alpha [0.05537245, 0.04531511, 0.040235445, 0.03511094, 0.034533177, 0.052816782, 0.041781373, 0.04092688, 0.041302286, 0.046698097, 0.037907686, 0.035713866, 0.034171775, 0.0416993]\n",
      "2020-03-17 21:35:28,046 : DEBUG : updating topics\n",
      "2020-03-17 21:35:28,048 : INFO : topic #12 (0.034): 0.053*\"sushi\" + 0.049*\"roll\" + 0.020*\"restaurant\" + 0.015*\"your\" + 0.014*\"best\" + 0.012*\"order\" + 0.012*\"mexican\" + 0.011*\"pretty\" + 0.010*\"spicy\" + 0.009*\"fresh\"\n",
      "2020-03-17 21:35:28,049 : INFO : topic #4 (0.035): 0.092*\"more_than\" + 0.079*\"better_than\" + 0.076*\"than\" + 0.044*\"better\" + 0.012*\"much\" + 0.010*\"restaurant\" + 0.009*\"other\" + 0.007*\"taste\" + 0.007*\"dish\" + 0.007*\"think\"\n",
      "2020-03-17 21:35:28,050 : INFO : topic #9 (0.047): 0.025*\"store\" + 0.023*\"your\" + 0.015*\"nice\" + 0.014*\"also\" + 0.013*\"room\" + 0.012*\"price\" + 0.012*\"find\" + 0.011*\"shop\" + 0.010*\"area\" + 0.010*\"little\"\n",
      "2020-03-17 21:35:28,051 : INFO : topic #5 (0.053): 0.027*\"love\" + 0.022*\"always\" + 0.019*\"drink\" + 0.018*\"beer\" + 0.017*\"staff\" + 0.016*\"friendly\" + 0.015*\"location\" + 0.013*\"night\" + 0.013*\"music\" + 0.010*\"nice\"\n",
      "2020-03-17 21:35:28,052 : INFO : topic #0 (0.055): 0.037*\"pizza\" + 0.025*\"salad\" + 0.023*\"sandwich\" + 0.016*\"cheese\" + 0.015*\"sauce\" + 0.013*\"chicken\" + 0.012*\"also\" + 0.012*\"delicious\" + 0.011*\"sweet\" + 0.011*\"lunch\"\n",
      "2020-03-17 21:35:28,053 : INFO : topic diff=0.077551, rho=0.258199\n",
      "2020-03-17 21:35:28,057 : INFO : PROGRESS: pass 14, at document #10000/10000\n",
      "2020-03-17 21:35:28,058 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:35:36,748 : DEBUG : 10000/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:35:36,808 : INFO : optimized alpha [0.05550347, 0.045551457, 0.04040834, 0.03456394, 0.0340221, 0.05312345, 0.04160766, 0.040636584, 0.04136496, 0.04683961, 0.03756745, 0.03522361, 0.033624165, 0.0417802]\n",
      "2020-03-17 21:35:36,810 : DEBUG : updating topics\n",
      "2020-03-17 21:35:36,813 : INFO : topic #12 (0.034): 0.054*\"sushi\" + 0.051*\"roll\" + 0.021*\"restaurant\" + 0.015*\"your\" + 0.014*\"best\" + 0.012*\"order\" + 0.011*\"mexican\" + 0.011*\"pretty\" + 0.011*\"spicy\" + 0.009*\"fresh\"\n",
      "2020-03-17 21:35:36,814 : INFO : topic #4 (0.034): 0.097*\"more_than\" + 0.082*\"better_than\" + 0.080*\"than\" + 0.046*\"better\" + 0.012*\"much\" + 0.010*\"restaurant\" + 0.009*\"other\" + 0.007*\"taste\" + 0.007*\"think\" + 0.007*\"dish\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 21:35:36,815 : INFO : topic #9 (0.047): 0.025*\"store\" + 0.023*\"your\" + 0.015*\"nice\" + 0.015*\"also\" + 0.014*\"room\" + 0.012*\"price\" + 0.012*\"find\" + 0.011*\"shop\" + 0.010*\"area\" + 0.010*\"little\"\n",
      "2020-03-17 21:35:36,816 : INFO : topic #5 (0.053): 0.027*\"love\" + 0.022*\"always\" + 0.019*\"beer\" + 0.019*\"drink\" + 0.017*\"staff\" + 0.016*\"friendly\" + 0.016*\"location\" + 0.014*\"night\" + 0.013*\"music\" + 0.010*\"nice\"\n",
      "2020-03-17 21:35:36,817 : INFO : topic #0 (0.056): 0.038*\"pizza\" + 0.025*\"salad\" + 0.024*\"sandwich\" + 0.017*\"cheese\" + 0.015*\"sauce\" + 0.013*\"chicken\" + 0.012*\"also\" + 0.012*\"delicious\" + 0.012*\"sweet\" + 0.012*\"lunch\"\n",
      "2020-03-17 21:35:36,818 : INFO : topic diff=0.079000, rho=0.250000\n",
      "2020-03-17 21:35:36,821 : INFO : PROGRESS: pass 15, at document #10000/10000\n",
      "2020-03-17 21:35:36,822 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:35:45,371 : DEBUG : 10000/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:35:45,430 : INFO : optimized alpha [0.055613242, 0.045835413, 0.04063974, 0.03406961, 0.033547822, 0.05342345, 0.04147343, 0.04035681, 0.04149106, 0.047009677, 0.03727563, 0.03477911, 0.03313871, 0.041887525]\n",
      "2020-03-17 21:35:45,431 : DEBUG : updating topics\n",
      "2020-03-17 21:35:45,433 : INFO : topic #12 (0.033): 0.056*\"sushi\" + 0.052*\"roll\" + 0.021*\"restaurant\" + 0.015*\"your\" + 0.015*\"best\" + 0.012*\"order\" + 0.012*\"pretty\" + 0.011*\"mexican\" + 0.011*\"spicy\" + 0.010*\"fresh\"\n",
      "2020-03-17 21:35:45,434 : INFO : topic #4 (0.034): 0.101*\"more_than\" + 0.084*\"better_than\" + 0.083*\"than\" + 0.048*\"better\" + 0.013*\"much\" + 0.010*\"restaurant\" + 0.009*\"other\" + 0.007*\"think\" + 0.007*\"taste\" + 0.007*\"thing\"\n",
      "2020-03-17 21:35:45,435 : INFO : topic #9 (0.047): 0.025*\"store\" + 0.023*\"your\" + 0.015*\"nice\" + 0.015*\"also\" + 0.014*\"room\" + 0.012*\"price\" + 0.012*\"find\" + 0.011*\"shop\" + 0.010*\"area\" + 0.010*\"little\"\n",
      "2020-03-17 21:35:45,436 : INFO : topic #5 (0.053): 0.028*\"love\" + 0.022*\"always\" + 0.020*\"beer\" + 0.019*\"drink\" + 0.017*\"staff\" + 0.016*\"location\" + 0.016*\"friendly\" + 0.014*\"night\" + 0.013*\"music\" + 0.010*\"nice\"\n",
      "2020-03-17 21:35:45,437 : INFO : topic #0 (0.056): 0.039*\"pizza\" + 0.026*\"salad\" + 0.024*\"sandwich\" + 0.017*\"cheese\" + 0.015*\"sauce\" + 0.013*\"chicken\" + 0.012*\"also\" + 0.012*\"delicious\" + 0.012*\"sweet\" + 0.012*\"lunch\"\n",
      "2020-03-17 21:35:45,437 : INFO : topic diff=0.080066, rho=0.242536\n",
      "2020-03-17 21:35:45,440 : INFO : PROGRESS: pass 16, at document #10000/10000\n",
      "2020-03-17 21:35:45,441 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:35:54,075 : DEBUG : 10000/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:35:54,129 : INFO : optimized alpha [0.05574883, 0.046136487, 0.040945426, 0.033616204, 0.033117566, 0.05368824, 0.041386347, 0.040096227, 0.041665263, 0.04717065, 0.037009485, 0.034369145, 0.032685436, 0.04203163]\n",
      "2020-03-17 21:35:54,130 : DEBUG : updating topics\n",
      "2020-03-17 21:35:54,132 : INFO : topic #12 (0.033): 0.057*\"sushi\" + 0.054*\"roll\" + 0.022*\"restaurant\" + 0.015*\"best\" + 0.015*\"your\" + 0.012*\"pretty\" + 0.012*\"order\" + 0.011*\"spicy\" + 0.011*\"mexican\" + 0.010*\"fresh\"\n",
      "2020-03-17 21:35:54,133 : INFO : topic #4 (0.033): 0.105*\"more_than\" + 0.086*\"than\" + 0.086*\"better_than\" + 0.049*\"better\" + 0.013*\"much\" + 0.010*\"other\" + 0.009*\"restaurant\" + 0.007*\"think\" + 0.007*\"taste\" + 0.007*\"thing\"\n",
      "2020-03-17 21:35:54,134 : INFO : topic #9 (0.047): 0.025*\"store\" + 0.023*\"your\" + 0.015*\"nice\" + 0.015*\"room\" + 0.015*\"also\" + 0.012*\"price\" + 0.012*\"find\" + 0.012*\"shop\" + 0.010*\"area\" + 0.010*\"little\"\n",
      "2020-03-17 21:35:54,135 : INFO : topic #5 (0.054): 0.028*\"love\" + 0.021*\"always\" + 0.021*\"beer\" + 0.020*\"drink\" + 0.016*\"staff\" + 0.016*\"location\" + 0.015*\"friendly\" + 0.014*\"night\" + 0.013*\"music\" + 0.011*\"nice\"\n",
      "2020-03-17 21:35:54,136 : INFO : topic #0 (0.056): 0.040*\"pizza\" + 0.026*\"salad\" + 0.025*\"sandwich\" + 0.018*\"cheese\" + 0.015*\"sauce\" + 0.013*\"chicken\" + 0.012*\"also\" + 0.012*\"sweet\" + 0.012*\"lunch\" + 0.012*\"delicious\"\n",
      "2020-03-17 21:35:54,137 : INFO : topic diff=0.081243, rho=0.235702\n",
      "2020-03-17 21:35:54,140 : INFO : PROGRESS: pass 17, at document #10000/10000\n",
      "2020-03-17 21:35:54,142 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:36:02,513 : DEBUG : 10000/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:36:02,577 : INFO : optimized alpha [0.05592179, 0.046442658, 0.04127527, 0.03319789, 0.03272362, 0.053937756, 0.04133196, 0.039864086, 0.041872337, 0.047359224, 0.036783103, 0.033989582, 0.03226909, 0.042203918]\n",
      "2020-03-17 21:36:02,578 : DEBUG : updating topics\n",
      "2020-03-17 21:36:02,580 : INFO : topic #12 (0.032): 0.058*\"sushi\" + 0.055*\"roll\" + 0.022*\"restaurant\" + 0.015*\"best\" + 0.014*\"your\" + 0.012*\"pretty\" + 0.012*\"order\" + 0.012*\"spicy\" + 0.010*\"mexican\" + 0.010*\"fresh\"\n",
      "2020-03-17 21:36:02,580 : INFO : topic #4 (0.033): 0.108*\"more_than\" + 0.089*\"than\" + 0.088*\"better_than\" + 0.050*\"better\" + 0.014*\"much\" + 0.010*\"other\" + 0.009*\"restaurant\" + 0.007*\"think\" + 0.007*\"taste\" + 0.007*\"le\"\n",
      "2020-03-17 21:36:02,581 : INFO : topic #9 (0.047): 0.025*\"store\" + 0.023*\"your\" + 0.015*\"nice\" + 0.015*\"room\" + 0.015*\"also\" + 0.012*\"find\" + 0.012*\"price\" + 0.012*\"shop\" + 0.010*\"area\" + 0.010*\"little\"\n",
      "2020-03-17 21:36:02,582 : INFO : topic #5 (0.054): 0.028*\"love\" + 0.021*\"always\" + 0.021*\"beer\" + 0.020*\"drink\" + 0.016*\"location\" + 0.016*\"staff\" + 0.015*\"friendly\" + 0.015*\"night\" + 0.014*\"music\" + 0.011*\"nice\"\n",
      "2020-03-17 21:36:02,583 : INFO : topic #0 (0.056): 0.040*\"pizza\" + 0.027*\"salad\" + 0.026*\"sandwich\" + 0.018*\"cheese\" + 0.015*\"sauce\" + 0.013*\"chicken\" + 0.012*\"also\" + 0.012*\"lunch\" + 0.012*\"sweet\" + 0.012*\"delicious\"\n",
      "2020-03-17 21:36:02,584 : INFO : topic diff=0.082048, rho=0.229416\n",
      "2020-03-17 21:36:02,589 : INFO : PROGRESS: pass 18, at document #10000/10000\n",
      "2020-03-17 21:36:02,590 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:36:10,896 : DEBUG : 10000/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:36:10,951 : INFO : optimized alpha [0.056101155, 0.046754543, 0.0416508, 0.032814197, 0.032368958, 0.054209284, 0.041301634, 0.039627355, 0.042078666, 0.047522813, 0.036597144, 0.033645257, 0.031897485, 0.04242221]\n",
      "2020-03-17 21:36:10,952 : DEBUG : updating topics\n",
      "2020-03-17 21:36:10,953 : INFO : topic #12 (0.032): 0.058*\"sushi\" + 0.056*\"roll\" + 0.023*\"restaurant\" + 0.015*\"best\" + 0.014*\"your\" + 0.013*\"pretty\" + 0.012*\"spicy\" + 0.012*\"order\" + 0.010*\"fresh\" + 0.010*\"mexican\"\n",
      "2020-03-17 21:36:10,954 : INFO : topic #4 (0.032): 0.111*\"more_than\" + 0.092*\"than\" + 0.090*\"better_than\" + 0.052*\"better\" + 0.014*\"much\" + 0.010*\"other\" + 0.009*\"restaurant\" + 0.007*\"think\" + 0.007*\"le\" + 0.007*\"thing\"\n",
      "2020-03-17 21:36:10,956 : INFO : topic #9 (0.048): 0.026*\"store\" + 0.023*\"your\" + 0.016*\"room\" + 0.015*\"nice\" + 0.015*\"also\" + 0.013*\"find\" + 0.012*\"price\" + 0.012*\"shop\" + 0.010*\"area\" + 0.010*\"little\"\n",
      "2020-03-17 21:36:10,956 : INFO : topic #5 (0.054): 0.029*\"love\" + 0.022*\"beer\" + 0.021*\"always\" + 0.020*\"drink\" + 0.017*\"location\" + 0.016*\"staff\" + 0.015*\"night\" + 0.015*\"friendly\" + 0.014*\"music\" + 0.011*\"nice\"\n",
      "2020-03-17 21:36:10,957 : INFO : topic #0 (0.056): 0.041*\"pizza\" + 0.027*\"salad\" + 0.027*\"sandwich\" + 0.019*\"cheese\" + 0.015*\"sauce\" + 0.013*\"chicken\" + 0.012*\"also\" + 0.012*\"lunch\" + 0.012*\"sweet\" + 0.012*\"delicious\"\n",
      "2020-03-17 21:36:10,958 : INFO : topic diff=0.082741, rho=0.223607\n",
      "2020-03-17 21:36:10,960 : INFO : PROGRESS: pass 19, at document #10000/10000\n",
      "2020-03-17 21:36:10,961 : DEBUG : performing inference on a chunk of 10000 documents\n",
      "2020-03-17 21:36:19,157 : DEBUG : 10000/10000 documents converged within 500 iterations\n",
      "2020-03-17 21:36:19,209 : INFO : optimized alpha [0.05628843, 0.047067378, 0.042059563, 0.032459255, 0.03204083, 0.05444935, 0.041302603, 0.039418925, 0.042334147, 0.047703054, 0.036430083, 0.033329345, 0.031551875, 0.04263335]\n",
      "2020-03-17 21:36:19,210 : DEBUG : updating topics\n",
      "2020-03-17 21:36:19,213 : INFO : topic #12 (0.032): 0.059*\"sushi\" + 0.057*\"roll\" + 0.023*\"restaurant\" + 0.016*\"best\" + 0.014*\"your\" + 0.013*\"pretty\" + 0.012*\"spicy\" + 0.012*\"order\" + 0.010*\"fresh\" + 0.010*\"mexican\"\n",
      "2020-03-17 21:36:19,215 : INFO : topic #4 (0.032): 0.114*\"more_than\" + 0.095*\"than\" + 0.092*\"better_than\" + 0.053*\"better\" + 0.014*\"much\" + 0.010*\"other\" + 0.009*\"restaurant\" + 0.007*\"le\" + 0.007*\"think\" + 0.007*\"thing\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 21:36:19,216 : INFO : topic #9 (0.048): 0.026*\"store\" + 0.023*\"your\" + 0.016*\"room\" + 0.015*\"nice\" + 0.015*\"also\" + 0.013*\"find\" + 0.012*\"price\" + 0.012*\"shop\" + 0.010*\"area\" + 0.010*\"little\"\n",
      "2020-03-17 21:36:19,217 : INFO : topic #5 (0.054): 0.029*\"love\" + 0.022*\"beer\" + 0.021*\"always\" + 0.021*\"drink\" + 0.017*\"location\" + 0.016*\"staff\" + 0.016*\"night\" + 0.014*\"friendly\" + 0.014*\"music\" + 0.011*\"nice\"\n",
      "2020-03-17 21:36:19,218 : INFO : topic #0 (0.056): 0.041*\"pizza\" + 0.028*\"sandwich\" + 0.028*\"salad\" + 0.019*\"cheese\" + 0.015*\"sauce\" + 0.013*\"chicken\" + 0.013*\"also\" + 0.012*\"lunch\" + 0.012*\"sweet\" + 0.012*\"delicious\"\n",
      "2020-03-17 21:36:19,220 : INFO : topic diff=0.082916, rho=0.218218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 14\n",
    "chunksize = 10000 # Size of the doc looked at every pass #500\n",
    "passes = 20 # Number of passes through documents #50\n",
    "iterations = 500 # Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 1 for Determining Optimal Number of Topics: Topic Coherence\n",
    "\n",
    "Topic Coherence is a measure used to evaluate topic models. Each such generated topic consists of words, and the topic coherence is applied to the top N words from the topic. \n",
    "\n",
    "Topic Coherence measures score a single topic by **measuring the degree of semantic similarity between high scoring words in the topic**. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. \n",
    "\n",
    "A set of statements or facts is said to be coherent, if they support each other. Thus, a coherent fact set can be interpreted in a context that covers all or most of the facts. An example of a coherent fact set is “the game is a team sport”, “the game is played with a ball”, “the game demands great physical efforts”\n",
    "\n",
    "Topic Coherence is defined as the average of the pairwise word-similarity scores of the words in the topic.\n",
    "\n",
    "A good model will generate coherent topics, i.e., topics with high topic coherence scores. Good topics are topics that can be described by a short label, therefore this is what the topic coherence measure should capture.\n",
    "\n",
    "\n",
    "Below we display \n",
    "- the average topic coherence and\n",
    "- print the topics in order of topic coherence\n",
    "\n",
    "\n",
    "We use LdaModel's \"top_topics\" method to get the topics with the highest coherence score the coherence for each topic.\n",
    "\n",
    "Note that we use the “Umass” topic coherence measure here (see gensim.models.ldamodel.LdaModel.top_topics())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 21:36:19,229 : DEBUG : Setting topics to those of the model: LdaModel(num_terms=531, num_topics=14, decay=0.5, chunksize=10000)\n",
      "2020-03-17 21:36:19,258 : INFO : CorpusAccumulator accumulated stats from 1000 documents\n",
      "2020-03-17 21:36:19,281 : INFO : CorpusAccumulator accumulated stats from 2000 documents\n",
      "2020-03-17 21:36:19,300 : INFO : CorpusAccumulator accumulated stats from 3000 documents\n",
      "2020-03-17 21:36:19,320 : INFO : CorpusAccumulator accumulated stats from 4000 documents\n",
      "2020-03-17 21:36:19,341 : INFO : CorpusAccumulator accumulated stats from 5000 documents\n",
      "2020-03-17 21:36:19,364 : INFO : CorpusAccumulator accumulated stats from 6000 documents\n",
      "2020-03-17 21:36:19,388 : INFO : CorpusAccumulator accumulated stats from 7000 documents\n",
      "2020-03-17 21:36:19,410 : INFO : CorpusAccumulator accumulated stats from 8000 documents\n",
      "2020-03-17 21:36:19,433 : INFO : CorpusAccumulator accumulated stats from 9000 documents\n",
      "2020-03-17 21:36:19,456 : INFO : CorpusAccumulator accumulated stats from 10000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.8491.\n",
      "[([(0.022851782, 'customer_service'),\n",
      "   (0.018153325, 'customer'),\n",
      "   (0.017723894, 'them'),\n",
      "   (0.012956987, 'then'),\n",
      "   (0.012516752, 'after'),\n",
      "   (0.012309641, 'told'),\n",
      "   (0.011260493, 'said'),\n",
      "   (0.010750193, 'minute'),\n",
      "   (0.010196096, 'because'),\n",
      "   (0.009698457, 'asked'),\n",
      "   (0.009417125, 'your'),\n",
      "   (0.009396388, 'didn'),\n",
      "   (0.009390397, 'could'),\n",
      "   (0.008917284, 'went'),\n",
      "   (0.0085692, 'make'),\n",
      "   (0.0085212635, 'order'),\n",
      "   (0.00840338, 'never'),\n",
      "   (0.008251017, 'another'),\n",
      "   (0.008216825, 'over'),\n",
      "   (0.008032461, 'people')],\n",
      "  -1.5326407886621778),\n",
      " ([(0.03609862, 'even_though'),\n",
      "   (0.02889225, 'even'),\n",
      "   (0.02646996, 'coffee'),\n",
      "   (0.022566702, 'though'),\n",
      "   (0.01795907, 'drink'),\n",
      "   (0.013913874, 'didn'),\n",
      "   (0.009937388, 'night'),\n",
      "   (0.0097421575, 'table'),\n",
      "   (0.009143943, 'went'),\n",
      "   (0.008727885, 'nice'),\n",
      "   (0.008644028, 'come'),\n",
      "   (0.008509229, 'because'),\n",
      "   (0.008310297, 'before'),\n",
      "   (0.0081869345, 'after'),\n",
      "   (0.007847875, 'wasn'),\n",
      "   (0.007065823, 'while'),\n",
      "   (0.0069013746, 'friend'),\n",
      "   (0.0068270024, 'came'),\n",
      "   (0.006648127, 'other'),\n",
      "   (0.0066083833, 'over')],\n",
      "  -1.5982880456495974),\n",
      " ([(0.11377938, 'more_than'),\n",
      "   (0.09463551, 'than'),\n",
      "   (0.091933034, 'better_than'),\n",
      "   (0.053003065, 'better'),\n",
      "   (0.014477771, 'much'),\n",
      "   (0.009978616, 'other'),\n",
      "   (0.008576884, 'restaurant'),\n",
      "   (0.0074814707, 'le'),\n",
      "   (0.0073022945, 'think'),\n",
      "   (0.006926849, 'thing'),\n",
      "   (0.0068217013, 'taste'),\n",
      "   (0.0067906645, 'could'),\n",
      "   (0.0064298203, 'star'),\n",
      "   (0.0062206485, 'make'),\n",
      "   (0.005983976, 'also'),\n",
      "   (0.005871303, 'most'),\n",
      "   (0.0057817907, 'price'),\n",
      "   (0.0055848025, 'dish'),\n",
      "   (0.0053228196, 'pretty'),\n",
      "   (0.005319871, 'something')],\n",
      "  -1.6749057258364748),\n",
      " ([(0.013495936, 'because'),\n",
      "   (0.013082824, 'know'),\n",
      "   (0.012413968, 'game'),\n",
      "   (0.012111019, 'star'),\n",
      "   (0.011153963, 'your'),\n",
      "   (0.010876809, 'people'),\n",
      "   (0.010838776, 'thing'),\n",
      "   (0.010753511, 'much'),\n",
      "   (0.010506596, 'think'),\n",
      "   (0.010388709, 'other'),\n",
      "   (0.009878011, 'them'),\n",
      "   (0.009651375, 'year'),\n",
      "   (0.008956156, 'going'),\n",
      "   (0.008894549, 'pretty'),\n",
      "   (0.008883759, 'want'),\n",
      "   (0.008564784, 'than'),\n",
      "   (0.007929549, 'parking'),\n",
      "   (0.0078059123, 'better'),\n",
      "   (0.0075048045, 'area'),\n",
      "   (0.00743553, 'after')],\n",
      "  -1.7121980836218005),\n",
      " ([(0.021035127, 'ordered'),\n",
      "   (0.019038899, 'burger'),\n",
      "   (0.018235927, 'table'),\n",
      "   (0.01284236, 'came'),\n",
      "   (0.01184327, 'menu'),\n",
      "   (0.010729538, 'server'),\n",
      "   (0.0106120845, 'fry'),\n",
      "   (0.010147001, 'order'),\n",
      "   (0.0097367475, 'after'),\n",
      "   (0.009080948, 'wine'),\n",
      "   (0.009023608, 'meal'),\n",
      "   (0.0088185975, 'didn'),\n",
      "   (0.008746828, 'night'),\n",
      "   (0.008540255, 'restaurant'),\n",
      "   (0.0077804704, 'side'),\n",
      "   (0.0076510296, 'dinner'),\n",
      "   (0.007303262, 'cheese'),\n",
      "   (0.007269356, 'friend'),\n",
      "   (0.007250193, 'steak'),\n",
      "   (0.0070836428, 'little')],\n",
      "  -1.7595159612422437),\n",
      " ([(0.04136981, 'pizza'),\n",
      "   (0.027775154, 'sandwich'),\n",
      "   (0.027618816, 'salad'),\n",
      "   (0.019099955, 'cheese'),\n",
      "   (0.015159836, 'sauce'),\n",
      "   (0.013147931, 'chicken'),\n",
      "   (0.012533084, 'also'),\n",
      "   (0.012326431, 'lunch'),\n",
      "   (0.012187151, 'sweet'),\n",
      "   (0.0117491, 'delicious'),\n",
      "   (0.011543742, 'bread'),\n",
      "   (0.011094139, 'fresh'),\n",
      "   (0.008860074, 'best'),\n",
      "   (0.0088195205, 'fry'),\n",
      "   (0.008273276, 'tomato'),\n",
      "   (0.008252235, 'love'),\n",
      "   (0.00817784, 'tasty'),\n",
      "   (0.007950775, 'ordered'),\n",
      "   (0.0076209856, 'little'),\n",
      "   (0.0075347354, 'potato')],\n",
      "  -1.8093957383693164),\n",
      " ([(0.023438, 'your'),\n",
      "   (0.022124618, 'chicken'),\n",
      "   (0.02117702, 'highly_recommend'),\n",
      "   (0.020258533, 'dish'),\n",
      "   (0.013357974, 'flavor'),\n",
      "   (0.011871865, 'rice'),\n",
      "   (0.011785623, 'meat'),\n",
      "   (0.011653772, 'make'),\n",
      "   (0.011471971, 'recommend'),\n",
      "   (0.01111001, 'best'),\n",
      "   (0.010659453, 'highly'),\n",
      "   (0.009865732, 'beef'),\n",
      "   (0.009695346, 'sauce'),\n",
      "   (0.009237539, 'fried'),\n",
      "   (0.008777977, 'meal'),\n",
      "   (0.008473215, 'know'),\n",
      "   (0.008245451, 'delicious'),\n",
      "   (0.00812254, 'little'),\n",
      "   (0.007897387, 'restaurant'),\n",
      "   (0.0077167545, 'menu')],\n",
      "  -1.8142764899649675),\n",
      " ([(0.15040411, 'happy_hour'),\n",
      "   (0.092457876, 'first_time'),\n",
      "   (0.06436125, 'hour'),\n",
      "   (0.060777735, 'happy'),\n",
      "   (0.027707899, 'first'),\n",
      "   (0.010629424, 'drink'),\n",
      "   (0.009475182, 'price'),\n",
      "   (0.009319382, 'beer'),\n",
      "   (0.008267911, 'went'),\n",
      "   (0.0077364193, 'special'),\n",
      "   (0.0076473337, 'pretty'),\n",
      "   (0.007470979, 'menu'),\n",
      "   (0.0072725876, 'love'),\n",
      "   (0.0063505545, 'night'),\n",
      "   (0.006023411, 'best'),\n",
      "   (0.0059104594, 'wine'),\n",
      "   (0.0058121476, 'during'),\n",
      "   (0.005591253, 'well'),\n",
      "   (0.0052783317, 'nice'),\n",
      "   (0.0051159034, 'appetizer')],\n",
      "  -1.845842225077088),\n",
      " ([(0.24931124, 'have_been'),\n",
      "   (0.01746188, 'year'),\n",
      "   (0.013982167, 'love'),\n",
      "   (0.012632571, 'since'),\n",
      "   (0.012471343, 'best'),\n",
      "   (0.012198532, 'chocolate'),\n",
      "   (0.011812939, 'always'),\n",
      "   (0.011439205, 'cake'),\n",
      "   (0.01004605, 'ever'),\n",
      "   (0.0092006605, 'never'),\n",
      "   (0.009125185, 'first'),\n",
      "   (0.008547963, 'many'),\n",
      "   (0.008499194, 'going'),\n",
      "   (0.008348494, 'last'),\n",
      "   (0.00806093, 'them'),\n",
      "   (0.007496905, 'over'),\n",
      "   (0.007420822, 'also'),\n",
      "   (0.0069821053, 'every'),\n",
      "   (0.006237791, 'course'),\n",
      "   (0.005906865, 'make')],\n",
      "  -1.8504191891379647),\n",
      " ([(0.034140605, 'taco'),\n",
      "   (0.02280464, 'breakfast'),\n",
      "   (0.020954657, 'salsa'),\n",
      "   (0.020278972, 'chip'),\n",
      "   (0.020003019, 'mexican'),\n",
      "   (0.018491538, 'burrito'),\n",
      "   (0.017211301, 'bean'),\n",
      "   (0.012565893, 'cheese'),\n",
      "   (0.011826684, 'little'),\n",
      "   (0.011260625, 'rice'),\n",
      "   (0.010717425, 'green'),\n",
      "   (0.0101383235, 'chicken'),\n",
      "   (0.010109097, 'sauce'),\n",
      "   (0.009969786, 'tasty'),\n",
      "   (0.009374222, 'fish'),\n",
      "   (0.008987137, 'also'),\n",
      "   (0.008425454, 'ordered'),\n",
      "   (0.008409031, 'pretty'),\n",
      "   (0.008313158, 'fresh'),\n",
      "   (0.008066057, 'shrimp')],\n",
      "  -1.8775908340208),\n",
      " ([(0.02892434, 'love'),\n",
      "   (0.022485899, 'beer'),\n",
      "   (0.021076966, 'always'),\n",
      "   (0.020542767, 'drink'),\n",
      "   (0.016730774, 'location'),\n",
      "   (0.015767038, 'staff'),\n",
      "   (0.0155658815, 'night'),\n",
      "   (0.014355384, 'friendly'),\n",
      "   (0.014096325, 'music'),\n",
      "   (0.011217179, 'nice'),\n",
      "   (0.01070565, 'pretty'),\n",
      "   (0.010312548, 'selection'),\n",
      "   (0.009517894, 'atmosphere'),\n",
      "   (0.009250078, 'people'),\n",
      "   (0.008989542, 'well'),\n",
      "   (0.008723705, 'never'),\n",
      "   (0.008578167, 'little'),\n",
      "   (0.008123429, 'friend'),\n",
      "   (0.008118242, 'patio'),\n",
      "   (0.008057983, 'cool')],\n",
      "  -1.962756907356962),\n",
      " ([(0.032715615, 'restaurant'),\n",
      "   (0.027899273, 'always'),\n",
      "   (0.027689988, 'friendly'),\n",
      "   (0.022715997, 'staff'),\n",
      "   (0.022395913, 'price'),\n",
      "   (0.017043723, 'lunch'),\n",
      "   (0.01490576, 'excellent'),\n",
      "   (0.013356365, 'love'),\n",
      "   (0.010798705, 'menu'),\n",
      "   (0.009923526, 'year'),\n",
      "   (0.009555071, 'wait'),\n",
      "   (0.009392975, 'clean'),\n",
      "   (0.009341413, 'well'),\n",
      "   (0.009246246, 'delicious'),\n",
      "   (0.009245221, 'favorite'),\n",
      "   (0.009202142, 'experience'),\n",
      "   (0.00918195, 'also'),\n",
      "   (0.009144441, 'definitely'),\n",
      "   (0.009121358, 'never'),\n",
      "   (0.009118143, 'very_friendly')],\n",
      "  -2.038061285604718),\n",
      " ([(0.05904622, 'sushi'),\n",
      "   (0.057107966, 'roll'),\n",
      "   (0.022896558, 'restaurant'),\n",
      "   (0.015549207, 'best'),\n",
      "   (0.013936181, 'your'),\n",
      "   (0.013188329, 'pretty'),\n",
      "   (0.0122695975, 'spicy'),\n",
      "   (0.011581457, 'order'),\n",
      "   (0.010380641, 'fresh'),\n",
      "   (0.009664856, 'mexican'),\n",
      "   (0.009576356, 'fish'),\n",
      "   (0.009231168, 'menu'),\n",
      "   (0.009168618, 'shrimp'),\n",
      "   (0.008979374, 'come'),\n",
      "   (0.008153026, 'star'),\n",
      "   (0.007908315, 'waitress'),\n",
      "   (0.007885386, 'know'),\n",
      "   (0.006744264, 'while'),\n",
      "   (0.006432232, 'chef'),\n",
      "   (0.006215173, 'nothing')],\n",
      "  -2.1830671393510364),\n",
      " ([(0.025899144, 'store'),\n",
      "   (0.0231887, 'your'),\n",
      "   (0.01605684, 'room'),\n",
      "   (0.015478796, 'nice'),\n",
      "   (0.0149847185, 'also'),\n",
      "   (0.012677923, 'find'),\n",
      "   (0.012437177, 'price'),\n",
      "   (0.012144982, 'shop'),\n",
      "   (0.010452091, 'area'),\n",
      "   (0.010412969, 'little'),\n",
      "   (0.00945891, 'well'),\n",
      "   (0.009271045, 'hotel'),\n",
      "   (0.008774343, 'other'),\n",
      "   (0.0084680645, 'need'),\n",
      "   (0.008250367, 'selection'),\n",
      "   (0.007962664, 'pool'),\n",
      "   (0.007893241, 'free'),\n",
      "   (0.007731692, 'kid'),\n",
      "   (0.0075032515, 'item'),\n",
      "   (0.0074339844, 'make')],\n",
      "  -2.228985915362821)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 2 for Determining Optimal Number of Topics: Visualization\n",
    "\n",
    "We use **pyLDAvis** to interpret the topics in a topic model that has been fit to a corpus of text data. \n",
    "\n",
    "It extracts information from a fitted LDA topic model to inform an interactive web-based visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-17 21:36:19,698 : DEBUG : performing inference on a chunk of 10000 documents\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "pyLDAvis.gensim.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Interpretation of the Visualization \n",
    "\n",
    "\n",
    "\n",
    "- Left Panel: \n",
    "The labeld Intertopic Distance Map, circles represent different topics and the distance between them. Similar topics appear closer and the dissimilar topics farther. The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus. An individual topic may be selected for closer scrutiny by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left.\n",
    "\n",
    "\n",
    "\n",
    "- Right Panel:\n",
    "It includes the bar chart of the top 30 terms. When no topic is selected in the plot on the left, the bar chart shows the top-30 most \"salient\" terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics. Selecting each topic on the right, modifies the bar chart to show the \"relevant\" terms for the selected topic. \n",
    "\n",
    "Relevence is defined as in footer 2 and can be tuned by parameter $\\lambda$.\n",
    "- Smaller $\\lambda$ gives higher weight to the term's distinctiveness.\n",
    "- larger $\\lambda$ corresponds to probablity of the term occurance per topics.\n",
    "\n",
    "Therefore, to get a better sense of terms per topic we use $\\lambda = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Top Words in the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_topics(model, num_topics, top_words):\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = top_words);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lda_topics(model, num_topics, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Labels for the Topics\n",
    "\n",
    "We can manually generate human-interpretable labels for each topic by looking at the terms that appear more in each topic.\n",
    "\n",
    "\n",
    "We use LdaModel's \"show_topic\" method that returns **Word-probability pairs** for the most relevant words generated by the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, a topic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:30} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_summaries = []\n",
    "\n",
    "print(u'{:25} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "\n",
    "for i in range(num_topics):\n",
    "    print('\\nTopic '+str(i)+' |---------------------------\\n')\n",
    "    tmp = explore_topic(model,topic_number=i, topn=10, output=True )\n",
    "#     print tmp[:5]\n",
    "    topic_summaries += [tmp[:5]]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Generate Topic Labels\n",
    "\n",
    "Based on the most probable words generated by each topic, we assign human-interpretable labels for the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_labels = {0: 'Asian Cuisine', 1:'Mall', 2:'First Visit', 3:'Customer Service', 4:'Comparison', 5:'Store', 6:'Pizza', 7:'Night', 8:'Happy Hour'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
