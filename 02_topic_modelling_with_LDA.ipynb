{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Note: This notebook is inspired by the Topic-Modeling-Latent-Dirichlet-Allocation series at: https://github.com/rhasanbd/Topic-Modeling-Latent-Dirichlet-Allocation </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation - Implementation on Yelp dataset\n",
    "\n",
    "In this notebook, we implement Latent Dirichlet Allocation(LDA) on the Yelp reviews data to carry out Topic Modelling. We use the Gensim topic modelling API https://radimrehurek.com/gensim/models/ldamodel.html. Scikit-Learn implementation is also available (we use Gensim since it provides more functionality and application like Topic Coherence Pipeline or Dynamic Topic Modeling.)\n",
    "\n",
    "We build an **end-to-end Natural Language Processing (NLP) pipeline**, starting with raw data and running through preparing, modeling, visualization.\n",
    "The steps that we will carry out involves the following:\n",
    "1. Exploratory Data Analysis\n",
    "2. Data Cleaning and Pre-processing\n",
    "3. Topic modeling with LDA\n",
    "4. Determine optimal number of Topics\n",
    "5. Visualize topic model using pyLDAvis\n",
    "\n",
    "### Yelp Review Dataset\n",
    "The Yelp Review Dataset is a CSV file that contains a sub-sample of 10,000 reviews extracted from the Yelp dataset available at: https://www.yelp.com/dataset.\n",
    "\n",
    "The review dataset contains the following fields:\n",
    "- business_id : Unique identifier of business\n",
    "- date : Data of review posted YYYY-MM-DD\n",
    "- review_id : Unique identifier of review\n",
    "- stars : Star rating (upto 4 stars)\n",
    "- text : Review text\n",
    "- user_id : Unique identifier of user who posted the review\n",
    "- cool : Number of cool votes received\n",
    "- useful : Number of useful votes received\n",
    "- funny : Number of funny votes received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rojin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)\n",
    "\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "from scipy import sparse as sp\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>1/26/2011</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>7/27/2011</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>6/14/2012</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>5/27/2010</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id       date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  1/26/2011  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  7/27/2011  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  6/14/2012  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  5/27/2010  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw   1/5/2012  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text                 user_id  \\\n",
       "0  My wife took me here on my birthday for breakf...  rLtl8ZkDX5vH5nAx9C3q5Q   \n",
       "1  I have no idea why some people give bad review...  0a2KyEL0d3Yb1V6aivbIuQ   \n",
       "2  love the gyro plate. Rice is so good and I als...  0hT2KtfLiobPvh6cDC8JQg   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  uZetl9T0NcROGOyFfughhg   \n",
       "4  General Manager Scott Petello is a good egg!!!...  vYmM4KTsC8ZfQBg-j5MWkw   \n",
       "\n",
       "   cool  useful  funny  \n",
       "0     2       5      0  \n",
       "1     0       0      0  \n",
       "2     0       1      0  \n",
       "3     1       2      0  \n",
       "4     0       0      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/yelp_academic_dataset_review_10000.csv')\n",
    "\n",
    "# df1 = pd.read_csv('/Users/hasan/datasets/NIPS2015_Papers.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the Data\n",
    "\n",
    "DataFrame’s info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 9 columns):\n",
      "business_id    10000 non-null object\n",
      "date           10000 non-null object\n",
      "review_id      10000 non-null object\n",
      "stars          10000 non-null int64\n",
      "text           10000 non-null object\n",
      "user_id        10000 non-null object\n",
      "cool           10000 non-null int64\n",
      "useful         10000 non-null int64\n",
      "funny          10000 non-null int64\n",
      "dtypes: int64(4), object(5)\n",
      "memory usage: 703.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension the Data\n",
    "\n",
    "Get the dimension (number of rows and columns) of the data using DataFrame's shape method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the data:  (10000, 9)\n",
      "No. of Rows: 10000\n",
      "No. of Columns: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of the data: \", df.shape)\n",
    "\n",
    "no_of_rows = df.shape[0]\n",
    "no_of_columns = df.shape[1]\n",
    "\n",
    "print(\"No. of Rows: %d\" % no_of_rows)\n",
    "print(\"No. of Columns: %d\" % no_of_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the DataFrame Object into a 2D Array of Documents\n",
    "\n",
    "We convert the documents from DataFrame object to an array of documents.\n",
    "\n",
    "It's a 2D array in which each row reprents a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the documents array:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "docs_array = array(df['text'])\n",
    "\n",
    "print(\"Dimension of the documents array: \", docs_array.shape)\n",
    "\n",
    "# Display the first document\n",
    "#print(docs_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the Data\n",
    "\n",
    "\n",
    "We pre-process the data as follows. \n",
    "\n",
    "- Convert to lowercase \n",
    "- Tokenize (split the documents into tokens or words)\n",
    "- Remove numbers, but not words that contain numbers\n",
    "- Remove words that are only one character\n",
    "- Lemmatize the tokens/words\n",
    "\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "We tokenize the text using a regular expression tokenizer from NLTK. We remove numeric tokens and tokens that are only a single character, as they don’t tend to be useful, and the dataset contains a lot of them.\n",
    "\n",
    "\n",
    "The NLTK Regular-Expression Tokenizer class \"RegexpTokenizer\" splits a string into substrings using a regular expression. We use the regular expression \"\\w+\" to matche token of words. \n",
    "\n",
    "See the following two links for a list of regular expressions and NLTK tokenize module.\n",
    "https://github.com/tartley/python-regex-cheatsheet/blob/master/cheatsheet.rst\n",
    "https://www.nltk.org/api/nltk.tokenize.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Convert the 2D Document Array into a 2D Array of Tokenized Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def docs_preprocessor(docs):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # Tokenize the words.\n",
    "    \n",
    "    for idx in range(len(docs)):\n",
    "#         docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only two character.\n",
    "    docs = [[token for token in doc if len(token) > 4] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the 2D Document Array into a 1D Array of Tokenized Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.3 s\n"
     ]
    }
   ],
   "source": [
    "# Convert the 2D Document Array into a 1D Array of Tokenized Words\n",
    "%time docs = docs_preprocessor(docs_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the 2D Array of Tokenized Documents:  10000\n",
      "[['birthday', 'breakfast', 'excellent', 'weather', 'perfect', 'which', 'sitting', 'outside', 'overlooking', 'their', 'ground', 'absolute', 'pleasure', 'waitress', 'excellent', 'arrived', 'quickly', 'Saturday', 'morning', 'looked', 'place', 'fill', 'pretty', 'quickly', 'earlier', 'better', 'yourself', 'favor', 'their', 'Bloody', 'phenomenal', 'simply', 'pretty', 'ingredient', 'their', 'garden', 'blend', 'fresh', 'order', 'amazing', 'While', 'EVERYTHING', 'look', 'excellent', 'white', 'truffle', 'scrambled', 'vegetable', 'skillet', 'tasty', 'delicious', 'piece', 'their', 'griddled', 'bread', 'amazing', 'absolutely', 'complete', 'toast', 'Anyway'], ['people', 'review', 'about', 'place', 'please', 'everyone', 'probably', 'griping', 'about', 'something', 'their', 'fault', 'there', 'people', 'friend', 'arrived', 'about', 'Sunday', 'pretty', 'crowded', 'thought', 'Sunday', 'evening', 'thought', 'would', 'forever', 'seated', 'come', 'seating', 'someone', 'seated', 'waiter', 'drink', 'order', 'Everyone', 'pleasant', 'seated', 'waiter', 'server', 'price', 'placed', 'order', 'decided', 'wanted', 'shared', 'baked', 'spaghetti', 'calzone', 'small', 'pizza', 'calzone', 'smallest', 'personal', 'small', 'pizza', 'awesome', 'friend', 'liked', 'pizza', 'better', 'liked', 'calzone', 'better', 'calzone', 'sweetish', 'sauce', 'sauce', 'pizza', 'everything', 'great', 'these', 'reviewer', 'these', 'thing', 'yourself', 'because', 'these', 'reviewer', 'serious', 'issue']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the 2D Array of Tokenized Documents: \", len(docs))\n",
    "\n",
    "#Display the first two document\n",
    "print(docs[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Bigrams/Trigrams:\n",
    "\n",
    "\n",
    "When topics are very similar, we may **use phrases** rather than single/individual words to distinguis each topic. \n",
    "\n",
    "Thus, we compute both bigrams and trigrams. Depending on the dataset it may not be necessary to create trigrams.\n",
    "\n",
    "Note that we only keep the **frequent** phrases (bigrams/trigrams).\n",
    "\n",
    "#### Bigrams\n",
    "Bigrams are sets of two adjacent words. Using bigrams we can get phrases like “machine_learning” in our output (spaces are replaced with underscores). Without bigrams we would only get “machine” and “learning”.\n",
    "\n",
    "Note that in the code below, we find bigrams and then add them to the original data, because we would like to keep the words “machine” and “learning” as well as the bigram “machine_learning”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 10 times or more).\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "trigram = Phrases(bigram[docs])\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "    for token in trigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Rare and Common Tokens/Words\n",
    "\n",
    "We remove rare words and common words based on their document frequency. \n",
    "\n",
    "For example, we may remove words that appear in less than 10 documents or in more than 20% of the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in initital documents: 29286\n",
      "Number of unique words after removing rare and common words: 5087\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "print('Number of unique words in initital documents:', len(dictionary))\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 20% of the documents.\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.15)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Representation of Data\n",
    "\n",
    "\n",
    "Finally, we transform the documents to a **vectorized form**. \n",
    "\n",
    "We simply compute the frequency of each word, including the bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 5087\n",
      "Number of documents: 10000\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the LDA Model\n",
    "\n",
    "We use the gensim.models.LdaModel class for performing LDA.\n",
    "\n",
    "We need to set the parameters of the LdaModel object carefully. The full list of the parameters are given:\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "\n",
    "#### Below we discuss the setting of some of the key parameters.\n",
    "\n",
    "- num_topics (int, optional) – The number of requested latent topics to be extracted from the training corpus.\n",
    "\n",
    " \n",
    "LDA is an unsupervised technique, meaning that we don't know prior to running the model how many topics exits in our corpus. It depends on the data and the application. We may use the following two technique to determine the number of topics.\n",
    "\n",
    "\n",
    "        Technique 1: Topic Coherence \n",
    "The main technique to determine the number of topics is **Topic coherence**:\n",
    "http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
    "\n",
    "\n",
    "        Technique 2: Visualizing Inter-Topic Distance \n",
    "Use the LDA visualization tool pyLDAvis to observe Intertopic Distance Map (discussed later). By varying the number of topics we could determine the optimal value from the visualization.\n",
    "\n",
    "We **use both techniques** to determine the optimal number of topics.\n",
    "\n",
    "\n",
    "- chunksize (int, optional) – Number of documents to be used in each training chunk.\n",
    "\n",
    "It controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory. \n",
    "\n",
    "We set chunksize = 2000, which is more than the amount of documents. Thus, it processes all the data in one go. \n",
    "\n",
    "Chunksize can however influence the quality of the model.\n",
    "\n",
    "\n",
    "- passes (int, optional) – Number of passes through the corpus during training.\n",
    "\n",
    "It controls how often we train the model on the entire corpus. Another word for passes might be “epochs”. \n",
    "\n",
    "\n",
    "- iterations (int, optional) – Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
    "\n",
    "It is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. \n",
    "\n",
    "        It is important to set the number of “passes” and “iterations” high enough.\n",
    "\n",
    "\n",
    "\n",
    "#### How to Set \"passes\" and \"iterations\":\n",
    "\n",
    "First, enable logging and set eval_every = 1 (however, it might slow down, so, we use None) in LdaModel. \n",
    "\n",
    "When training the model look for a line in the log that looks something like this:\n",
    "\n",
    "        2020-02-25 19:07:04,716 : DEBUG : 49/403 documents converged within 400 iterations\n",
    "\n",
    "If we set passes = 20, we will see this line 20 times. \n",
    "\n",
    "### Important: We need to make sure that by the final passes, most of the documents have converged. \n",
    "\n",
    "For example, if passes = 20 and iterations = 400, then, we should see something like following:\n",
    "\n",
    "\n",
    "        2020-02-25 19:07:18,041 : INFO : PROGRESS: pass 19, at document #403/403\n",
    "        2020-02-25 19:07:18,042 : DEBUG : performing inference on a chunk of 403 documents\n",
    "        2020-02-25 19:07:18,627 : DEBUG : 402/403 documents converged within 400 iterations\n",
    "\n",
    "Thus, want to choose both passes and iterations to be high enough for this to happen.\n",
    "\n",
    "\n",
    "- eval_every (int, optional) – Log perplexity is estimated every that many updates. Setting this to 1 slows down training by ~2x.\n",
    "\n",
    "\n",
    "- alpha ({numpy.ndarray, str}, optional): Can be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. \n",
    "\n",
    "Alternatively default prior selecting strategies can be employed by supplying a string:\n",
    "\n",
    "        ’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.\n",
    "\n",
    "        ’auto’: Learns an asymmetric prior from the corpus (not available if distributed==True).\n",
    "        \n",
    "        \n",
    "- eta ({float, np.array, str}, optional) – A-priori belief on word probability.\n",
    "\n",
    "It can be:\n",
    "\n",
    "        scalar for a symmetric prior over topic/word probability,\n",
    "\n",
    "        vector of length num_words to denote an asymmetric user defined probability for each word,\n",
    "\n",
    "        matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
    "\n",
    "        the string ‘auto’ to learn the asymmetric prior from the data.\n",
    "\n",
    "\n",
    "We set alpha = 'auto' and eta = 'auto'. Again this is somewhat technical, but essentially we are automatically learning two parameters in the model that we usually would have to specify explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.9 s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 5\n",
    "chunksize = 500 # Size of the doc looked at every pass\n",
    "passes = 20 # Number of passes through documents\n",
    "iterations = 400 # Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "                       alpha='auto', eta='auto', \\\n",
    "                       iterations=iterations, num_topics=num_topics, \\\n",
    "                       passes=passes, eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 1 for Determining Optimal Number of Topics: Topic Coherence\n",
    "\n",
    "Topic Coherence is a measure used to evaluate topic models. Each such generated topic consists of words, and the topic coherence is applied to the top N words from the topic. \n",
    "\n",
    "Topic Coherence measures score a single topic by **measuring the degree of semantic similarity between high scoring words in the topic**. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. \n",
    "\n",
    "A set of statements or facts is said to be coherent, if they support each other. Thus, a coherent fact set can be interpreted in a context that covers all or most of the facts. An example of a coherent fact set is “the game is a team sport”, “the game is played with a ball”, “the game demands great physical efforts”\n",
    "\n",
    "Topic Coherence is defined as the average of the pairwise word-similarity scores of the words in the topic.\n",
    "\n",
    "A good model will generate coherent topics, i.e., topics with high topic coherence scores. Good topics are topics that can be described by a short label, therefore this is what the topic coherence measure should capture.\n",
    "\n",
    "\n",
    "Below we display \n",
    "- the average topic coherence and\n",
    "- print the topics in order of topic coherence\n",
    "\n",
    "\n",
    "We use LdaModel's \"top_topics\" method to get the topics with the highest coherence score the coherence for each topic.\n",
    "\n",
    "Note that we use the “Umass” topic coherence measure here (see gensim.models.ldamodel.LdaModel.top_topics())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -3.1269.\n",
      "[([(0.011593476, 'drink'),\n",
      "   (0.011368852, 'night'),\n",
      "   (0.009687498, 'table'),\n",
      "   (0.00916343, 'could'),\n",
      "   (0.0091171805, 'people'),\n",
      "   (0.009064639, 'pretty'),\n",
      "   (0.009057821, 'thing'),\n",
      "   (0.008961214, 'think'),\n",
      "   (0.008953973, 'friend'),\n",
      "   (0.008762954, 'after'),\n",
      "   (0.0077471994, 'before'),\n",
      "   (0.0074814204, 'first'),\n",
      "   (0.007239155, 'right'),\n",
      "   (0.006789095, 'going'),\n",
      "   (0.0067751314, 'again'),\n",
      "   (0.0067248307, 'while'),\n",
      "   (0.0066727963, 'around'),\n",
      "   (0.0063811378, 'order'),\n",
      "   (0.006224875, 'happy'),\n",
      "   (0.006106414, 'review')],\n",
      "  -1.7609037739449036),\n",
      " ([(0.016031815, 'ordered'),\n",
      "   (0.014687053, 'chicken'),\n",
      "   (0.013844945, 'sauce'),\n",
      "   (0.013504017, 'salad'),\n",
      "   (0.012765066, 'pizza'),\n",
      "   (0.012104686, 'lunch'),\n",
      "   (0.011776938, 'fresh'),\n",
      "   (0.011679871, 'cheese'),\n",
      "   (0.011498498, 'sandwich'),\n",
      "   (0.009784301, 'delicious'),\n",
      "   (0.009240105, 'taste'),\n",
      "   (0.00912928, 'flavor'),\n",
      "   (0.008288048, 'tasty'),\n",
      "   (0.007930164, 'dinner'),\n",
      "   (0.007920272, 'bread'),\n",
      "   (0.007860483, 'order'),\n",
      "   (0.007358835, 'plate'),\n",
      "   (0.00644638, 'sushi'),\n",
      "   (0.0058313548, 'dish'),\n",
      "   (0.0056537404, 'sweet')],\n",
      "  -2.13435797196174),\n",
      " ([(0.02592512, 'always'),\n",
      "   (0.019507982, 'staff'),\n",
      "   (0.019194903, 'friendly'),\n",
      "   (0.016810745, 'price'),\n",
      "   (0.013134346, 'burger'),\n",
      "   (0.012126995, 'location'),\n",
      "   (0.011480629, 'selection'),\n",
      "   (0.011057738, 'Great'),\n",
      "   (0.010456812, 'Phoenix'),\n",
      "   (0.010054617, 'visit'),\n",
      "   (0.010014349, 'fry'),\n",
      "   (0.010009829, 'amazing'),\n",
      "   (0.009160414, 'recommend'),\n",
      "   (0.008918149, 'favorite'),\n",
      "   (0.008848916, 'awesome'),\n",
      "   (0.00817935, 'atmosphere'),\n",
      "   (0.007598908, 'every'),\n",
      "   (0.0072720353, 'staff_friendly'),\n",
      "   (0.007173979, 'Scottsdale'),\n",
      "   (0.0070247934, 'excellent')],\n",
      "  -2.563662061357906),\n",
      " ([(0.016203605, 'store'),\n",
      "   (0.011120837, 'customer_service'),\n",
      "   (0.0104237795, 'customer'),\n",
      "   (0.0070272977, 'going'),\n",
      "   (0.007020383, 'business'),\n",
      "   (0.0065031326, 'There'),\n",
      "   (0.0064977785, 'owner'),\n",
      "   (0.0061771506, 'through'),\n",
      "   (0.0060847853, 'money'),\n",
      "   (0.005738942, 'these'),\n",
      "   (0.005660386, 'year'),\n",
      "   (0.0055904733, 'where'),\n",
      "   (0.004991485, 'month'),\n",
      "   (0.004924771, 'item'),\n",
      "   (0.0048845005, 'hotel'),\n",
      "   (0.0047552027, 'people'),\n",
      "   (0.0047163786, 'would_recommend'),\n",
      "   (0.004443439, 'employee'),\n",
      "   (0.0044277976, 'stuff'),\n",
      "   (0.0043494254, 'drive')],\n",
      "  -2.7329899853359745),\n",
      " ([(0.022296365, 'breakfast'),\n",
      "   (0.021787254, 'coffee'),\n",
      "   (0.014554534, 'chip'),\n",
      "   (0.014538429, 'taco'),\n",
      "   (0.013332879, 'Mexican'),\n",
      "   (0.012880522, 'cream'),\n",
      "   (0.012739069, 'chocolate'),\n",
      "   (0.012223037, 'salsa'),\n",
      "   (0.010585272, 'chip_salsa'),\n",
      "   (0.010056615, 'burrito'),\n",
      "   (0.009008602, 'bean'),\n",
      "   (0.0074021616, 'star_because'),\n",
      "   (0.0069120657, 'lunch_special'),\n",
      "   (0.0068905004, 'carne_asada'),\n",
      "   (0.0066788513, 'talking_about'),\n",
      "   (0.0065252203, 'super_friendly'),\n",
      "   (0.006166511, 'fried_chicken'),\n",
      "   (0.006124905, 'tortilla'),\n",
      "   (0.006003315, 'margarita'),\n",
      "   (0.005553417, 'cream_cheese')],\n",
      "  -6.442760849897527)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique 2 for Determining Optimal Number of Topics: Visualization\n",
    "\n",
    "We use **pyLDAvis** to interpret the topics in a topic model that has been fit to a corpus of text data. \n",
    "\n",
    "It extracts information from a fitted LDA topic model to inform an interactive web-based visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyLDAvis.gensim\n",
    "# pyLDAvis.enable_notebook()\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "# pyLDAvis.gensim.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Interpretation of the Visualization \n",
    "\n",
    "\n",
    "\n",
    "- Left Panel: \n",
    "The labeld Intertopic Distance Map, circles represent different topics and the distance between them. Similar topics appear closer and the dissimilar topics farther. The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus. An individual topic may be selected for closer scrutiny by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left.\n",
    "\n",
    "\n",
    "\n",
    "- Right Panel:\n",
    "It includes the bar chart of the top 30 terms. When no topic is selected in the plot on the left, the bar chart shows the top-30 most \"salient\" terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics. Selecting each topic on the right, modifies the bar chart to show the \"relevant\" terms for the selected topic. \n",
    "\n",
    "Relevence is defined as in footer 2 and can be tuned by parameter $\\lambda$.\n",
    "- Smaller $\\lambda$ gives higher weight to the term's distinctiveness.\n",
    "- larger $\\lambda$ corresponds to probablity of the term occurance per topics.\n",
    "\n",
    "Therefore, to get a better sense of terms per topic we use $\\lambda = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Top Words in the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_topics(model, num_topics, top_words):\n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        words = model.show_topic(i, topn = top_words);\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>always</td>\n",
       "      <td>drink</td>\n",
       "      <td>breakfast</td>\n",
       "      <td>ordered</td>\n",
       "      <td>store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>staff</td>\n",
       "      <td>night</td>\n",
       "      <td>coffee</td>\n",
       "      <td>chicken</td>\n",
       "      <td>customer_service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>friendly</td>\n",
       "      <td>table</td>\n",
       "      <td>chip</td>\n",
       "      <td>sauce</td>\n",
       "      <td>customer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>price</td>\n",
       "      <td>could</td>\n",
       "      <td>taco</td>\n",
       "      <td>salad</td>\n",
       "      <td>going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>burger</td>\n",
       "      <td>people</td>\n",
       "      <td>Mexican</td>\n",
       "      <td>pizza</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>location</td>\n",
       "      <td>pretty</td>\n",
       "      <td>cream</td>\n",
       "      <td>lunch</td>\n",
       "      <td>There</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>selection</td>\n",
       "      <td>thing</td>\n",
       "      <td>chocolate</td>\n",
       "      <td>fresh</td>\n",
       "      <td>owner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Great</td>\n",
       "      <td>think</td>\n",
       "      <td>salsa</td>\n",
       "      <td>cheese</td>\n",
       "      <td>through</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Phoenix</td>\n",
       "      <td>friend</td>\n",
       "      <td>chip_salsa</td>\n",
       "      <td>sandwich</td>\n",
       "      <td>money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>visit</td>\n",
       "      <td>after</td>\n",
       "      <td>burrito</td>\n",
       "      <td>delicious</td>\n",
       "      <td>these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fry</td>\n",
       "      <td>before</td>\n",
       "      <td>bean</td>\n",
       "      <td>taste</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>amazing</td>\n",
       "      <td>first</td>\n",
       "      <td>star_because</td>\n",
       "      <td>flavor</td>\n",
       "      <td>where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>recommend</td>\n",
       "      <td>right</td>\n",
       "      <td>lunch_special</td>\n",
       "      <td>tasty</td>\n",
       "      <td>month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>favorite</td>\n",
       "      <td>going</td>\n",
       "      <td>carne_asada</td>\n",
       "      <td>dinner</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>awesome</td>\n",
       "      <td>again</td>\n",
       "      <td>talking_about</td>\n",
       "      <td>bread</td>\n",
       "      <td>hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>atmosphere</td>\n",
       "      <td>while</td>\n",
       "      <td>super_friendly</td>\n",
       "      <td>order</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>every</td>\n",
       "      <td>around</td>\n",
       "      <td>fried_chicken</td>\n",
       "      <td>plate</td>\n",
       "      <td>would_recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>staff_friendly</td>\n",
       "      <td>order</td>\n",
       "      <td>tortilla</td>\n",
       "      <td>sushi</td>\n",
       "      <td>employee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Scottsdale</td>\n",
       "      <td>happy</td>\n",
       "      <td>margarita</td>\n",
       "      <td>dish</td>\n",
       "      <td>stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>excellent</td>\n",
       "      <td>review</td>\n",
       "      <td>cream_cheese</td>\n",
       "      <td>sweet</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Topic # 01 Topic # 02      Topic # 03 Topic # 04        Topic # 05\n",
       "0           always      drink       breakfast    ordered             store\n",
       "1            staff      night          coffee    chicken  customer_service\n",
       "2         friendly      table            chip      sauce          customer\n",
       "3            price      could            taco      salad             going\n",
       "4           burger     people         Mexican      pizza          business\n",
       "5         location     pretty           cream      lunch             There\n",
       "6        selection      thing       chocolate      fresh             owner\n",
       "7            Great      think           salsa     cheese           through\n",
       "8          Phoenix     friend      chip_salsa   sandwich             money\n",
       "9            visit      after         burrito  delicious             these\n",
       "10             fry     before            bean      taste              year\n",
       "11         amazing      first    star_because     flavor             where\n",
       "12       recommend      right   lunch_special      tasty             month\n",
       "13        favorite      going     carne_asada     dinner              item\n",
       "14         awesome      again   talking_about      bread             hotel\n",
       "15      atmosphere      while  super_friendly      order            people\n",
       "16           every     around   fried_chicken      plate   would_recommend\n",
       "17  staff_friendly      order        tortilla      sushi          employee\n",
       "18      Scottsdale      happy       margarita       dish             stuff\n",
       "19       excellent     review    cream_cheese      sweet             drive"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lda_topics(model, num_topics, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Labels for the Topics\n",
    "\n",
    "We can manually generate human-interpretable labels for each topic by looking at the terms that appear more in each topic.\n",
    "\n",
    "\n",
    "We use LdaModel's \"show_topic\" method that returns **Word-probability pairs** for the most relevant words generated by the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(lda_model, topic_number, topn, output=True):\n",
    "    \"\"\"\n",
    "    accept a ldamodel, a topic number and topn vocabs of interest\n",
    "    prints a formatted list of the topn terms\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    for term, frequency in lda_model.show_topic(topic_number, topn=topn):\n",
    "        terms += [term]\n",
    "        if output:\n",
    "            print(u'{:30} {:.3f}'.format(term, round(frequency, 3)))\n",
    "    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                      frequency\n",
      "\n",
      "\n",
      "Topic 0 |---------------------------\n",
      "\n",
      "always                         0.026\n",
      "staff                          0.020\n",
      "friendly                       0.019\n",
      "price                          0.017\n",
      "burger                         0.013\n",
      "location                       0.012\n",
      "selection                      0.011\n",
      "Great                          0.011\n",
      "Phoenix                        0.010\n",
      "visit                          0.010\n",
      "\n",
      "Topic 1 |---------------------------\n",
      "\n",
      "drink                          0.012\n",
      "night                          0.011\n",
      "table                          0.010\n",
      "could                          0.009\n",
      "people                         0.009\n",
      "pretty                         0.009\n",
      "thing                          0.009\n",
      "think                          0.009\n",
      "friend                         0.009\n",
      "after                          0.009\n",
      "\n",
      "Topic 2 |---------------------------\n",
      "\n",
      "breakfast                      0.022\n",
      "coffee                         0.022\n",
      "chip                           0.015\n",
      "taco                           0.015\n",
      "Mexican                        0.013\n",
      "cream                          0.013\n",
      "chocolate                      0.013\n",
      "salsa                          0.012\n",
      "chip_salsa                     0.011\n",
      "burrito                        0.010\n",
      "\n",
      "Topic 3 |---------------------------\n",
      "\n",
      "ordered                        0.016\n",
      "chicken                        0.015\n",
      "sauce                          0.014\n",
      "salad                          0.014\n",
      "pizza                          0.013\n",
      "lunch                          0.012\n",
      "fresh                          0.012\n",
      "cheese                         0.012\n",
      "sandwich                       0.011\n",
      "delicious                      0.010\n",
      "\n",
      "Topic 4 |---------------------------\n",
      "\n",
      "store                          0.016\n",
      "customer_service               0.011\n",
      "customer                       0.010\n",
      "going                          0.007\n",
      "business                       0.007\n",
      "There                          0.007\n",
      "owner                          0.006\n",
      "through                        0.006\n",
      "money                          0.006\n",
      "these                          0.006\n"
     ]
    }
   ],
   "source": [
    "topic_summaries = []\n",
    "\n",
    "print(u'{:25} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "\n",
    "for i in range(num_topics):\n",
    "    print('\\nTopic '+str(i)+' |---------------------------\\n')\n",
    "    tmp = explore_topic(model,topic_number=i, topn=10, output=True )\n",
    "#     print tmp[:5]\n",
    "    topic_summaries += [tmp[:5]]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Generate Topic Labels\n",
    "\n",
    "Based on the most probable words generated by each topic, we assign human-interpretable labels for the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_labels = {0: 'Statistics', 1:'Numerical Analysis', 2:'Online Learning', 3:'Deep Learning'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
