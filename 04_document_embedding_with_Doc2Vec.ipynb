{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<i> Note: This notebook is inspired by the Doc2Vec Text Classification series at:  </i>\n",
    "https://github.com/rhasanbd/Document-Embedding-Doc2vec-Text-Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Embedding\n",
    "\n",
    "In the previous notebook, we implemented Word2Vec model for text classification. The model carried out Word emedding by representing words numerically.\n",
    "Now, we try to explore how documents as a whole can be represented numerically by retaiing the word orders and it's semantics.\n",
    "\n",
    "## Doc2vec\n",
    "\n",
    "The Doc2vec model is an implementation of the  **Paragraph Vector** model proposed by (Quoc Le and Tomas Mikolov, 1994) in \"Distributed Representations of Sentences and Documents\". \n",
    "\n",
    "The Doc2vec improves the Word2vec model where every paragraph is mappeed to a unique vecot D and every word is also mapped to a unique vector W as in Word2vec. It is capable of constructing representations of input sequences of variable length like sentences, paragraph and documents.\n",
    "\n",
    "## Distributed Memory Model of Paragraph Vectors(PV-DM)\n",
    "This is one of the types of the Doc2vec model. The paragraph acts as a memory that retains what is missing from the current context from the words (i.e. Topic of the paragraph). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
