{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|<i> Note: This notebook is inspired by the Doc2Vec Text Classification series at:  </i>\n",
    "https://github.com/rhasanbd/Document-Embedding-Doc2vec-Text-Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Embedding\n",
    "\n",
    "In the previous notebook, we implemented Word2Vec model for text classification. The model carried out Word emedding by representing words numerically.\n",
    "Now, we try to explore how documents as a whole can be represented numerically by retaiing the word orders and it's semantics.\n",
    "\n",
    "## Doc2vec\n",
    "\n",
    "The Doc2vec model is an implementation of the  **Paragraph Vector** model proposed by (Quoc Le and Tomas Mikolov, 1994) in \"Distributed Representations of Sentences and Documents\". \n",
    "\n",
    "The Doc2vec improves the Word2vec model where every paragraph is mappeed to a unique vecot D and every word is also mapped to a unique vector W as in Word2vec. It is capable of constructing representations of input sequences of variable length like sentences, paragraph and documents.\n",
    "\n",
    "## Distributed Memory Model of Paragraph Vectors(PV-DM)\n",
    "This is one of the types of the Doc2vec model. The paragraph acts as a memory that retains what is missing from the current context from the words (i.e. Topic of the paragraph). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bolt is within walking distance of The Drake H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When people say Korean food, what do you think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feast Buffet at Palace Station Casino\\n\\nMaybe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm such a fan!  Our Nishikawa Black Ramen bow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Several of our friends that live in the area s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  Bolt is within walking distance of The Drake H...\n",
       "1  When people say Korean food, what do you think...\n",
       "2  Feast Buffet at Palace Station Casino\\n\\nMaybe...\n",
       "3  I'm such a fan!  Our Nishikawa Black Ramen bow...\n",
       "4  Several of our friends that live in the area s..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client.yelp_database\n",
    "df = pd.DataFrame(db.business_restaurant.find({},{\"reviews.text\":1, \"_id\":0}))\n",
    "df = df.applymap(lambda x : x[0]['text'])\n",
    "df.head() #Quick Check of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8688 entries, 0 to 8687\n",
      "Data columns (total 1 columns):\n",
      "reviews    8688 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 68.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the data:  (8688, 1)\n",
      "No. of Rows: 8688\n",
      "No. of Columns: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of the data: \", df.shape)\n",
    "\n",
    "no_of_rows = df.shape[0]\n",
    "no_of_columns = df.shape[1]\n",
    "\n",
    "print(\"No. of Rows: %d\" % no_of_rows)\n",
    "print(\"No. of Columns: %d\" % no_of_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Document Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents (emails) in the corpus:  8688\n"
     ]
    }
   ],
   "source": [
    "corpus = df['reviews']\n",
    "print(\"Number of Documents (emails) in the corpus: \", len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the Data\n",
    "Pre-processing of the text data is done using the following steps:\n",
    "\n",
    "Convert to lowercase\n",
    "Tokenize (split the documents into tokens or words)\n",
    "Remove numbers, but not words that contain numbers\n",
    "Remove words that are only a single character\n",
    "Lemmatize the tokens/words\n",
    "\n",
    "## Tokenization and Lemmatization\n",
    "We convert all the words into lowercase then tokenize each word using NLTK Regular-Expression Tokenizer class \"RegexpTokenizer\".\n",
    "It splits a given string to substrings using a regular expression.\n",
    "Then we remove numbers and single character words since they usually don't impart much useful information and are very high in number.\n",
    "Finally, we lemmatize the tokens using WordNetLemmatizer from NLTK, where we extract the root words of the tokens using the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def docs_preprocessor(docs):\n",
    "    '''Function to Convert the 2D Document Array into a 2D Array of Tokenized Documents'''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # Tokenize the words.\n",
    "    \n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 3] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the 2D Array of Tokenized Documents:  8688\n",
      "Wall time: 8.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Convert a list of sentences to a list of lists containing tokenized words\n",
    "docs = docs_preprocessor(corpus)\n",
    "print(\"Length of the 2D Array of Tokenized Documents: \", len(docs))\n",
    "\n",
    "# Store the data locally\n",
    "pickle.dump(docs, open(\"tokenized_reviews_doc2vec.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Bigrams/Trigrams:\n",
    "\n",
    "- N-grams are combinations of adjacent words or letters of length 'n' that you can find in your source text. These combinations of words carry a special meaning. For example: car-pool is an n-gram formed using the two words car and pool that carries a distinct meaning different from the individual words. \n",
    "\n",
    "- If n=2, it is called a Bigram and if n=3, it is called a Trigram.\n",
    "\n",
    "- We find all the combinations of Bigrams and Trigrams. Then, we keep only the frequent phrases. \n",
    "- We finally add the frequent phrases to the original data, since we would like to keep the words “car” and “pool” as well as the bigram “car_pool”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 10 times or more).\n",
    "bigram = Phrases(docs, min_count=10, threshold=100)\n",
    "trigram = Phrases(bigram[docs], min_count=10,  threshold=100)\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n",
    "    for token in trigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a trigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tagged Documents\n",
    "For training the Doc2Vec model, we need to create tagged documented.\n",
    "A single document, made up of words and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of lists containing tokenized words\n",
    "docs = pickle.load( open(\"tokenized_reviews_doc2vec.p\", \"rb\" ) )\n",
    "#print(docs[0])\n",
    "\n",
    "# Create Tagged documents\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
    "#print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['such', 'nishikawa', 'black', 'ramen', 'bowl', 'everything', 'hope', 'would', 'this', 'chilly', 'rainy', 'winter', 'evening', 'after', 'spent', 'hiking', 'rain', 'rich', 'deep', 'full', 'interesting', 'flavor', 'texture', 'umami', 'dreamed', 'spring', 'roll', 'gyoza', 'were', 'also', 'yummy', 'star', 'show', 'definitely', 'ramen', 'soooo', 'busy', 'tonight', 'were', 'seated', 'fairly', 'quickly', 'food', 'came', 'fast', 'waitress', 'forgot', 'drink', 'order', 'didn', 'really', 'check', 'sure', 'were', 'busy', 'slurping', 'noodle', 'restaurant', 'clean', 'small', 'price', 'good', 'looking', 'good', 'ramen', 'spot', 'back', 'soon', 'incredible'], [3])\n"
     ]
    }
   ],
   "source": [
    "print(documents[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Doc2vec Model\n",
    "We use the gensim.models.Doc2Vec class.\n",
    "\n",
    "    class gensim.models.doc2vec.Doc2Vec(documents=None, corpus_file=None, dm_mean=None, dm=1, dbow_words=0, dm_concat=0, dm_tag_count=1, docvecs=None, docvecs_mapfile=None, comment=None, trim_rule=None, callbacks=(), **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Doc2vec Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set training parameters\n",
    "doc_vector_length = 300       # Dimension of the document vector\n",
    "window_size = 2               # We set it 2 as the sentences weren't too long\n",
    "epochs = 600                  # Number of iterations (epochs) over the corpus\n",
    "min_count = 10                 # Ignores all words with total frequency lower than min_count\n",
    "workers = 4                   # Number of worker threads to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create the Doc2vec model using gensim (If dm=1, ‘distributed memory’ (DM) algorithm is used)\n",
    "model = Doc2Vec(vector_size=doc_vector_length, dm=1, window=window_size, min_count=min_count, \n",
    "                workers=workers, sample=0.01, epochs=epochs)\n",
    "# Create vocabulary\n",
    "model.build_vocab(documents)\n",
    "\n",
    "# Train the model\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('d2v_model_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc2vec model\n",
    "model = Doc2Vec.load('d2v_model_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size:  5702\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary Size: \", len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8688, 300)\n"
     ]
    }
   ],
   "source": [
    "#Create Data Matrix\n",
    "noOfDocuments = len(documents)\n",
    "\n",
    "X = np.empty([noOfDocuments, doc_vector_length])\n",
    "\n",
    "for i in range(noOfDocuments):\n",
    "    X[i] = model.docvecs[i].reshape(1, -1)\n",
    "    \n",
    "print(X.shape)\n",
    "#print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('place', 0.6286035180091858),\n",
       " ('location', 0.4697224199771881),\n",
       " ('establishment', 0.4061688184738159),\n",
       " ('cafe', 0.3623812794685364),\n",
       " ('spot', 0.35029566287994385),\n",
       " ('dish', 0.34263864159584045),\n",
       " ('joint', 0.2884396016597748),\n",
       " ('room', 0.2779262363910675),\n",
       " ('store', 0.2631685137748718),\n",
       " ('operation', 0.261784166097641)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('restaurant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
